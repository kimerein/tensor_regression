{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cabfea0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'CONDA_DEFAULT_ENV'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39m# check environment\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mConda Environment: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m os\u001b[39m.\u001b[39;49menviron[\u001b[39m'\u001b[39;49m\u001b[39mCONDA_DEFAULT_ENV\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\os.py:680\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    677\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencodekey(key)]\n\u001b[0;32m    678\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# raise KeyError with the original key value\u001b[39;00m\n\u001b[1;32m--> 680\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    681\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecodevalue(value)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'CONDA_DEFAULT_ENV'"
     ]
    }
   ],
   "source": [
    "# widen jupyter notebook window\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:95% !important; }</style>\"))\n",
    "\n",
    "# check environment\n",
    "import os\n",
    "print(f'Conda Environment: ' + os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e41a2d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sabatini\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function is_available at 0x00000151FD38E050>\n",
      "<functools._lru_cache_wrapper object at 0x00000151FD3A2C40>\n",
      "<function current_device at 0x00000151FD6A7EB0>\n",
      "Quadro P2000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import tensorly as tl\n",
    "import scipy.signal\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection\n",
    "\n",
    "import itertools\n",
    "\n",
    "print(torch.cuda.is_available)\n",
    "print(torch.cuda.device_count)\n",
    "print(torch.cuda.current_device)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "50afa093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\Users\\sabatini\\Documents\\GitHub')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from tensor_regression import multinomial_tensor_regression as mtr\n",
    "\n",
    "import tensor_regression.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "1b292cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = scipy.io.loadmat('C:/Users/sabatini/Documents/currtens/allLabels.mat', simplify_cells=True)['allLabels']\n",
    "tensor = scipy.io.loadmat('C:/Users/sabatini/Documents/currtens/tensor.mat', simplify_cells=True)['tensor']\n",
    "timepoints = scipy.io.loadmat('C:/Users/sabatini/Documents/currtens/timepoints_for_tensor.mat', simplify_cells=True)['timepoints_for_tensor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "7a24908b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shapes:\n",
      "labels: (40,)\n",
      "tensor: (1041, 450, 40)\n",
      "timepoints: (450,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Input data shapes:')\n",
    "print(f'labels: {labels.shape}')\n",
    "print(f'tensor: {tensor.shape}')\n",
    "print(f'timepoints: {timepoints.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "a62e01ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1110.,  170.,  150.,  110.,  120.,  120.,  270.,  420.,  500.,\n",
       "         400.,  680., 1080.,  900.,  870.,  820.,  430.,  530.,  670.,\n",
       "         560.,  760.,  450.,  800.,  380.,  180.,  170.,  180.,  180.,\n",
       "         360.,  290.,  160.,  120.,  110.,  150.,  170.,  180.,  250.,\n",
       "         230.,   80.,   60.,   50.,   80.,   50.,   70.,   60.,   90.,\n",
       "          40.,   50.,   60.,   60.,   70.,  120.,  110.,   70.,  100.,\n",
       "         100.,  110.,  140.,  110.,  190.,   80.,   60.,  130.,   50.,\n",
       "          20.,   20.,    0.,   30.,   10.,   20.,   10.,   30.,   10.,\n",
       "          30.,   20.,   20.,   20.,   20.,   40.,   20.,   40.,   20.,\n",
       "          40.,   50.,   10.,   10.,   10.,   10.,   10.,   10.,   10.,\n",
       "          10.,   10.,   20.,   10.,   10.,   20.,   10.,   10.,   20.,\n",
       "         120.]),\n",
       " array([2.99480362e-16, 4.00000000e-02, 8.00000000e-02, 1.20000000e-01,\n",
       "        1.60000000e-01, 2.00000000e-01, 2.40000000e-01, 2.80000000e-01,\n",
       "        3.20000000e-01, 3.60000000e-01, 4.00000000e-01, 4.40000000e-01,\n",
       "        4.80000000e-01, 5.20000000e-01, 5.60000000e-01, 6.00000000e-01,\n",
       "        6.40000000e-01, 6.80000000e-01, 7.20000000e-01, 7.60000000e-01,\n",
       "        8.00000000e-01, 8.40000000e-01, 8.80000000e-01, 9.20000000e-01,\n",
       "        9.60000000e-01, 1.00000000e+00, 1.04000000e+00, 1.08000000e+00,\n",
       "        1.12000000e+00, 1.16000000e+00, 1.20000000e+00, 1.24000000e+00,\n",
       "        1.28000000e+00, 1.32000000e+00, 1.36000000e+00, 1.40000000e+00,\n",
       "        1.44000000e+00, 1.48000000e+00, 1.52000000e+00, 1.56000000e+00,\n",
       "        1.60000000e+00, 1.64000000e+00, 1.68000000e+00, 1.72000000e+00,\n",
       "        1.76000000e+00, 1.80000000e+00, 1.84000000e+00, 1.88000000e+00,\n",
       "        1.92000000e+00, 1.96000000e+00, 2.00000000e+00, 2.04000000e+00,\n",
       "        2.08000000e+00, 2.12000000e+00, 2.16000000e+00, 2.20000000e+00,\n",
       "        2.24000000e+00, 2.28000000e+00, 2.32000000e+00, 2.36000000e+00,\n",
       "        2.40000000e+00, 2.44000000e+00, 2.48000000e+00, 2.52000000e+00,\n",
       "        2.56000000e+00, 2.60000000e+00, 2.64000000e+00, 2.68000000e+00,\n",
       "        2.72000000e+00, 2.76000000e+00, 2.80000000e+00, 2.84000000e+00,\n",
       "        2.88000000e+00, 2.92000000e+00, 2.96000000e+00, 3.00000000e+00,\n",
       "        3.04000000e+00, 3.08000000e+00, 3.12000000e+00, 3.16000000e+00,\n",
       "        3.20000000e+00, 3.24000000e+00, 3.28000000e+00, 3.32000000e+00,\n",
       "        3.36000000e+00, 3.40000000e+00, 3.44000000e+00, 3.48000000e+00,\n",
       "        3.52000000e+00, 3.56000000e+00, 3.60000000e+00, 3.64000000e+00,\n",
       "        3.68000000e+00, 3.72000000e+00, 3.76000000e+00, 3.80000000e+00,\n",
       "        3.84000000e+00, 3.88000000e+00, 3.92000000e+00, 3.96000000e+00,\n",
       "        4.00000000e+00]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(tensor[0].reshape(-1), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "6479c1e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (40, 1041, 450), y: (40,)\n",
      "device: 'cuda'\n",
      "[16. 16. 16. 16.]\n",
      "hyperparameters: {'L2': 0.001, 'lr': 0.007, 'rank': 4, 'Bcp_init_scale': 0.625, 'non_negative': [True, False, True], 'iteration': 0}\n",
      "Iteration: 0, Loss: 1.4759434461593628\n",
      "Iteration: 1, Loss: 1.506574273109436\n",
      "Iteration: 2, Loss: 1.5040959119796753\n",
      "Iteration: 3, Loss: 1.4934571981430054\n",
      "Iteration: 4, Loss: 1.491241455078125\n",
      "Iteration: 5, Loss: 1.5028076171875\n",
      "Iteration: 6, Loss: 1.5082762241363525\n",
      "Iteration: 7, Loss: 1.5121811628341675\n",
      "Iteration: 8, Loss: 1.5083808898925781\n",
      "Iteration: 9, Loss: 1.5072007179260254\n",
      "Iteration: 10, Loss: 1.5070346593856812\n",
      "Iteration: 11, Loss: 1.5079319477081299\n",
      "Iteration: 12, Loss: 1.5116117000579834\n",
      "Iteration: 13, Loss: 1.511065125465393\n",
      "Iteration: 14, Loss: 1.5076453685760498\n",
      "Iteration: 15, Loss: 1.5068068504333496\n",
      "Iteration: 16, Loss: 1.5066726207733154\n",
      "Iteration: 17, Loss: 1.5066561698913574\n",
      "Iteration: 18, Loss: 1.506652593612671\n",
      "Iteration: 19, Loss: 1.5066583156585693\n",
      "Iteration: 20, Loss: 1.5067282915115356\n",
      "Iteration: 21, Loss: 1.50746750831604\n",
      "Iteration: 22, Loss: 1.5126526355743408\n",
      "Iteration: 23, Loss: 1.512714147567749\n",
      "Iteration: 24, Loss: 1.5071616172790527\n",
      "Iteration: 25, Loss: 1.5067830085754395\n",
      "Iteration: 26, Loss: 1.5097919702529907\n",
      "Iteration: 27, Loss: 1.5142779350280762\n",
      "Iteration: 28, Loss: 1.5066120624542236\n",
      "Iteration: 29, Loss: 1.5140794515609741\n",
      "Iteration: 30, Loss: 1.5081689357757568\n",
      "Iteration: 31, Loss: 1.5066546201705933\n",
      "Iteration: 32, Loss: 1.5094975233078003\n",
      "Iteration: 33, Loss: 1.5114829540252686\n",
      "Iteration: 34, Loss: 1.5068095922470093\n",
      "Iteration: 35, Loss: 1.507842779159546\n",
      "Iteration: 36, Loss: 1.511338472366333\n",
      "Iteration: 37, Loss: 1.5070075988769531\n",
      "Iteration: 38, Loss: 1.5073944330215454\n",
      "Iteration: 39, Loss: 1.5103167295455933\n",
      "Iteration: 40, Loss: 1.5071485042572021\n",
      "Iteration: 41, Loss: 1.5069501399993896\n",
      "Iteration: 42, Loss: 1.5091476440429688\n",
      "Iteration: 43, Loss: 1.5073316097259521\n",
      "Iteration: 44, Loss: 1.5065964460372925\n",
      "Iteration: 45, Loss: 1.5080839395523071\n",
      "Iteration: 46, Loss: 1.5073798894882202\n",
      "Iteration: 47, Loss: 1.5064541101455688\n",
      "Iteration: 48, Loss: 1.5074082612991333\n",
      "Iteration: 49, Loss: 1.50727117061615\n",
      "Iteration: 50, Loss: 1.5064139366149902\n",
      "Iteration: 51, Loss: 1.506986141204834\n",
      "Iteration: 52, Loss: 1.5070624351501465\n",
      "Iteration: 53, Loss: 1.5063871145248413\n",
      "Iteration: 54, Loss: 1.506738305091858\n",
      "Iteration: 55, Loss: 1.5068459510803223\n",
      "Iteration: 56, Loss: 1.5063433647155762\n",
      "Iteration: 57, Loss: 1.5065743923187256\n",
      "Iteration: 58, Loss: 1.5066421031951904\n",
      "Iteration: 59, Loss: 1.5062766075134277\n",
      "Iteration: 60, Loss: 1.5064440965652466\n",
      "Iteration: 61, Loss: 1.5064491033554077\n",
      "Iteration: 62, Loss: 1.5061700344085693\n",
      "Iteration: 63, Loss: 1.506269931793213\n",
      "Iteration: 64, Loss: 1.5061715841293335\n",
      "Iteration: 65, Loss: 1.5059117078781128\n",
      "Iteration: 66, Loss: 1.5058156251907349\n",
      "Iteration: 67, Loss: 1.5052961111068726\n",
      "Iteration: 68, Loss: 1.5039900541305542\n",
      "Iteration: 69, Loss: 1.4976922273635864\n",
      "Iteration: 70, Loss: 1.3878581523895264\n",
      "Iteration: 71, Loss: 1.2746450901031494\n",
      "Iteration: 72, Loss: 1.2766180038452148\n",
      "Iteration: 73, Loss: 1.3040101528167725\n",
      "Iteration: 74, Loss: 1.2756308317184448\n",
      "Iteration: 75, Loss: 1.2754663228988647\n",
      "Iteration: 76, Loss: 1.2754920721054077\n",
      "Iteration: 77, Loss: 1.2755182981491089\n",
      "Iteration: 78, Loss: 1.2755438089370728\n",
      "Iteration: 79, Loss: 1.2755680084228516\n",
      "Iteration: 80, Loss: 1.2755908966064453\n",
      "Iteration: 81, Loss: 1.2756123542785645\n",
      "Iteration: 82, Loss: 1.2756327390670776\n",
      "Iteration: 83, Loss: 1.2756544351577759\n",
      "Iteration: 84, Loss: 1.275685429573059\n",
      "Iteration: 85, Loss: 1.2757467031478882\n",
      "Iteration: 86, Loss: 1.2758255004882812\n",
      "Iteration: 87, Loss: 1.2758115530014038\n",
      "Iteration: 88, Loss: 1.2757569551467896\n",
      "Iteration: 89, Loss: 1.2757378816604614\n",
      "Iteration: 90, Loss: 1.275736689567566\n",
      "Iteration: 91, Loss: 1.275740385055542\n",
      "Iteration: 92, Loss: 1.2757446765899658\n",
      "Iteration: 93, Loss: 1.275748610496521\n",
      "Iteration: 94, Loss: 1.2757515907287598\n",
      "Iteration: 95, Loss: 1.2757537364959717\n",
      "Iteration: 96, Loss: 1.2757551670074463\n",
      "Iteration: 97, Loss: 1.2757558822631836\n",
      "Iteration: 98, Loss: 1.2757556438446045\n",
      "Iteration: 99, Loss: 1.2757549285888672\n",
      "Iteration: 100, Loss: 1.2757536172866821\n",
      "Iteration: 101, Loss: 1.2757515907287598\n",
      "Iteration: 102, Loss: 1.2757492065429688\n",
      "Iteration: 103, Loss: 1.27574622631073\n",
      "Iteration: 104, Loss: 1.2757428884506226\n",
      "Iteration: 105, Loss: 1.2757391929626465\n",
      "Iteration: 106, Loss: 1.2757350206375122\n",
      "Iteration: 107, Loss: 1.2757306098937988\n",
      "Iteration: 108, Loss: 1.2757258415222168\n",
      "Iteration: 109, Loss: 1.2757208347320557\n",
      "Iteration: 110, Loss: 1.2757154703140259\n",
      "Iteration: 111, Loss: 1.2757099866867065\n",
      "Iteration: 112, Loss: 1.2757041454315186\n",
      "Iteration: 113, Loss: 1.2756983041763306\n",
      "Iteration: 114, Loss: 1.275692105293274\n",
      "Iteration: 115, Loss: 1.2756857872009277\n",
      "Iteration: 116, Loss: 1.275679349899292\n",
      "Iteration: 117, Loss: 1.2756727933883667\n",
      "Iteration: 118, Loss: 1.2756659984588623\n",
      "Iteration: 119, Loss: 1.275659203529358\n",
      "Iteration: 120, Loss: 1.2756521701812744\n",
      "Iteration: 121, Loss: 1.275645136833191\n",
      "Iteration: 122, Loss: 1.2756379842758179\n",
      "Iteration: 123, Loss: 1.2756307125091553\n",
      "Iteration: 124, Loss: 1.2756234407424927\n",
      "Iteration: 125, Loss: 1.2756160497665405\n",
      "Iteration: 126, Loss: 1.2756085395812988\n",
      "Iteration: 127, Loss: 1.2756010293960571\n",
      "Iteration: 128, Loss: 1.2755934000015259\n",
      "Iteration: 129, Loss: 1.275585651397705\n",
      "Iteration: 130, Loss: 1.2755780220031738\n",
      "Iteration: 131, Loss: 1.2755701541900635\n",
      "Iteration: 132, Loss: 1.2755624055862427\n",
      "Iteration: 133, Loss: 1.2755545377731323\n",
      "Iteration: 134, Loss: 1.2755465507507324\n",
      "Iteration: 135, Loss: 1.2755385637283325\n",
      "Iteration: 136, Loss: 1.2755305767059326\n",
      "Iteration: 137, Loss: 1.2755225896835327\n",
      "Iteration: 138, Loss: 1.2755144834518433\n",
      "Iteration: 139, Loss: 1.2755063772201538\n",
      "Iteration: 140, Loss: 1.2754982709884644\n",
      "Iteration: 141, Loss: 1.2754900455474854\n",
      "Iteration: 142, Loss: 1.2754818201065063\n",
      "Iteration: 143, Loss: 1.2754735946655273\n",
      "Iteration: 144, Loss: 1.2754652500152588\n",
      "Iteration: 145, Loss: 1.2754570245742798\n",
      "Iteration: 146, Loss: 1.2754486799240112\n",
      "Iteration: 147, Loss: 1.2754403352737427\n",
      "Iteration: 148, Loss: 1.2754318714141846\n",
      "Iteration: 149, Loss: 1.275423526763916\n",
      "Iteration: 150, Loss: 1.275415062904358\n",
      "Iteration: 151, Loss: 1.2754065990447998\n",
      "Iteration: 152, Loss: 1.2753981351852417\n",
      "Iteration: 153, Loss: 1.2753896713256836\n",
      "Iteration: 154, Loss: 1.275381088256836\n",
      "Iteration: 155, Loss: 1.2753725051879883\n",
      "Iteration: 156, Loss: 1.2753639221191406\n",
      "Iteration: 157, Loss: 1.275355339050293\n",
      "Iteration: 158, Loss: 1.2753467559814453\n",
      "Iteration: 159, Loss: 1.275338053703308\n",
      "Iteration: 160, Loss: 1.2753294706344604\n",
      "Iteration: 161, Loss: 1.2753207683563232\n",
      "Iteration: 162, Loss: 1.275312066078186\n",
      "Iteration: 163, Loss: 1.2753033638000488\n",
      "Iteration: 164, Loss: 1.275294542312622\n",
      "Iteration: 165, Loss: 1.2752858400344849\n",
      "Iteration: 166, Loss: 1.275277018547058\n",
      "Iteration: 167, Loss: 1.2752681970596313\n",
      "Iteration: 168, Loss: 1.2752593755722046\n",
      "Iteration: 169, Loss: 1.2752505540847778\n",
      "Iteration: 170, Loss: 1.2752416133880615\n",
      "Iteration: 171, Loss: 1.2752327919006348\n",
      "Iteration: 172, Loss: 1.2752238512039185\n",
      "Iteration: 173, Loss: 1.2752149105072021\n",
      "Iteration: 174, Loss: 1.2752060890197754\n",
      "Iteration: 175, Loss: 1.2751970291137695\n",
      "Iteration: 176, Loss: 1.2751880884170532\n",
      "Iteration: 177, Loss: 1.275179147720337\n",
      "Iteration: 178, Loss: 1.275170087814331\n",
      "Iteration: 179, Loss: 1.2751610279083252\n",
      "Iteration: 180, Loss: 1.2751519680023193\n",
      "Iteration: 181, Loss: 1.2751429080963135\n",
      "Iteration: 182, Loss: 1.2751338481903076\n",
      "Iteration: 183, Loss: 1.2751247882843018\n",
      "Iteration: 184, Loss: 1.2751156091690063\n",
      "Iteration: 185, Loss: 1.2751065492630005\n",
      "Iteration: 186, Loss: 1.275097370147705\n",
      "Iteration: 187, Loss: 1.2750881910324097\n",
      "Iteration: 188, Loss: 1.2750790119171143\n",
      "Iteration: 189, Loss: 1.2750698328018188\n",
      "Iteration: 190, Loss: 1.2750606536865234\n",
      "Iteration: 191, Loss: 1.2750513553619385\n",
      "Iteration: 192, Loss: 1.275042176246643\n",
      "Iteration: 193, Loss: 1.275032877922058\n",
      "Iteration: 194, Loss: 1.2750235795974731\n",
      "Iteration: 195, Loss: 1.2750142812728882\n",
      "Iteration: 196, Loss: 1.2750049829483032\n",
      "Iteration: 197, Loss: 1.2749956846237183\n",
      "Iteration: 198, Loss: 1.2749862670898438\n",
      "Iteration: 199, Loss: 1.2749769687652588\n",
      "Iteration: 200, Loss: 1.2749675512313843\n",
      "Iteration: 201, Loss: 1.2749581336975098\n",
      "Iteration: 202, Loss: 1.2749488353729248\n",
      "Iteration: 203, Loss: 1.2749392986297607\n",
      "Iteration: 204, Loss: 1.2749298810958862\n",
      "Iteration: 205, Loss: 1.2749204635620117\n",
      "Iteration: 206, Loss: 1.2749110460281372\n",
      "Iteration: 207, Loss: 1.2749015092849731\n",
      "Iteration: 208, Loss: 1.2748920917510986\n",
      "Iteration: 209, Loss: 1.2748825550079346\n",
      "Iteration: 210, Loss: 1.2748730182647705\n",
      "Iteration: 211, Loss: 1.2748634815216064\n",
      "Iteration: 212, Loss: 1.2748539447784424\n",
      "Iteration: 213, Loss: 1.2748444080352783\n",
      "Iteration: 214, Loss: 1.2748347520828247\n",
      "Iteration: 215, Loss: 1.2748252153396606\n",
      "Iteration: 216, Loss: 1.274815559387207\n",
      "Iteration: 217, Loss: 1.2748059034347534\n",
      "Iteration: 218, Loss: 1.2747963666915894\n",
      "Iteration: 219, Loss: 1.2747867107391357\n",
      "Iteration: 220, Loss: 1.2747770547866821\n",
      "Iteration: 221, Loss: 1.274767279624939\n",
      "Iteration: 222, Loss: 1.2747576236724854\n",
      "Iteration: 223, Loss: 1.2747479677200317\n",
      "Iteration: 224, Loss: 1.2747381925582886\n",
      "Iteration: 225, Loss: 1.274728536605835\n",
      "Iteration: 226, Loss: 1.2747187614440918\n",
      "Iteration: 227, Loss: 1.2747089862823486\n",
      "Iteration: 228, Loss: 1.2746992111206055\n",
      "Iteration: 229, Loss: 1.2746894359588623\n",
      "Iteration: 230, Loss: 1.2746796607971191\n",
      "Iteration: 231, Loss: 1.274669885635376\n",
      "Iteration: 232, Loss: 1.2746599912643433\n",
      "Iteration: 233, Loss: 1.2746502161026\n",
      "Iteration: 234, Loss: 1.2746403217315674\n",
      "Iteration: 235, Loss: 1.2746305465698242\n",
      "Iteration: 236, Loss: 1.2746206521987915\n",
      "Iteration: 237, Loss: 1.2746107578277588\n",
      "Iteration: 238, Loss: 1.274600863456726\n",
      "Iteration: 239, Loss: 1.2745909690856934\n",
      "Iteration: 240, Loss: 1.2745811939239502\n",
      "Iteration: 241, Loss: 1.2745712995529175\n",
      "Iteration: 242, Loss: 1.2745612859725952\n",
      "Iteration: 243, Loss: 1.2745513916015625\n",
      "Iteration: 244, Loss: 1.2745413780212402\n",
      "Iteration: 245, Loss: 1.2745314836502075\n",
      "Iteration: 246, Loss: 1.2745214700698853\n",
      "Iteration: 247, Loss: 1.274511456489563\n",
      "Iteration: 248, Loss: 1.2745014429092407\n",
      "Iteration: 249, Loss: 1.2744914293289185\n",
      "Iteration: 250, Loss: 1.2744814157485962\n",
      "Iteration: 251, Loss: 1.274471402168274\n",
      "Iteration: 252, Loss: 1.2744613885879517\n",
      "Iteration: 253, Loss: 1.2744512557983398\n",
      "Iteration: 254, Loss: 1.2744412422180176\n",
      "Iteration: 255, Loss: 1.2744311094284058\n",
      "Iteration: 256, Loss: 1.274420976638794\n",
      "Iteration: 257, Loss: 1.2744109630584717\n",
      "Iteration: 258, Loss: 1.2744008302688599\n",
      "Iteration: 259, Loss: 1.274390697479248\n",
      "Iteration: 260, Loss: 1.2743805646896362\n",
      "Iteration: 261, Loss: 1.2743704319000244\n",
      "Iteration: 262, Loss: 1.274360179901123\n",
      "Iteration: 263, Loss: 1.2743500471115112\n",
      "Iteration: 264, Loss: 1.2743399143218994\n",
      "Iteration: 265, Loss: 1.274329662322998\n",
      "Iteration: 266, Loss: 1.2743195295333862\n",
      "Iteration: 267, Loss: 1.2743092775344849\n",
      "Iteration: 268, Loss: 1.2742990255355835\n",
      "Iteration: 269, Loss: 1.2742888927459717\n",
      "Iteration: 270, Loss: 1.2742786407470703\n",
      "Iteration: 271, Loss: 1.274268388748169\n",
      "Iteration: 272, Loss: 1.2742581367492676\n",
      "Iteration: 273, Loss: 1.2742478847503662\n",
      "Iteration: 274, Loss: 1.2742375135421753\n",
      "Iteration: 275, Loss: 1.274227261543274\n",
      "Iteration: 276, Loss: 1.2742170095443726\n",
      "Iteration: 277, Loss: 1.2742066383361816\n",
      "Iteration: 278, Loss: 1.2741963863372803\n",
      "Iteration: 279, Loss: 1.2741860151290894\n",
      "Iteration: 280, Loss: 1.274175763130188\n",
      "Iteration: 281, Loss: 1.274165391921997\n",
      "Iteration: 282, Loss: 1.2741550207138062\n",
      "Iteration: 283, Loss: 1.2741446495056152\n",
      "Iteration: 284, Loss: 1.2741342782974243\n",
      "Iteration: 285, Loss: 1.2741239070892334\n",
      "Iteration: 286, Loss: 1.2741135358810425\n",
      "Iteration: 287, Loss: 1.2741031646728516\n",
      "Iteration: 288, Loss: 1.274092674255371\n",
      "Iteration: 289, Loss: 1.2740823030471802\n",
      "Iteration: 290, Loss: 1.2740720510482788\n",
      "Iteration: 291, Loss: 1.2740615606307983\n",
      "Iteration: 292, Loss: 1.2740511894226074\n",
      "Iteration: 293, Loss: 1.274040699005127\n",
      "Iteration: 294, Loss: 1.2740302085876465\n",
      "Iteration: 295, Loss: 1.274019718170166\n",
      "Iteration: 296, Loss: 1.2740092277526855\n",
      "Iteration: 297, Loss: 1.2739988565444946\n",
      "Iteration: 298, Loss: 1.2739883661270142\n",
      "Iteration: 299, Loss: 1.2739778757095337\n",
      "Iteration: 300, Loss: 1.2739672660827637\n",
      "Iteration: 301, Loss: 1.2739567756652832\n",
      "Iteration: 302, Loss: 1.2739462852478027\n",
      "Iteration: 303, Loss: 1.2739357948303223\n",
      "Iteration: 304, Loss: 1.2739251852035522\n",
      "Iteration: 305, Loss: 1.2739146947860718\n",
      "Iteration: 306, Loss: 1.2739040851593018\n",
      "Iteration: 307, Loss: 1.2738935947418213\n",
      "Iteration: 308, Loss: 1.2738829851150513\n",
      "Iteration: 309, Loss: 1.2738723754882812\n",
      "Iteration: 310, Loss: 1.2738617658615112\n",
      "Iteration: 311, Loss: 1.2738511562347412\n",
      "Iteration: 312, Loss: 1.2738405466079712\n",
      "Iteration: 313, Loss: 1.2738300561904907\n",
      "Iteration: 314, Loss: 1.2738193273544312\n",
      "Iteration: 315, Loss: 1.2738087177276611\n",
      "Iteration: 316, Loss: 1.2737981081008911\n",
      "Iteration: 317, Loss: 1.273787498474121\n",
      "Iteration: 318, Loss: 1.273776888847351\n",
      "Iteration: 319, Loss: 1.2737661600112915\n",
      "Iteration: 320, Loss: 1.2737555503845215\n",
      "Iteration: 321, Loss: 1.273744821548462\n",
      "Iteration: 322, Loss: 1.273734211921692\n",
      "Iteration: 323, Loss: 1.2737234830856323\n",
      "Iteration: 324, Loss: 1.2737127542495728\n",
      "Iteration: 325, Loss: 1.2737021446228027\n",
      "Iteration: 326, Loss: 1.2736914157867432\n",
      "Iteration: 327, Loss: 1.2736806869506836\n",
      "Iteration: 328, Loss: 1.273669958114624\n",
      "Iteration: 329, Loss: 1.2736592292785645\n",
      "Iteration: 330, Loss: 1.2736485004425049\n",
      "Iteration: 331, Loss: 1.2736377716064453\n",
      "Iteration: 332, Loss: 1.2736270427703857\n",
      "Iteration: 333, Loss: 1.2736163139343262\n",
      "Iteration: 334, Loss: 1.273605465888977\n",
      "Iteration: 335, Loss: 1.2735947370529175\n",
      "Iteration: 336, Loss: 1.273584008216858\n",
      "Iteration: 337, Loss: 1.2735731601715088\n",
      "Iteration: 338, Loss: 1.2735624313354492\n",
      "Iteration: 339, Loss: 1.2735515832901\n",
      "Iteration: 340, Loss: 1.2735408544540405\n",
      "Iteration: 341, Loss: 1.2735300064086914\n",
      "Iteration: 342, Loss: 1.2735192775726318\n",
      "Iteration: 343, Loss: 1.2735084295272827\n",
      "Iteration: 344, Loss: 1.2734975814819336\n",
      "Iteration: 345, Loss: 1.2734867334365845\n",
      "Iteration: 346, Loss: 1.2734758853912354\n",
      "Iteration: 347, Loss: 1.2734650373458862\n",
      "Iteration: 348, Loss: 1.273454189300537\n",
      "Iteration: 349, Loss: 1.273443341255188\n",
      "Iteration: 350, Loss: 1.2734324932098389\n",
      "Iteration: 351, Loss: 1.2734216451644897\n",
      "Iteration: 352, Loss: 1.2734107971191406\n",
      "Iteration: 353, Loss: 1.2733999490737915\n",
      "Iteration: 354, Loss: 1.2733891010284424\n",
      "Iteration: 355, Loss: 1.2733782529830933\n",
      "Iteration: 356, Loss: 1.2733672857284546\n",
      "Iteration: 357, Loss: 1.2733564376831055\n",
      "Iteration: 358, Loss: 1.2733454704284668\n",
      "Iteration: 359, Loss: 1.2733346223831177\n",
      "Iteration: 360, Loss: 1.273323655128479\n",
      "Iteration: 361, Loss: 1.2733128070831299\n",
      "Iteration: 362, Loss: 1.2733018398284912\n",
      "Iteration: 363, Loss: 1.2732908725738525\n",
      "Iteration: 364, Loss: 1.2732799053192139\n",
      "Iteration: 365, Loss: 1.2732690572738647\n",
      "Iteration: 366, Loss: 1.273258090019226\n",
      "Iteration: 367, Loss: 1.2732471227645874\n",
      "Iteration: 368, Loss: 1.2732361555099487\n",
      "Iteration: 369, Loss: 1.27322518825531\n",
      "Iteration: 370, Loss: 1.2732142210006714\n",
      "Iteration: 371, Loss: 1.2732031345367432\n",
      "Iteration: 372, Loss: 1.2731921672821045\n",
      "Iteration: 373, Loss: 1.2731812000274658\n",
      "Iteration: 374, Loss: 1.2731702327728271\n",
      "Iteration: 375, Loss: 1.273159146308899\n",
      "Iteration: 376, Loss: 1.2731481790542603\n",
      "Iteration: 377, Loss: 1.2731372117996216\n",
      "Iteration: 378, Loss: 1.2731261253356934\n",
      "Iteration: 379, Loss: 1.2731151580810547\n",
      "Iteration: 380, Loss: 1.2731040716171265\n",
      "Iteration: 381, Loss: 1.2730931043624878\n",
      "Iteration: 382, Loss: 1.2730820178985596\n",
      "Iteration: 383, Loss: 1.2730709314346313\n",
      "Iteration: 384, Loss: 1.2730599641799927\n",
      "Iteration: 385, Loss: 1.2730488777160645\n",
      "Iteration: 386, Loss: 1.2730377912521362\n",
      "Iteration: 387, Loss: 1.273026704788208\n",
      "Iteration: 388, Loss: 1.2730157375335693\n",
      "Iteration: 389, Loss: 1.2730046510696411\n",
      "Iteration: 390, Loss: 1.272993564605713\n",
      "Iteration: 391, Loss: 1.2729824781417847\n",
      "Iteration: 392, Loss: 1.2729713916778564\n",
      "Iteration: 393, Loss: 1.2729603052139282\n",
      "Iteration: 394, Loss: 1.27294921875\n",
      "Iteration: 395, Loss: 1.2729380130767822\n",
      "Iteration: 396, Loss: 1.272926926612854\n",
      "Iteration: 397, Loss: 1.2729158401489258\n",
      "Iteration: 398, Loss: 1.2729047536849976\n",
      "Iteration: 399, Loss: 1.2728935480117798\n",
      "Iteration: 400, Loss: 1.2728824615478516\n",
      "Iteration: 401, Loss: 1.2728713750839233\n",
      "Iteration: 402, Loss: 1.2728601694107056\n",
      "Iteration: 403, Loss: 1.2728490829467773\n",
      "Iteration: 404, Loss: 1.27283775806427\n",
      "Iteration: 405, Loss: 1.2728267908096313\n",
      "Iteration: 406, Loss: 1.2728155851364136\n",
      "Iteration: 407, Loss: 1.2728044986724854\n",
      "Iteration: 408, Loss: 1.2727932929992676\n",
      "Iteration: 409, Loss: 1.2727820873260498\n",
      "Iteration: 410, Loss: 1.272770881652832\n",
      "Iteration: 411, Loss: 1.2727597951889038\n",
      "Iteration: 412, Loss: 1.272748589515686\n",
      "Iteration: 413, Loss: 1.2727373838424683\n",
      "Iteration: 414, Loss: 1.2727261781692505\n",
      "Iteration: 415, Loss: 1.2727149724960327\n",
      "Iteration: 416, Loss: 1.2727038860321045\n",
      "Iteration: 417, Loss: 1.2726926803588867\n",
      "Iteration: 418, Loss: 1.272681474685669\n",
      "Iteration: 419, Loss: 1.2726701498031616\n",
      "Iteration: 420, Loss: 1.2726589441299438\n",
      "Iteration: 421, Loss: 1.272647738456726\n",
      "Iteration: 422, Loss: 1.2726365327835083\n",
      "Iteration: 423, Loss: 1.2726253271102905\n",
      "Iteration: 424, Loss: 1.2726141214370728\n",
      "Iteration: 425, Loss: 1.272602915763855\n",
      "Iteration: 426, Loss: 1.2725915908813477\n",
      "Iteration: 427, Loss: 1.2725803852081299\n",
      "Iteration: 428, Loss: 1.272569179534912\n",
      "Iteration: 429, Loss: 1.2725578546524048\n",
      "Iteration: 430, Loss: 1.272546648979187\n",
      "Iteration: 431, Loss: 1.2725353240966797\n",
      "Iteration: 432, Loss: 1.272524118423462\n",
      "Iteration: 433, Loss: 1.2725127935409546\n",
      "Iteration: 434, Loss: 1.2725015878677368\n",
      "Iteration: 435, Loss: 1.27249014377594\n",
      "Iteration: 436, Loss: 1.2724789381027222\n",
      "Iteration: 437, Loss: 1.2724676132202148\n",
      "Iteration: 438, Loss: 1.272456407546997\n",
      "Iteration: 439, Loss: 1.2724450826644897\n",
      "Iteration: 440, Loss: 1.2724337577819824\n",
      "Iteration: 441, Loss: 1.2724226713180542\n",
      "Iteration: 442, Loss: 1.2724113464355469\n",
      "Iteration: 443, Loss: 1.2724000215530396\n",
      "Iteration: 444, Loss: 1.2723886966705322\n",
      "Iteration: 445, Loss: 1.272377371788025\n",
      "Iteration: 446, Loss: 1.2723660469055176\n",
      "Iteration: 447, Loss: 1.2723548412322998\n",
      "Iteration: 448, Loss: 1.2723435163497925\n",
      "Iteration: 449, Loss: 1.2723321914672852\n",
      "Iteration: 450, Loss: 1.2723208665847778\n",
      "Iteration: 451, Loss: 1.2723095417022705\n",
      "Iteration: 452, Loss: 1.2722982168197632\n",
      "Iteration: 453, Loss: 1.2722867727279663\n",
      "Iteration: 454, Loss: 1.2722753286361694\n",
      "Iteration: 455, Loss: 1.272264003753662\n",
      "Iteration: 456, Loss: 1.2722526788711548\n",
      "Iteration: 457, Loss: 1.2722413539886475\n",
      "Iteration: 458, Loss: 1.2722300291061401\n",
      "Iteration: 459, Loss: 1.2722185850143433\n",
      "Iteration: 460, Loss: 1.272207260131836\n",
      "Iteration: 461, Loss: 1.2721959352493286\n",
      "Iteration: 462, Loss: 1.2721846103668213\n",
      "Iteration: 463, Loss: 1.272173285484314\n",
      "Iteration: 464, Loss: 1.2721619606018066\n",
      "Iteration: 465, Loss: 1.2721506357192993\n",
      "Iteration: 466, Loss: 1.2721391916275024\n",
      "Iteration: 467, Loss: 1.2721278667449951\n",
      "Iteration: 468, Loss: 1.2721164226531982\n",
      "Iteration: 469, Loss: 1.272105097770691\n",
      "Iteration: 470, Loss: 1.272093653678894\n",
      "Iteration: 471, Loss: 1.2720823287963867\n",
      "Iteration: 472, Loss: 1.2720708847045898\n",
      "Iteration: 473, Loss: 1.272059440612793\n",
      "Iteration: 474, Loss: 1.2720481157302856\n",
      "Iteration: 475, Loss: 1.2720365524291992\n",
      "Iteration: 476, Loss: 1.272025227546692\n",
      "Iteration: 477, Loss: 1.272013783454895\n",
      "Iteration: 478, Loss: 1.2720023393630981\n",
      "Iteration: 479, Loss: 1.2719910144805908\n",
      "Iteration: 480, Loss: 1.271979570388794\n",
      "Iteration: 481, Loss: 1.271968126296997\n",
      "Iteration: 482, Loss: 1.2719566822052002\n",
      "Iteration: 483, Loss: 1.2719452381134033\n",
      "Iteration: 484, Loss: 1.2719337940216064\n",
      "Iteration: 485, Loss: 1.2719225883483887\n",
      "Iteration: 486, Loss: 1.2719111442565918\n",
      "Iteration: 487, Loss: 1.271899700164795\n",
      "Iteration: 488, Loss: 1.271888256072998\n",
      "Iteration: 489, Loss: 1.2718768119812012\n",
      "Iteration: 490, Loss: 1.2718653678894043\n",
      "Iteration: 491, Loss: 1.2718539237976074\n",
      "Iteration: 492, Loss: 1.2718424797058105\n",
      "Iteration: 493, Loss: 1.2718310356140137\n",
      "Iteration: 494, Loss: 1.2718195915222168\n",
      "Iteration: 495, Loss: 1.27180814743042\n",
      "Iteration: 496, Loss: 1.2717965841293335\n",
      "Iteration: 497, Loss: 1.271785020828247\n",
      "Iteration: 498, Loss: 1.2717735767364502\n",
      "Iteration: 499, Loss: 1.2717621326446533\n",
      "Iteration: 500, Loss: 1.2717506885528564\n",
      "Iteration: 501, Loss: 1.2717392444610596\n",
      "Iteration: 502, Loss: 1.2717278003692627\n",
      "Iteration: 503, Loss: 1.2717162370681763\n",
      "Iteration: 504, Loss: 1.2717047929763794\n",
      "Iteration: 505, Loss: 1.2716933488845825\n",
      "Iteration: 506, Loss: 1.271681785583496\n",
      "Iteration: 507, Loss: 1.2716703414916992\n",
      "Iteration: 508, Loss: 1.2716588973999023\n",
      "Iteration: 509, Loss: 1.2716474533081055\n",
      "Iteration: 510, Loss: 1.2716360092163086\n",
      "Iteration: 511, Loss: 1.2716245651245117\n",
      "Iteration: 512, Loss: 1.2716130018234253\n",
      "Iteration: 513, Loss: 1.2716015577316284\n",
      "Iteration: 514, Loss: 1.271589994430542\n",
      "Iteration: 515, Loss: 1.2715785503387451\n",
      "Iteration: 516, Loss: 1.2715669870376587\n",
      "Iteration: 517, Loss: 1.2715555429458618\n",
      "Iteration: 518, Loss: 1.2715438604354858\n",
      "Iteration: 519, Loss: 1.271532416343689\n",
      "Iteration: 520, Loss: 1.2715208530426025\n",
      "Iteration: 521, Loss: 1.2715094089508057\n",
      "Iteration: 522, Loss: 1.2714978456497192\n",
      "Iteration: 523, Loss: 1.2714862823486328\n",
      "Iteration: 524, Loss: 1.271474838256836\n",
      "Iteration: 525, Loss: 1.2714632749557495\n",
      "Iteration: 526, Loss: 1.2714518308639526\n",
      "Iteration: 527, Loss: 1.2714402675628662\n",
      "Iteration: 528, Loss: 1.2714287042617798\n",
      "Iteration: 529, Loss: 1.2714171409606934\n",
      "Iteration: 530, Loss: 1.2714056968688965\n",
      "Iteration: 531, Loss: 1.27139413356781\n",
      "Iteration: 532, Loss: 1.2713825702667236\n",
      "Iteration: 533, Loss: 1.2713710069656372\n",
      "Iteration: 534, Loss: 1.2713595628738403\n",
      "Iteration: 535, Loss: 1.271347999572754\n",
      "Iteration: 536, Loss: 1.2713364362716675\n",
      "Iteration: 537, Loss: 1.271324872970581\n",
      "Iteration: 538, Loss: 1.2713133096694946\n",
      "Iteration: 539, Loss: 1.2713018655776978\n",
      "Iteration: 540, Loss: 1.2712903022766113\n",
      "Iteration: 541, Loss: 1.271278738975525\n",
      "Iteration: 542, Loss: 1.2712671756744385\n",
      "Iteration: 543, Loss: 1.271255612373352\n",
      "Iteration: 544, Loss: 1.2712440490722656\n",
      "Iteration: 545, Loss: 1.2712324857711792\n",
      "Iteration: 546, Loss: 1.2712209224700928\n",
      "Iteration: 547, Loss: 1.2712093591690063\n",
      "Iteration: 548, Loss: 1.27119779586792\n",
      "Iteration: 549, Loss: 1.2711862325668335\n",
      "Iteration: 550, Loss: 1.271174669265747\n",
      "Iteration: 551, Loss: 1.2711631059646606\n",
      "Iteration: 552, Loss: 1.2711515426635742\n",
      "Iteration: 553, Loss: 1.2711399793624878\n",
      "Iteration: 554, Loss: 1.2711284160614014\n",
      "Iteration: 555, Loss: 1.271116852760315\n",
      "Iteration: 556, Loss: 1.2711052894592285\n",
      "Iteration: 557, Loss: 1.271093726158142\n",
      "Iteration: 558, Loss: 1.2710821628570557\n",
      "Iteration: 559, Loss: 1.2710705995559692\n",
      "Iteration: 560, Loss: 1.2710590362548828\n",
      "Iteration: 561, Loss: 1.2710473537445068\n",
      "Iteration: 562, Loss: 1.2710357904434204\n",
      "Iteration: 563, Loss: 1.2710241079330444\n",
      "Iteration: 564, Loss: 1.271012544631958\n",
      "Iteration: 565, Loss: 1.2710009813308716\n",
      "Iteration: 566, Loss: 1.2709894180297852\n",
      "Iteration: 567, Loss: 1.2709777355194092\n",
      "Iteration: 568, Loss: 1.2709661722183228\n",
      "Iteration: 569, Loss: 1.2709546089172363\n",
      "Iteration: 570, Loss: 1.27094304561615\n",
      "Iteration: 571, Loss: 1.270931363105774\n",
      "Iteration: 572, Loss: 1.2709197998046875\n",
      "Iteration: 573, Loss: 1.270908236503601\n",
      "Iteration: 574, Loss: 1.270896553993225\n",
      "Iteration: 575, Loss: 1.2708849906921387\n",
      "Iteration: 576, Loss: 1.2708734273910522\n",
      "Iteration: 577, Loss: 1.2708617448806763\n",
      "Iteration: 578, Loss: 1.2708501815795898\n",
      "Iteration: 579, Loss: 1.2708386182785034\n",
      "Iteration: 580, Loss: 1.2708269357681274\n",
      "Iteration: 581, Loss: 1.270815372467041\n",
      "Iteration: 582, Loss: 1.270803689956665\n",
      "Iteration: 583, Loss: 1.2707921266555786\n",
      "Iteration: 584, Loss: 1.2707805633544922\n",
      "Iteration: 585, Loss: 1.2707688808441162\n",
      "Iteration: 586, Loss: 1.2707573175430298\n",
      "Iteration: 587, Loss: 1.2707456350326538\n",
      "Iteration: 588, Loss: 1.2707340717315674\n",
      "Iteration: 589, Loss: 1.2707223892211914\n",
      "Iteration: 590, Loss: 1.270710825920105\n",
      "Iteration: 591, Loss: 1.270699143409729\n",
      "Iteration: 592, Loss: 1.2706875801086426\n",
      "Iteration: 593, Loss: 1.2706758975982666\n",
      "Iteration: 594, Loss: 1.2706643342971802\n",
      "Iteration: 595, Loss: 1.2706526517868042\n",
      "Iteration: 596, Loss: 1.2706410884857178\n",
      "Iteration: 597, Loss: 1.2706294059753418\n",
      "Iteration: 598, Loss: 1.2706178426742554\n",
      "Iteration: 599, Loss: 1.2706061601638794\n",
      "Iteration: 600, Loss: 1.2705944776535034\n",
      "Iteration: 601, Loss: 1.270582914352417\n",
      "Iteration: 602, Loss: 1.270571231842041\n",
      "Iteration: 603, Loss: 1.2705596685409546\n",
      "Iteration: 604, Loss: 1.2705479860305786\n",
      "Iteration: 605, Loss: 1.2705363035202026\n",
      "Iteration: 606, Loss: 1.2705247402191162\n",
      "Iteration: 607, Loss: 1.2705130577087402\n",
      "Iteration: 608, Loss: 1.2705014944076538\n",
      "Iteration: 609, Loss: 1.2704898118972778\n",
      "Iteration: 610, Loss: 1.2704781293869019\n",
      "Iteration: 611, Loss: 1.2704665660858154\n",
      "Iteration: 612, Loss: 1.2704548835754395\n",
      "Iteration: 613, Loss: 1.2704432010650635\n",
      "Iteration: 614, Loss: 1.270431637763977\n",
      "Iteration: 615, Loss: 1.270419955253601\n",
      "Iteration: 616, Loss: 1.270408272743225\n",
      "Iteration: 617, Loss: 1.2703967094421387\n",
      "Iteration: 618, Loss: 1.2703850269317627\n",
      "Iteration: 619, Loss: 1.2703733444213867\n",
      "Iteration: 620, Loss: 1.2703616619110107\n",
      "Iteration: 621, Loss: 1.2703500986099243\n",
      "Iteration: 622, Loss: 1.2703384160995483\n",
      "Iteration: 623, Loss: 1.2703267335891724\n",
      "Iteration: 624, Loss: 1.270315170288086\n",
      "Iteration: 625, Loss: 1.27030348777771\n",
      "Iteration: 626, Loss: 1.270291805267334\n",
      "Iteration: 627, Loss: 1.270280122756958\n",
      "Iteration: 628, Loss: 1.2702685594558716\n",
      "Iteration: 629, Loss: 1.2702568769454956\n",
      "Iteration: 630, Loss: 1.2702451944351196\n",
      "Iteration: 631, Loss: 1.2702335119247437\n",
      "Iteration: 632, Loss: 1.2702218294143677\n",
      "Iteration: 633, Loss: 1.2702102661132812\n",
      "Iteration: 634, Loss: 1.2701985836029053\n",
      "Iteration: 635, Loss: 1.2701869010925293\n",
      "Iteration: 636, Loss: 1.2701752185821533\n",
      "Iteration: 637, Loss: 1.2701635360717773\n",
      "Iteration: 638, Loss: 1.270151972770691\n",
      "Iteration: 639, Loss: 1.270140290260315\n",
      "Iteration: 640, Loss: 1.270128607749939\n",
      "Iteration: 641, Loss: 1.270116925239563\n",
      "Iteration: 642, Loss: 1.270105242729187\n",
      "Iteration: 643, Loss: 1.270093560218811\n",
      "Iteration: 644, Loss: 1.2700819969177246\n",
      "Iteration: 645, Loss: 1.2700703144073486\n",
      "Iteration: 646, Loss: 1.2700586318969727\n",
      "Iteration: 647, Loss: 1.2700469493865967\n",
      "Iteration: 648, Loss: 1.2700352668762207\n",
      "Iteration: 649, Loss: 1.2700235843658447\n",
      "Iteration: 650, Loss: 1.2700119018554688\n",
      "Iteration: 651, Loss: 1.2700003385543823\n",
      "Iteration: 652, Loss: 1.2699886560440063\n",
      "Iteration: 653, Loss: 1.2699769735336304\n",
      "Iteration: 654, Loss: 1.2699652910232544\n",
      "Iteration: 655, Loss: 1.2699536085128784\n",
      "Iteration: 656, Loss: 1.2699419260025024\n",
      "Iteration: 657, Loss: 1.2699302434921265\n",
      "Iteration: 658, Loss: 1.2699185609817505\n",
      "Iteration: 659, Loss: 1.269906997680664\n",
      "Iteration: 660, Loss: 1.269895315170288\n",
      "Iteration: 661, Loss: 1.269883632659912\n",
      "Iteration: 662, Loss: 1.2698719501495361\n",
      "Iteration: 663, Loss: 1.2698602676391602\n",
      "Iteration: 664, Loss: 1.2698485851287842\n",
      "Iteration: 665, Loss: 1.2698369026184082\n",
      "Iteration: 666, Loss: 1.2698252201080322\n",
      "Iteration: 667, Loss: 1.2698135375976562\n",
      "Iteration: 668, Loss: 1.2698018550872803\n",
      "Iteration: 669, Loss: 1.2697901725769043\n",
      "Iteration: 670, Loss: 1.2697786092758179\n",
      "Iteration: 671, Loss: 1.269766926765442\n",
      "Iteration: 672, Loss: 1.269755244255066\n",
      "Iteration: 673, Loss: 1.26974356174469\n",
      "Iteration: 674, Loss: 1.269731879234314\n",
      "Iteration: 675, Loss: 1.269720196723938\n",
      "Iteration: 676, Loss: 1.269708514213562\n",
      "Iteration: 677, Loss: 1.269696831703186\n",
      "Iteration: 678, Loss: 1.26968514919281\n",
      "Iteration: 679, Loss: 1.269673466682434\n",
      "Iteration: 680, Loss: 1.269661784172058\n",
      "Iteration: 681, Loss: 1.2696501016616821\n",
      "Iteration: 682, Loss: 1.2696384191513062\n",
      "Iteration: 683, Loss: 1.2696267366409302\n",
      "Iteration: 684, Loss: 1.2696150541305542\n",
      "Iteration: 685, Loss: 1.2696033716201782\n",
      "Iteration: 686, Loss: 1.2695916891098022\n",
      "Iteration: 687, Loss: 1.2695800065994263\n",
      "Iteration: 688, Loss: 1.2695683240890503\n",
      "Iteration: 689, Loss: 1.2695567607879639\n",
      "Iteration: 690, Loss: 1.269545078277588\n",
      "Iteration: 691, Loss: 1.269533395767212\n",
      "Iteration: 692, Loss: 1.269521713256836\n",
      "Iteration: 693, Loss: 1.26951003074646\n",
      "Iteration: 694, Loss: 1.269498348236084\n",
      "Iteration: 695, Loss: 1.269486665725708\n",
      "Iteration: 696, Loss: 1.269474983215332\n",
      "Iteration: 697, Loss: 1.269463300704956\n",
      "Iteration: 698, Loss: 1.26945161819458\n",
      "Iteration: 699, Loss: 1.269439935684204\n",
      "Iteration: 700, Loss: 1.2694282531738281\n",
      "Iteration: 701, Loss: 1.2694165706634521\n",
      "Iteration: 702, Loss: 1.2694048881530762\n",
      "Iteration: 703, Loss: 1.2693932056427002\n",
      "Iteration: 704, Loss: 1.2693815231323242\n",
      "Iteration: 705, Loss: 1.2693698406219482\n",
      "Iteration: 706, Loss: 1.2693581581115723\n",
      "Iteration: 707, Loss: 1.2693464756011963\n",
      "Iteration: 708, Loss: 1.2693347930908203\n",
      "Iteration: 709, Loss: 1.2693231105804443\n",
      "Iteration: 710, Loss: 1.2693114280700684\n",
      "Iteration: 711, Loss: 1.269299864768982\n",
      "Iteration: 712, Loss: 1.269288182258606\n",
      "Iteration: 713, Loss: 1.26927649974823\n",
      "Iteration: 714, Loss: 1.269264817237854\n",
      "Iteration: 715, Loss: 1.269253134727478\n",
      "Iteration: 716, Loss: 1.269241452217102\n",
      "Iteration: 717, Loss: 1.269229769706726\n",
      "Iteration: 718, Loss: 1.26921808719635\n",
      "Iteration: 719, Loss: 1.2692064046859741\n",
      "Iteration: 720, Loss: 1.2691947221755981\n",
      "Iteration: 721, Loss: 1.2691830396652222\n",
      "Iteration: 722, Loss: 1.2691713571548462\n",
      "Iteration: 723, Loss: 1.2691596746444702\n",
      "Iteration: 724, Loss: 1.2691479921340942\n",
      "Iteration: 725, Loss: 1.2691363096237183\n",
      "Iteration: 726, Loss: 1.2691247463226318\n",
      "Iteration: 727, Loss: 1.2691130638122559\n",
      "Iteration: 728, Loss: 1.2691013813018799\n",
      "Iteration: 729, Loss: 1.269089698791504\n",
      "Iteration: 730, Loss: 1.269078016281128\n",
      "Iteration: 731, Loss: 1.269066333770752\n",
      "Iteration: 732, Loss: 1.269054651260376\n",
      "Iteration: 733, Loss: 1.26904296875\n",
      "Iteration: 734, Loss: 1.269031286239624\n",
      "Iteration: 735, Loss: 1.269019603729248\n",
      "Iteration: 736, Loss: 1.269007921218872\n",
      "Iteration: 737, Loss: 1.268996238708496\n",
      "Iteration: 738, Loss: 1.2689846754074097\n",
      "Iteration: 739, Loss: 1.2689729928970337\n",
      "Iteration: 740, Loss: 1.2689613103866577\n",
      "Iteration: 741, Loss: 1.2689496278762817\n",
      "Iteration: 742, Loss: 1.2689379453659058\n",
      "Iteration: 743, Loss: 1.2689262628555298\n",
      "Iteration: 744, Loss: 1.2689145803451538\n",
      "Iteration: 745, Loss: 1.2689028978347778\n",
      "Iteration: 746, Loss: 1.2688912153244019\n",
      "Iteration: 747, Loss: 1.2688796520233154\n",
      "Iteration: 748, Loss: 1.2688679695129395\n",
      "Iteration: 749, Loss: 1.2688562870025635\n",
      "Iteration: 750, Loss: 1.2688446044921875\n",
      "Iteration: 751, Loss: 1.2688329219818115\n",
      "Iteration: 752, Loss: 1.2688212394714355\n",
      "Iteration: 753, Loss: 1.2688095569610596\n",
      "Iteration: 754, Loss: 1.2687979936599731\n",
      "Iteration: 755, Loss: 1.2687863111495972\n",
      "Iteration: 756, Loss: 1.2687746286392212\n",
      "Iteration: 757, Loss: 1.2687629461288452\n",
      "Iteration: 758, Loss: 1.2687512636184692\n",
      "Iteration: 759, Loss: 1.2687395811080933\n",
      "Iteration: 760, Loss: 1.2687278985977173\n",
      "Iteration: 761, Loss: 1.2687163352966309\n",
      "Iteration: 762, Loss: 1.2687046527862549\n",
      "Iteration: 763, Loss: 1.268692970275879\n",
      "Iteration: 764, Loss: 1.268681287765503\n",
      "Iteration: 765, Loss: 1.268669605255127\n",
      "Iteration: 766, Loss: 1.2686580419540405\n",
      "Iteration: 767, Loss: 1.2686463594436646\n",
      "Iteration: 768, Loss: 1.2686346769332886\n",
      "Iteration: 769, Loss: 1.2686229944229126\n",
      "Iteration: 770, Loss: 1.2686113119125366\n",
      "Iteration: 771, Loss: 1.2685997486114502\n",
      "Iteration: 772, Loss: 1.2685880661010742\n",
      "Iteration: 773, Loss: 1.2685763835906982\n",
      "Iteration: 774, Loss: 1.2685647010803223\n",
      "Iteration: 775, Loss: 1.2685530185699463\n",
      "Iteration: 776, Loss: 1.2685414552688599\n",
      "Iteration: 777, Loss: 1.2685297727584839\n",
      "Iteration: 778, Loss: 1.268518090248108\n",
      "Iteration: 779, Loss: 1.268506407737732\n",
      "Iteration: 780, Loss: 1.2684948444366455\n",
      "Iteration: 781, Loss: 1.2684831619262695\n",
      "Iteration: 782, Loss: 1.2684714794158936\n",
      "Iteration: 783, Loss: 1.2684599161148071\n",
      "Iteration: 784, Loss: 1.2684482336044312\n",
      "Iteration: 785, Loss: 1.2684365510940552\n",
      "Iteration: 786, Loss: 1.2684248685836792\n",
      "Iteration: 787, Loss: 1.2684133052825928\n",
      "Iteration: 788, Loss: 1.2684016227722168\n",
      "Iteration: 789, Loss: 1.2683898210525513\n",
      "Iteration: 790, Loss: 1.2683783769607544\n",
      "Iteration: 791, Loss: 1.2683666944503784\n",
      "Iteration: 792, Loss: 1.2683550119400024\n",
      "Iteration: 793, Loss: 1.2683433294296265\n",
      "Iteration: 794, Loss: 1.2683316469192505\n",
      "Iteration: 795, Loss: 1.2683199644088745\n",
      "Iteration: 796, Loss: 1.268308401107788\n",
      "Iteration: 797, Loss: 1.268296718597412\n",
      "Iteration: 798, Loss: 1.2682850360870361\n",
      "Iteration: 799, Loss: 1.2682734727859497\n",
      "Iteration: 800, Loss: 1.2682617902755737\n",
      "Iteration: 801, Loss: 1.2682502269744873\n",
      "Iteration: 802, Loss: 1.2682385444641113\n",
      "Iteration: 803, Loss: 1.2682268619537354\n",
      "Iteration: 804, Loss: 1.268215298652649\n",
      "Iteration: 805, Loss: 1.268203616142273\n",
      "Iteration: 806, Loss: 1.2681920528411865\n",
      "Iteration: 807, Loss: 1.2681803703308105\n",
      "Iteration: 808, Loss: 1.2681686878204346\n",
      "Iteration: 809, Loss: 1.2681571245193481\n",
      "Iteration: 810, Loss: 1.2681454420089722\n",
      "Iteration: 811, Loss: 1.2681338787078857\n",
      "Iteration: 812, Loss: 1.2681221961975098\n",
      "Iteration: 813, Loss: 1.2681106328964233\n",
      "Iteration: 814, Loss: 1.2680989503860474\n",
      "Iteration: 815, Loss: 1.268087387084961\n",
      "Iteration: 816, Loss: 1.268075704574585\n",
      "Iteration: 817, Loss: 1.2680641412734985\n",
      "Iteration: 818, Loss: 1.2680524587631226\n",
      "Iteration: 819, Loss: 1.2680408954620361\n",
      "Iteration: 820, Loss: 1.2680292129516602\n",
      "Iteration: 821, Loss: 1.2680176496505737\n",
      "Iteration: 822, Loss: 1.2680059671401978\n",
      "Iteration: 823, Loss: 1.2679944038391113\n",
      "Iteration: 824, Loss: 1.2679827213287354\n",
      "Iteration: 825, Loss: 1.267971158027649\n",
      "Iteration: 826, Loss: 1.267959475517273\n",
      "Iteration: 827, Loss: 1.2679479122161865\n",
      "Iteration: 828, Loss: 1.2679363489151\n",
      "Iteration: 829, Loss: 1.2679246664047241\n",
      "Iteration: 830, Loss: 1.2679131031036377\n",
      "Iteration: 831, Loss: 1.2679014205932617\n",
      "Iteration: 832, Loss: 1.2678898572921753\n",
      "Iteration: 833, Loss: 1.2678781747817993\n",
      "Iteration: 834, Loss: 1.267866611480713\n",
      "Iteration: 835, Loss: 1.2678550481796265\n",
      "Iteration: 836, Loss: 1.2678433656692505\n",
      "Iteration: 837, Loss: 1.267831802368164\n",
      "Iteration: 838, Loss: 1.267820119857788\n",
      "Iteration: 839, Loss: 1.2678085565567017\n",
      "Iteration: 840, Loss: 1.2677968740463257\n",
      "Iteration: 841, Loss: 1.2677851915359497\n",
      "Iteration: 842, Loss: 1.2677736282348633\n",
      "Iteration: 843, Loss: 1.2677620649337769\n",
      "Iteration: 844, Loss: 1.2677503824234009\n",
      "Iteration: 845, Loss: 1.2677388191223145\n",
      "Iteration: 846, Loss: 1.267727017402649\n",
      "Iteration: 847, Loss: 1.267715334892273\n",
      "Iteration: 848, Loss: 1.2677037715911865\n",
      "Iteration: 849, Loss: 1.2676922082901\n",
      "Iteration: 850, Loss: 1.2676804065704346\n",
      "Iteration: 851, Loss: 1.2676688432693481\n",
      "Iteration: 852, Loss: 1.2676572799682617\n",
      "Iteration: 853, Loss: 1.2676457166671753\n",
      "Iteration: 854, Loss: 1.2676340341567993\n",
      "Iteration: 855, Loss: 1.267622470855713\n",
      "Iteration: 856, Loss: 1.2676109075546265\n",
      "Iteration: 857, Loss: 1.26759934425354\n",
      "Iteration: 858, Loss: 1.267587661743164\n",
      "Iteration: 859, Loss: 1.2675760984420776\n",
      "Iteration: 860, Loss: 1.2675644159317017\n",
      "Iteration: 861, Loss: 1.2675526142120361\n",
      "Iteration: 862, Loss: 1.2675409317016602\n",
      "Iteration: 863, Loss: 1.2675293684005737\n",
      "Iteration: 864, Loss: 1.2675178050994873\n",
      "Iteration: 865, Loss: 1.2675061225891113\n",
      "Iteration: 866, Loss: 1.2674944400787354\n",
      "Iteration: 867, Loss: 1.267482876777649\n",
      "Iteration: 868, Loss: 1.2674713134765625\n",
      "Iteration: 869, Loss: 1.267459750175476\n",
      "Iteration: 870, Loss: 1.2674479484558105\n",
      "Iteration: 871, Loss: 1.2674363851547241\n",
      "Iteration: 872, Loss: 1.2674245834350586\n",
      "Iteration: 873, Loss: 1.2674129009246826\n",
      "Iteration: 874, Loss: 1.2674012184143066\n",
      "Iteration: 875, Loss: 1.2673896551132202\n",
      "Iteration: 876, Loss: 1.2673779726028442\n",
      "Iteration: 877, Loss: 1.2673661708831787\n",
      "Iteration: 878, Loss: 1.2673543691635132\n",
      "Iteration: 879, Loss: 1.2673428058624268\n",
      "Iteration: 880, Loss: 1.2673311233520508\n",
      "Iteration: 881, Loss: 1.2673192024230957\n",
      "Iteration: 882, Loss: 1.2673075199127197\n",
      "Iteration: 883, Loss: 1.2672958374023438\n",
      "Iteration: 884, Loss: 1.2672837972640991\n",
      "Iteration: 885, Loss: 1.2672722339630127\n",
      "Iteration: 886, Loss: 1.2672603130340576\n",
      "Iteration: 887, Loss: 1.267248511314392\n",
      "Iteration: 888, Loss: 1.2672364711761475\n",
      "Iteration: 889, Loss: 1.267224669456482\n",
      "Iteration: 890, Loss: 1.2672127485275269\n",
      "Iteration: 891, Loss: 1.2672005891799927\n",
      "Iteration: 892, Loss: 1.267188549041748\n",
      "Iteration: 893, Loss: 1.2671767473220825\n",
      "Iteration: 894, Loss: 1.267164707183838\n",
      "Iteration: 895, Loss: 1.267152190208435\n",
      "Iteration: 896, Loss: 1.2671399116516113\n",
      "Iteration: 897, Loss: 1.2671273946762085\n",
      "Iteration: 898, Loss: 1.2671147584915161\n",
      "Iteration: 899, Loss: 1.267101764678955\n",
      "Iteration: 900, Loss: 1.267088532447815\n",
      "Iteration: 901, Loss: 1.2670753002166748\n",
      "Iteration: 902, Loss: 1.2670612335205078\n",
      "Iteration: 903, Loss: 1.2670466899871826\n",
      "Iteration: 904, Loss: 1.2670310735702515\n",
      "Iteration: 905, Loss: 1.2670143842697144\n",
      "Iteration: 906, Loss: 1.2669957876205444\n",
      "Iteration: 907, Loss: 1.2669734954833984\n",
      "Iteration: 908, Loss: 1.266945242881775\n",
      "Iteration: 909, Loss: 1.2669049501419067\n",
      "Iteration: 910, Loss: 1.2668368816375732\n",
      "Iteration: 911, Loss: 1.2666897773742676\n",
      "Iteration: 912, Loss: 1.2661689519882202\n",
      "Iteration: 913, Loss: 1.26106595993042\n",
      "Iteration: 914, Loss: 1.2668765783309937\n",
      "Iteration: 915, Loss: 1.2662712335586548\n",
      "Iteration: 916, Loss: 0.8827459812164307\n",
      "Iteration: 917, Loss: 1.250712275505066\n",
      "Iteration: 918, Loss: 1.2655318975448608\n",
      "Iteration: 919, Loss: 1.2691240310668945\n",
      "Iteration: 920, Loss: 1.2673536539077759\n",
      "Iteration: 921, Loss: 1.2675774097442627\n",
      "Iteration: 922, Loss: 1.2677557468414307\n",
      "Iteration: 923, Loss: 1.26804780960083\n",
      "Iteration: 924, Loss: 1.2682983875274658\n",
      "Iteration: 925, Loss: 1.2685298919677734\n",
      "Iteration: 926, Loss: 1.2687501907348633\n",
      "Iteration: 927, Loss: 1.268957495689392\n",
      "Iteration: 928, Loss: 1.2691508531570435\n",
      "Iteration: 929, Loss: 1.2693300247192383\n",
      "Iteration: 930, Loss: 1.2694951295852661\n",
      "Iteration: 931, Loss: 1.2696466445922852\n",
      "Iteration: 932, Loss: 1.269784927368164\n",
      "Iteration: 933, Loss: 1.2699108123779297\n",
      "Iteration: 934, Loss: 1.2700251340866089\n",
      "Iteration: 935, Loss: 1.270128607749939\n",
      "Iteration: 936, Loss: 1.2702220678329468\n",
      "Iteration: 937, Loss: 1.2703062295913696\n",
      "Iteration: 938, Loss: 1.2703818082809448\n",
      "Iteration: 939, Loss: 1.2704496383666992\n",
      "Iteration: 940, Loss: 1.270510196685791\n",
      "Iteration: 941, Loss: 1.270564317703247\n",
      "Iteration: 942, Loss: 1.2706124782562256\n",
      "Iteration: 943, Loss: 1.2706550359725952\n",
      "Iteration: 944, Loss: 1.2706928253173828\n",
      "Iteration: 945, Loss: 1.2707260847091675\n",
      "Iteration: 946, Loss: 1.2707552909851074\n",
      "Iteration: 947, Loss: 1.2707808017730713\n",
      "Iteration: 948, Loss: 1.2708029747009277\n",
      "Iteration: 949, Loss: 1.2708221673965454\n",
      "Iteration: 950, Loss: 1.2708386182785034\n",
      "Iteration: 951, Loss: 1.2708525657653809\n",
      "Iteration: 952, Loss: 1.2708642482757568\n",
      "Iteration: 953, Loss: 1.2708739042282104\n",
      "Iteration: 954, Loss: 1.2708818912506104\n",
      "Iteration: 955, Loss: 1.270888090133667\n",
      "Iteration: 956, Loss: 1.270892858505249\n",
      "Iteration: 957, Loss: 1.270896315574646\n",
      "Iteration: 958, Loss: 1.2708985805511475\n",
      "Iteration: 959, Loss: 1.2708996534347534\n",
      "Iteration: 960, Loss: 1.2708998918533325\n",
      "Iteration: 961, Loss: 1.2708991765975952\n",
      "Iteration: 962, Loss: 1.270897626876831\n",
      "Iteration: 963, Loss: 1.2708953619003296\n",
      "Iteration: 964, Loss: 1.2708925008773804\n",
      "Iteration: 965, Loss: 1.2708890438079834\n",
      "Iteration: 966, Loss: 1.2708851099014282\n",
      "Iteration: 967, Loss: 1.2708805799484253\n",
      "Iteration: 968, Loss: 1.2708756923675537\n",
      "Iteration: 969, Loss: 1.2708704471588135\n",
      "Iteration: 970, Loss: 1.2708648443222046\n",
      "Iteration: 971, Loss: 1.2708590030670166\n",
      "Iteration: 972, Loss: 1.27085280418396\n",
      "Iteration: 973, Loss: 1.2708463668823242\n",
      "Iteration: 974, Loss: 1.2708396911621094\n",
      "Iteration: 975, Loss: 1.2708327770233154\n",
      "Iteration: 976, Loss: 1.270825743675232\n",
      "Iteration: 977, Loss: 1.2708185911178589\n",
      "Iteration: 978, Loss: 1.2708112001419067\n",
      "Iteration: 979, Loss: 1.270803689956665\n",
      "Iteration: 980, Loss: 1.2707960605621338\n",
      "Iteration: 981, Loss: 1.2707884311676025\n",
      "Iteration: 982, Loss: 1.2707805633544922\n",
      "Iteration: 983, Loss: 1.2707726955413818\n",
      "Iteration: 984, Loss: 1.270764708518982\n",
      "Iteration: 985, Loss: 1.270756721496582\n",
      "Iteration: 986, Loss: 1.2707486152648926\n",
      "Iteration: 987, Loss: 1.2707403898239136\n",
      "Iteration: 988, Loss: 1.2707321643829346\n",
      "Iteration: 989, Loss: 1.2707239389419556\n",
      "Iteration: 990, Loss: 1.2707157135009766\n",
      "Iteration: 991, Loss: 1.270707368850708\n",
      "Iteration: 992, Loss: 1.2706990242004395\n",
      "Iteration: 993, Loss: 1.2706905603408813\n",
      "Iteration: 994, Loss: 1.2706822156906128\n",
      "Iteration: 995, Loss: 1.2706737518310547\n",
      "Iteration: 996, Loss: 1.2706652879714966\n",
      "Iteration: 997, Loss: 1.2706568241119385\n",
      "Iteration: 998, Loss: 1.2706483602523804\n",
      "Iteration: 999, Loss: 1.2706398963928223\n",
      "Iteration: 1000, Loss: 1.2706313133239746\n",
      "Iteration: 1001, Loss: 1.2706228494644165\n",
      "Iteration: 1002, Loss: 1.2706142663955688\n",
      "Iteration: 1003, Loss: 1.2706058025360107\n",
      "Iteration: 1004, Loss: 1.270597219467163\n",
      "Iteration: 1005, Loss: 1.2705886363983154\n",
      "Iteration: 1006, Loss: 1.2705801725387573\n",
      "Iteration: 1007, Loss: 1.2705715894699097\n",
      "Iteration: 1008, Loss: 1.270563006401062\n",
      "Iteration: 1009, Loss: 1.2705544233322144\n",
      "Iteration: 1010, Loss: 1.2705458402633667\n",
      "Iteration: 1011, Loss: 1.270537257194519\n",
      "Iteration: 1012, Loss: 1.270528793334961\n",
      "Iteration: 1013, Loss: 1.2705202102661133\n",
      "Iteration: 1014, Loss: 1.2705116271972656\n",
      "Iteration: 1015, Loss: 1.270503044128418\n",
      "Iteration: 1016, Loss: 1.2704944610595703\n",
      "Iteration: 1017, Loss: 1.2704858779907227\n",
      "Iteration: 1018, Loss: 1.270477294921875\n",
      "Iteration: 1019, Loss: 1.2704687118530273\n",
      "Iteration: 1020, Loss: 1.2704602479934692\n",
      "Iteration: 1021, Loss: 1.2704516649246216\n",
      "Iteration: 1022, Loss: 1.270443081855774\n",
      "Iteration: 1023, Loss: 1.2704344987869263\n",
      "Iteration: 1024, Loss: 1.2704259157180786\n",
      "Iteration: 1025, Loss: 1.2704174518585205\n",
      "Iteration: 1026, Loss: 1.2704088687896729\n",
      "Iteration: 1027, Loss: 1.2704002857208252\n",
      "Iteration: 1028, Loss: 1.2703917026519775\n",
      "Iteration: 1029, Loss: 1.2703832387924194\n",
      "Iteration: 1030, Loss: 1.2703746557235718\n",
      "Iteration: 1031, Loss: 1.2703660726547241\n",
      "Iteration: 1032, Loss: 1.270357608795166\n",
      "Iteration: 1033, Loss: 1.2703490257263184\n",
      "Iteration: 1034, Loss: 1.2703404426574707\n",
      "Iteration: 1035, Loss: 1.2703319787979126\n",
      "Iteration: 1036, Loss: 1.270323395729065\n",
      "Iteration: 1037, Loss: 1.2703149318695068\n",
      "Iteration: 1038, Loss: 1.2703063488006592\n",
      "Iteration: 1039, Loss: 1.270297884941101\n",
      "Iteration: 1040, Loss: 1.2702893018722534\n",
      "Iteration: 1041, Loss: 1.2702808380126953\n",
      "Iteration: 1042, Loss: 1.2702723741531372\n",
      "Iteration: 1043, Loss: 1.2702637910842896\n",
      "Iteration: 1044, Loss: 1.2702553272247314\n",
      "Iteration: 1045, Loss: 1.2702467441558838\n",
      "Iteration: 1046, Loss: 1.2702382802963257\n",
      "Iteration: 1047, Loss: 1.2702298164367676\n",
      "Iteration: 1048, Loss: 1.2702213525772095\n",
      "Iteration: 1049, Loss: 1.2702127695083618\n",
      "Iteration: 1050, Loss: 1.2702043056488037\n",
      "Iteration: 1051, Loss: 1.2701958417892456\n",
      "Iteration: 1052, Loss: 1.2701873779296875\n",
      "Iteration: 1053, Loss: 1.2701789140701294\n",
      "Iteration: 1054, Loss: 1.2701703310012817\n",
      "Iteration: 1055, Loss: 1.2701618671417236\n",
      "Iteration: 1056, Loss: 1.2701534032821655\n",
      "Iteration: 1057, Loss: 1.2701449394226074\n",
      "Iteration: 1058, Loss: 1.2701364755630493\n",
      "Iteration: 1059, Loss: 1.2701280117034912\n",
      "Iteration: 1060, Loss: 1.270119547843933\n",
      "Iteration: 1061, Loss: 1.270111083984375\n",
      "Iteration: 1062, Loss: 1.270102620124817\n",
      "Iteration: 1063, Loss: 1.2700941562652588\n",
      "Iteration: 1064, Loss: 1.2700858116149902\n",
      "Iteration: 1065, Loss: 1.2700773477554321\n",
      "Iteration: 1066, Loss: 1.270068883895874\n",
      "Iteration: 1067, Loss: 1.270060420036316\n",
      "Iteration: 1068, Loss: 1.2700519561767578\n",
      "Iteration: 1069, Loss: 1.2700436115264893\n",
      "Iteration: 1070, Loss: 1.2700351476669312\n",
      "Iteration: 1071, Loss: 1.270026683807373\n",
      "Iteration: 1072, Loss: 1.270018219947815\n",
      "Iteration: 1073, Loss: 1.2700098752975464\n",
      "Iteration: 1074, Loss: 1.2700014114379883\n",
      "Iteration: 1075, Loss: 1.2699929475784302\n",
      "Iteration: 1076, Loss: 1.2699846029281616\n",
      "Iteration: 1077, Loss: 1.2699761390686035\n",
      "Iteration: 1078, Loss: 1.269967794418335\n",
      "Iteration: 1079, Loss: 1.2699593305587769\n",
      "Iteration: 1080, Loss: 1.2699509859085083\n",
      "Iteration: 1081, Loss: 1.2699425220489502\n",
      "Iteration: 1082, Loss: 1.2699341773986816\n",
      "Iteration: 1083, Loss: 1.2699257135391235\n",
      "Iteration: 1084, Loss: 1.269917368888855\n",
      "Iteration: 1085, Loss: 1.2699089050292969\n",
      "Iteration: 1086, Loss: 1.2699005603790283\n",
      "Iteration: 1087, Loss: 1.2698922157287598\n",
      "Iteration: 1088, Loss: 1.2698837518692017\n",
      "Iteration: 1089, Loss: 1.269875407218933\n",
      "Iteration: 1090, Loss: 1.2698670625686646\n",
      "Iteration: 1091, Loss: 1.269858717918396\n",
      "Iteration: 1092, Loss: 1.269850254058838\n",
      "Iteration: 1093, Loss: 1.2698419094085693\n",
      "Iteration: 1094, Loss: 1.2698335647583008\n",
      "Iteration: 1095, Loss: 1.2698252201080322\n",
      "Iteration: 1096, Loss: 1.2698168754577637\n",
      "Iteration: 1097, Loss: 1.2698085308074951\n",
      "Iteration: 1098, Loss: 1.2698001861572266\n",
      "Iteration: 1099, Loss: 1.269791841506958\n",
      "Iteration: 1100, Loss: 1.2697834968566895\n",
      "Iteration: 1101, Loss: 1.269775152206421\n",
      "Iteration: 1102, Loss: 1.2697668075561523\n",
      "Iteration: 1103, Loss: 1.2697584629058838\n",
      "Iteration: 1104, Loss: 1.2697501182556152\n",
      "Iteration: 1105, Loss: 1.2697417736053467\n",
      "Iteration: 1106, Loss: 1.2697334289550781\n",
      "Iteration: 1107, Loss: 1.2697250843048096\n",
      "Iteration: 1108, Loss: 1.269716739654541\n",
      "Iteration: 1109, Loss: 1.2697083950042725\n",
      "Iteration: 1110, Loss: 1.269700050354004\n",
      "Iteration: 1111, Loss: 1.269691824913025\n",
      "Iteration: 1112, Loss: 1.2696834802627563\n",
      "Iteration: 1113, Loss: 1.2696751356124878\n",
      "Iteration: 1114, Loss: 1.2696667909622192\n",
      "Iteration: 1115, Loss: 1.2696585655212402\n",
      "Iteration: 1116, Loss: 1.2696502208709717\n",
      "Iteration: 1117, Loss: 1.2696418762207031\n",
      "Iteration: 1118, Loss: 1.2696336507797241\n",
      "Iteration: 1119, Loss: 1.2696253061294556\n",
      "Iteration: 1120, Loss: 1.2696170806884766\n",
      "Iteration: 1121, Loss: 1.269608736038208\n",
      "Iteration: 1122, Loss: 1.2696003913879395\n",
      "Iteration: 1123, Loss: 1.2695921659469604\n",
      "Iteration: 1124, Loss: 1.269583821296692\n",
      "Iteration: 1125, Loss: 1.269575595855713\n",
      "Iteration: 1126, Loss: 1.2695673704147339\n",
      "Iteration: 1127, Loss: 1.2695590257644653\n",
      "Iteration: 1128, Loss: 1.2695508003234863\n",
      "Iteration: 1129, Loss: 1.2695424556732178\n",
      "Iteration: 1130, Loss: 1.2695342302322388\n",
      "Iteration: 1131, Loss: 1.2695260047912598\n",
      "Iteration: 1132, Loss: 1.2695176601409912\n",
      "Iteration: 1133, Loss: 1.2695094347000122\n",
      "Iteration: 1134, Loss: 1.2695012092590332\n",
      "Iteration: 1135, Loss: 1.2694929838180542\n",
      "Iteration: 1136, Loss: 1.2694846391677856\n",
      "Iteration: 1137, Loss: 1.2694764137268066\n",
      "Iteration: 1138, Loss: 1.2694681882858276\n",
      "Iteration: 1139, Loss: 1.2694599628448486\n",
      "Iteration: 1140, Loss: 1.2694517374038696\n",
      "Iteration: 1141, Loss: 1.2694435119628906\n",
      "Iteration: 1142, Loss: 1.269435167312622\n",
      "Iteration: 1143, Loss: 1.269426941871643\n",
      "Iteration: 1144, Loss: 1.269418716430664\n",
      "Iteration: 1145, Loss: 1.269410490989685\n",
      "Iteration: 1146, Loss: 1.269402265548706\n",
      "Iteration: 1147, Loss: 1.269394040107727\n",
      "Iteration: 1148, Loss: 1.269385814666748\n",
      "Iteration: 1149, Loss: 1.269377589225769\n",
      "Iteration: 1150, Loss: 1.26936936378479\n",
      "Iteration: 1151, Loss: 1.2693612575531006\n",
      "Iteration: 1152, Loss: 1.2693530321121216\n",
      "Iteration: 1153, Loss: 1.2693448066711426\n",
      "Iteration: 1154, Loss: 1.2693365812301636\n",
      "Iteration: 1155, Loss: 1.2693283557891846\n",
      "Iteration: 1156, Loss: 1.2693201303482056\n",
      "Iteration: 1157, Loss: 1.2693120241165161\n",
      "Iteration: 1158, Loss: 1.269303798675537\n",
      "Iteration: 1159, Loss: 1.269295573234558\n",
      "Iteration: 1160, Loss: 1.269287347793579\n",
      "Iteration: 1161, Loss: 1.2692792415618896\n",
      "Iteration: 1162, Loss: 1.2692710161209106\n",
      "Iteration: 1163, Loss: 1.2692627906799316\n",
      "Iteration: 1164, Loss: 1.2692546844482422\n",
      "Iteration: 1165, Loss: 1.2692464590072632\n",
      "Iteration: 1166, Loss: 1.2692383527755737\n",
      "Iteration: 1167, Loss: 1.2692301273345947\n",
      "Iteration: 1168, Loss: 1.2692220211029053\n",
      "Iteration: 1169, Loss: 1.2692137956619263\n",
      "Iteration: 1170, Loss: 1.2692056894302368\n",
      "Iteration: 1171, Loss: 1.2691974639892578\n",
      "Iteration: 1172, Loss: 1.2691893577575684\n",
      "Iteration: 1173, Loss: 1.2691811323165894\n",
      "Iteration: 1174, Loss: 1.2691730260849\n",
      "Iteration: 1175, Loss: 1.269164800643921\n",
      "Iteration: 1176, Loss: 1.2691566944122314\n",
      "Iteration: 1177, Loss: 1.269148588180542\n",
      "Iteration: 1178, Loss: 1.269140362739563\n",
      "Iteration: 1179, Loss: 1.2691322565078735\n",
      "Iteration: 1180, Loss: 1.269124150276184\n",
      "Iteration: 1181, Loss: 1.2691160440444946\n",
      "Iteration: 1182, Loss: 1.2691078186035156\n",
      "Iteration: 1183, Loss: 1.2690997123718262\n",
      "Iteration: 1184, Loss: 1.2690916061401367\n",
      "Iteration: 1185, Loss: 1.2690834999084473\n",
      "Iteration: 1186, Loss: 1.2690753936767578\n",
      "Iteration: 1187, Loss: 1.2690672874450684\n",
      "Iteration: 1188, Loss: 1.2690590620040894\n",
      "Iteration: 1189, Loss: 1.2690509557724\n",
      "Iteration: 1190, Loss: 1.2690428495407104\n",
      "Iteration: 1191, Loss: 1.269034743309021\n",
      "Iteration: 1192, Loss: 1.2690266370773315\n",
      "Iteration: 1193, Loss: 1.269018530845642\n",
      "Iteration: 1194, Loss: 1.2690104246139526\n",
      "Iteration: 1195, Loss: 1.2690023183822632\n",
      "Iteration: 1196, Loss: 1.2689942121505737\n",
      "Iteration: 1197, Loss: 1.2689861059188843\n",
      "Iteration: 1198, Loss: 1.2689779996871948\n",
      "Iteration: 1199, Loss: 1.268970012664795\n",
      "Iteration: 1200, Loss: 1.2689619064331055\n",
      "Iteration: 1201, Loss: 1.268953800201416\n",
      "Iteration: 1202, Loss: 1.2689456939697266\n",
      "Iteration: 1203, Loss: 1.268937587738037\n",
      "Iteration: 1204, Loss: 1.2689294815063477\n",
      "Iteration: 1205, Loss: 1.2689214944839478\n",
      "Iteration: 1206, Loss: 1.2689133882522583\n",
      "Iteration: 1207, Loss: 1.2689052820205688\n",
      "Iteration: 1208, Loss: 1.268897294998169\n",
      "Iteration: 1209, Loss: 1.2688891887664795\n",
      "Iteration: 1210, Loss: 1.26888108253479\n",
      "Iteration: 1211, Loss: 1.2688730955123901\n",
      "Iteration: 1212, Loss: 1.2688649892807007\n",
      "Iteration: 1213, Loss: 1.2688568830490112\n",
      "Iteration: 1214, Loss: 1.2688488960266113\n",
      "Iteration: 1215, Loss: 1.2688407897949219\n",
      "Iteration: 1216, Loss: 1.268832802772522\n",
      "Iteration: 1217, Loss: 1.2688246965408325\n",
      "Iteration: 1218, Loss: 1.2688167095184326\n",
      "Iteration: 1219, Loss: 1.2688086032867432\n",
      "Iteration: 1220, Loss: 1.2688006162643433\n",
      "Iteration: 1221, Loss: 1.2687925100326538\n",
      "Iteration: 1222, Loss: 1.268784523010254\n",
      "Iteration: 1223, Loss: 1.268776535987854\n",
      "Iteration: 1224, Loss: 1.2687684297561646\n",
      "Iteration: 1225, Loss: 1.2687604427337646\n",
      "Iteration: 1226, Loss: 1.2687523365020752\n",
      "Iteration: 1227, Loss: 1.2687443494796753\n",
      "Iteration: 1228, Loss: 1.2687363624572754\n",
      "Iteration: 1229, Loss: 1.2687283754348755\n",
      "Iteration: 1230, Loss: 1.268720269203186\n",
      "Iteration: 1231, Loss: 1.2687122821807861\n",
      "Iteration: 1232, Loss: 1.2687042951583862\n",
      "Iteration: 1233, Loss: 1.2686963081359863\n",
      "Iteration: 1234, Loss: 1.2686883211135864\n",
      "Iteration: 1235, Loss: 1.268680214881897\n",
      "Iteration: 1236, Loss: 1.268672227859497\n",
      "Iteration: 1237, Loss: 1.2686642408370972\n",
      "Iteration: 1238, Loss: 1.2686562538146973\n",
      "Iteration: 1239, Loss: 1.2686482667922974\n",
      "Iteration: 1240, Loss: 1.2686402797698975\n",
      "Iteration: 1241, Loss: 1.2686322927474976\n",
      "Iteration: 1242, Loss: 1.2686243057250977\n",
      "Iteration: 1243, Loss: 1.2686163187026978\n",
      "Iteration: 1244, Loss: 1.2686083316802979\n",
      "Iteration: 1245, Loss: 1.268600344657898\n",
      "Iteration: 1246, Loss: 1.268592357635498\n",
      "Iteration: 1247, Loss: 1.2685843706130981\n",
      "Iteration: 1248, Loss: 1.2685763835906982\n",
      "Iteration: 1249, Loss: 1.2685683965682983\n",
      "Iteration: 1250, Loss: 1.2685604095458984\n",
      "Iteration: 1251, Loss: 1.268552541732788\n",
      "Iteration: 1252, Loss: 1.2685445547103882\n",
      "Iteration: 1253, Loss: 1.2685365676879883\n",
      "Iteration: 1254, Loss: 1.2685285806655884\n",
      "Iteration: 1255, Loss: 1.2685205936431885\n",
      "Iteration: 1256, Loss: 1.2685127258300781\n",
      "Iteration: 1257, Loss: 1.2685047388076782\n",
      "Iteration: 1258, Loss: 1.2684967517852783\n",
      "Iteration: 1259, Loss: 1.268488883972168\n",
      "Iteration: 1260, Loss: 1.268480896949768\n",
      "Iteration: 1261, Loss: 1.2684729099273682\n",
      "Iteration: 1262, Loss: 1.2684650421142578\n",
      "Iteration: 1263, Loss: 1.268457055091858\n",
      "Iteration: 1264, Loss: 1.2684491872787476\n",
      "Iteration: 1265, Loss: 1.2684412002563477\n",
      "Iteration: 1266, Loss: 1.2684332132339478\n",
      "Iteration: 1267, Loss: 1.2684253454208374\n",
      "Iteration: 1268, Loss: 1.2684173583984375\n",
      "Iteration: 1269, Loss: 1.2684094905853271\n",
      "Iteration: 1270, Loss: 1.2684016227722168\n",
      "Iteration: 1271, Loss: 1.268393635749817\n",
      "Iteration: 1272, Loss: 1.2683857679367065\n",
      "Iteration: 1273, Loss: 1.2683777809143066\n",
      "Iteration: 1274, Loss: 1.2683699131011963\n",
      "Iteration: 1275, Loss: 1.2683619260787964\n",
      "Iteration: 1276, Loss: 1.268354058265686\n",
      "Iteration: 1277, Loss: 1.2683461904525757\n",
      "Iteration: 1278, Loss: 1.2683382034301758\n",
      "Iteration: 1279, Loss: 1.2683303356170654\n",
      "Iteration: 1280, Loss: 1.268322467803955\n",
      "Iteration: 1281, Loss: 1.2683145999908447\n",
      "Iteration: 1282, Loss: 1.2683066129684448\n",
      "Iteration: 1283, Loss: 1.2682987451553345\n",
      "Iteration: 1284, Loss: 1.2682908773422241\n",
      "Iteration: 1285, Loss: 1.2682830095291138\n",
      "Iteration: 1286, Loss: 1.2682751417160034\n",
      "Iteration: 1287, Loss: 1.268267273902893\n",
      "Iteration: 1288, Loss: 1.2682592868804932\n",
      "Iteration: 1289, Loss: 1.2682514190673828\n",
      "Iteration: 1290, Loss: 1.2682435512542725\n",
      "Iteration: 1291, Loss: 1.268235683441162\n",
      "Iteration: 1292, Loss: 1.2682278156280518\n",
      "Iteration: 1293, Loss: 1.2682199478149414\n",
      "Iteration: 1294, Loss: 1.268212080001831\n",
      "Iteration: 1295, Loss: 1.2682042121887207\n",
      "Iteration: 1296, Loss: 1.2681963443756104\n",
      "Iteration: 1297, Loss: 1.2681884765625\n",
      "Iteration: 1298, Loss: 1.2681806087493896\n",
      "Iteration: 1299, Loss: 1.2681727409362793\n",
      "Iteration: 1300, Loss: 1.2681649923324585\n",
      "Iteration: 1301, Loss: 1.2681571245193481\n",
      "Iteration: 1302, Loss: 1.2681492567062378\n",
      "Iteration: 1303, Loss: 1.2681413888931274\n",
      "Iteration: 1304, Loss: 1.268133521080017\n",
      "Iteration: 1305, Loss: 1.2681256532669067\n",
      "Iteration: 1306, Loss: 1.268117904663086\n",
      "Iteration: 1307, Loss: 1.2681100368499756\n",
      "Iteration: 1308, Loss: 1.2681021690368652\n",
      "Iteration: 1309, Loss: 1.2680943012237549\n",
      "Iteration: 1310, Loss: 1.268086552619934\n",
      "Iteration: 1311, Loss: 1.2680786848068237\n",
      "Iteration: 1312, Loss: 1.2680708169937134\n",
      "Iteration: 1313, Loss: 1.2680630683898926\n",
      "Iteration: 1314, Loss: 1.2680552005767822\n",
      "Iteration: 1315, Loss: 1.2680473327636719\n",
      "Iteration: 1316, Loss: 1.268039584159851\n",
      "Iteration: 1317, Loss: 1.2680317163467407\n",
      "Iteration: 1318, Loss: 1.26802396774292\n",
      "Iteration: 1319, Loss: 1.2680160999298096\n",
      "Iteration: 1320, Loss: 1.2680083513259888\n",
      "Iteration: 1321, Loss: 1.2680004835128784\n",
      "Iteration: 1322, Loss: 1.2679927349090576\n",
      "Iteration: 1323, Loss: 1.2679848670959473\n",
      "Iteration: 1324, Loss: 1.2679771184921265\n",
      "Iteration: 1325, Loss: 1.2679692506790161\n",
      "Iteration: 1326, Loss: 1.2679615020751953\n",
      "Iteration: 1327, Loss: 1.2679537534713745\n",
      "Iteration: 1328, Loss: 1.2679458856582642\n",
      "Iteration: 1329, Loss: 1.2679381370544434\n",
      "Iteration: 1330, Loss: 1.2679303884506226\n",
      "Iteration: 1331, Loss: 1.2679225206375122\n",
      "Iteration: 1332, Loss: 1.2679147720336914\n",
      "Iteration: 1333, Loss: 1.2679070234298706\n",
      "Iteration: 1334, Loss: 1.2678992748260498\n",
      "Iteration: 1335, Loss: 1.2678914070129395\n",
      "Iteration: 1336, Loss: 1.2678836584091187\n",
      "Iteration: 1337, Loss: 1.2678759098052979\n",
      "Iteration: 1338, Loss: 1.267868161201477\n",
      "Iteration: 1339, Loss: 1.2678604125976562\n",
      "Iteration: 1340, Loss: 1.267852544784546\n",
      "Iteration: 1341, Loss: 1.267844796180725\n",
      "Iteration: 1342, Loss: 1.2678370475769043\n",
      "Iteration: 1343, Loss: 1.2678292989730835\n",
      "Iteration: 1344, Loss: 1.2678215503692627\n",
      "Iteration: 1345, Loss: 1.267813801765442\n",
      "Iteration: 1346, Loss: 1.267806053161621\n",
      "Iteration: 1347, Loss: 1.2677983045578003\n",
      "Iteration: 1348, Loss: 1.2677905559539795\n",
      "Iteration: 1349, Loss: 1.2677828073501587\n",
      "Iteration: 1350, Loss: 1.267775058746338\n",
      "Iteration: 1351, Loss: 1.267767310142517\n",
      "Iteration: 1352, Loss: 1.2677595615386963\n",
      "Iteration: 1353, Loss: 1.2677518129348755\n",
      "Iteration: 1354, Loss: 1.2677440643310547\n",
      "Iteration: 1355, Loss: 1.2677364349365234\n",
      "Iteration: 1356, Loss: 1.2677286863327026\n",
      "Iteration: 1357, Loss: 1.2677209377288818\n",
      "Iteration: 1358, Loss: 1.267713189125061\n",
      "Iteration: 1359, Loss: 1.2677054405212402\n",
      "Iteration: 1360, Loss: 1.2676976919174194\n",
      "Iteration: 1361, Loss: 1.2676900625228882\n",
      "Iteration: 1362, Loss: 1.2676823139190674\n",
      "Iteration: 1363, Loss: 1.2676745653152466\n",
      "Iteration: 1364, Loss: 1.2676669359207153\n",
      "Iteration: 1365, Loss: 1.2676591873168945\n",
      "Iteration: 1366, Loss: 1.2676514387130737\n",
      "Iteration: 1367, Loss: 1.2676438093185425\n",
      "Iteration: 1368, Loss: 1.2676360607147217\n",
      "Iteration: 1369, Loss: 1.2676283121109009\n",
      "Iteration: 1370, Loss: 1.2676206827163696\n",
      "Iteration: 1371, Loss: 1.2676129341125488\n",
      "Iteration: 1372, Loss: 1.2676053047180176\n",
      "Iteration: 1373, Loss: 1.2675975561141968\n",
      "Iteration: 1374, Loss: 1.2675899267196655\n",
      "Iteration: 1375, Loss: 1.2675821781158447\n",
      "Iteration: 1376, Loss: 1.2675745487213135\n",
      "Iteration: 1377, Loss: 1.2675668001174927\n",
      "Iteration: 1378, Loss: 1.2675591707229614\n",
      "Iteration: 1379, Loss: 1.2675514221191406\n",
      "Iteration: 1380, Loss: 1.2675437927246094\n",
      "Iteration: 1381, Loss: 1.2675360441207886\n",
      "Iteration: 1382, Loss: 1.2675284147262573\n",
      "Iteration: 1383, Loss: 1.267520785331726\n",
      "Iteration: 1384, Loss: 1.2675130367279053\n",
      "Iteration: 1385, Loss: 1.267505407333374\n",
      "Iteration: 1386, Loss: 1.2674977779388428\n",
      "Iteration: 1387, Loss: 1.267490029335022\n",
      "Iteration: 1388, Loss: 1.2674823999404907\n",
      "Iteration: 1389, Loss: 1.2674747705459595\n",
      "Iteration: 1390, Loss: 1.2674671411514282\n",
      "Iteration: 1391, Loss: 1.267459511756897\n",
      "Iteration: 1392, Loss: 1.2674517631530762\n",
      "Iteration: 1393, Loss: 1.267444133758545\n",
      "Iteration: 1394, Loss: 1.2674365043640137\n",
      "Iteration: 1395, Loss: 1.2674288749694824\n",
      "Iteration: 1396, Loss: 1.2674212455749512\n",
      "Iteration: 1397, Loss: 1.26741361618042\n",
      "Iteration: 1398, Loss: 1.2674059867858887\n",
      "Iteration: 1399, Loss: 1.2673982381820679\n",
      "Iteration: 1400, Loss: 1.2673906087875366\n",
      "Iteration: 1401, Loss: 1.2673829793930054\n",
      "Iteration: 1402, Loss: 1.2673753499984741\n",
      "Iteration: 1403, Loss: 1.2673677206039429\n",
      "Iteration: 1404, Loss: 1.2673600912094116\n",
      "Iteration: 1405, Loss: 1.2673524618148804\n",
      "Iteration: 1406, Loss: 1.2673449516296387\n",
      "Iteration: 1407, Loss: 1.2673373222351074\n",
      "Iteration: 1408, Loss: 1.2673296928405762\n",
      "Iteration: 1409, Loss: 1.267322063446045\n",
      "Iteration: 1410, Loss: 1.2673144340515137\n",
      "Iteration: 1411, Loss: 1.2673068046569824\n",
      "Iteration: 1412, Loss: 1.2672991752624512\n",
      "Iteration: 1413, Loss: 1.26729154586792\n",
      "Iteration: 1414, Loss: 1.2672840356826782\n",
      "Iteration: 1415, Loss: 1.267276406288147\n",
      "Iteration: 1416, Loss: 1.2672687768936157\n",
      "Iteration: 1417, Loss: 1.2672611474990845\n",
      "Iteration: 1418, Loss: 1.2672536373138428\n",
      "Iteration: 1419, Loss: 1.2672460079193115\n",
      "Iteration: 1420, Loss: 1.2672383785247803\n",
      "Iteration: 1421, Loss: 1.267230749130249\n",
      "Iteration: 1422, Loss: 1.2672232389450073\n",
      "Iteration: 1423, Loss: 1.267215609550476\n",
      "Iteration: 1424, Loss: 1.2672080993652344\n",
      "Iteration: 1425, Loss: 1.2672004699707031\n",
      "Iteration: 1426, Loss: 1.2671928405761719\n",
      "Iteration: 1427, Loss: 1.2671853303909302\n",
      "Iteration: 1428, Loss: 1.267177700996399\n",
      "Iteration: 1429, Loss: 1.2671701908111572\n",
      "Iteration: 1430, Loss: 1.267162561416626\n",
      "Iteration: 1431, Loss: 1.2671550512313843\n",
      "Iteration: 1432, Loss: 1.267147421836853\n",
      "Iteration: 1433, Loss: 1.2671399116516113\n",
      "Iteration: 1434, Loss: 1.26713228225708\n",
      "Iteration: 1435, Loss: 1.2671247720718384\n",
      "Iteration: 1436, Loss: 1.2671171426773071\n",
      "Iteration: 1437, Loss: 1.2671096324920654\n",
      "Iteration: 1438, Loss: 1.2671021223068237\n",
      "Iteration: 1439, Loss: 1.2670944929122925\n",
      "Iteration: 1440, Loss: 1.2670869827270508\n",
      "Iteration: 1441, Loss: 1.267079472541809\n",
      "Iteration: 1442, Loss: 1.2670718431472778\n",
      "Iteration: 1443, Loss: 1.2670643329620361\n",
      "Iteration: 1444, Loss: 1.2670568227767944\n",
      "Iteration: 1445, Loss: 1.2670493125915527\n",
      "Iteration: 1446, Loss: 1.2670416831970215\n",
      "Iteration: 1447, Loss: 1.2670341730117798\n",
      "Iteration: 1448, Loss: 1.267026662826538\n",
      "Iteration: 1449, Loss: 1.2670191526412964\n",
      "Iteration: 1450, Loss: 1.2670116424560547\n",
      "Iteration: 1451, Loss: 1.2670040130615234\n",
      "Iteration: 1452, Loss: 1.2669965028762817\n",
      "Iteration: 1453, Loss: 1.26698899269104\n",
      "Iteration: 1454, Loss: 1.2669814825057983\n",
      "Iteration: 1455, Loss: 1.2669739723205566\n",
      "Iteration: 1456, Loss: 1.266966462135315\n",
      "Iteration: 1457, Loss: 1.2669589519500732\n",
      "Iteration: 1458, Loss: 1.2669514417648315\n",
      "Iteration: 1459, Loss: 1.2669439315795898\n",
      "Iteration: 1460, Loss: 1.2669364213943481\n",
      "Iteration: 1461, Loss: 1.2669289112091064\n",
      "Iteration: 1462, Loss: 1.2669214010238647\n",
      "Iteration: 1463, Loss: 1.266913890838623\n",
      "Iteration: 1464, Loss: 1.2669063806533813\n",
      "Iteration: 1465, Loss: 1.2668988704681396\n",
      "Iteration: 1466, Loss: 1.266891360282898\n",
      "Iteration: 1467, Loss: 1.2668838500976562\n",
      "Iteration: 1468, Loss: 1.2668763399124146\n",
      "Iteration: 1469, Loss: 1.2668689489364624\n",
      "Iteration: 1470, Loss: 1.2668614387512207\n",
      "Iteration: 1471, Loss: 1.266853928565979\n",
      "Iteration: 1472, Loss: 1.2668464183807373\n",
      "Iteration: 1473, Loss: 1.2668389081954956\n",
      "Iteration: 1474, Loss: 1.2668315172195435\n",
      "Iteration: 1475, Loss: 1.2668240070343018\n",
      "Iteration: 1476, Loss: 1.26681649684906\n",
      "Iteration: 1477, Loss: 1.266809105873108\n",
      "Iteration: 1478, Loss: 1.2668015956878662\n",
      "Iteration: 1479, Loss: 1.2667940855026245\n",
      "Iteration: 1480, Loss: 1.2667866945266724\n",
      "Iteration: 1481, Loss: 1.2667791843414307\n",
      "Iteration: 1482, Loss: 1.266771674156189\n",
      "Iteration: 1483, Loss: 1.2667642831802368\n",
      "Iteration: 1484, Loss: 1.2667567729949951\n",
      "Iteration: 1485, Loss: 1.266749382019043\n",
      "Iteration: 1486, Loss: 1.2667418718338013\n",
      "Iteration: 1487, Loss: 1.2667343616485596\n",
      "Iteration: 1488, Loss: 1.2667269706726074\n",
      "Iteration: 1489, Loss: 1.2667194604873657\n",
      "Iteration: 1490, Loss: 1.2667120695114136\n",
      "Iteration: 1491, Loss: 1.2667046785354614\n",
      "Iteration: 1492, Loss: 1.2666971683502197\n",
      "Iteration: 1493, Loss: 1.2666897773742676\n",
      "Iteration: 1494, Loss: 1.2666822671890259\n",
      "Iteration: 1495, Loss: 1.2666748762130737\n",
      "Iteration: 1496, Loss: 1.266667366027832\n",
      "Iteration: 1497, Loss: 1.2666599750518799\n",
      "Iteration: 1498, Loss: 1.2666525840759277\n",
      "Iteration: 1499, Loss: 1.266645073890686\n",
      "Iteration: 1500, Loss: 1.2666376829147339\n",
      "Iteration: 1501, Loss: 1.2666302919387817\n",
      "Iteration: 1502, Loss: 1.2666229009628296\n",
      "Iteration: 1503, Loss: 1.266615390777588\n",
      "Iteration: 1504, Loss: 1.2666079998016357\n",
      "Iteration: 1505, Loss: 1.2666006088256836\n",
      "Iteration: 1506, Loss: 1.2665932178497314\n",
      "Iteration: 1507, Loss: 1.2665858268737793\n",
      "Iteration: 1508, Loss: 1.2665783166885376\n",
      "Iteration: 1509, Loss: 1.2665709257125854\n",
      "Iteration: 1510, Loss: 1.2665635347366333\n",
      "Iteration: 1511, Loss: 1.2665561437606812\n",
      "Iteration: 1512, Loss: 1.266548752784729\n",
      "Iteration: 1513, Loss: 1.2665413618087769\n",
      "Iteration: 1514, Loss: 1.2665339708328247\n",
      "Iteration: 1515, Loss: 1.2665265798568726\n",
      "Iteration: 1516, Loss: 1.2665191888809204\n",
      "Iteration: 1517, Loss: 1.2665117979049683\n",
      "Iteration: 1518, Loss: 1.2665044069290161\n",
      "Iteration: 1519, Loss: 1.266497015953064\n",
      "Iteration: 1520, Loss: 1.2664896249771118\n",
      "Iteration: 1521, Loss: 1.2664822340011597\n",
      "Iteration: 1522, Loss: 1.2664748430252075\n",
      "Iteration: 1523, Loss: 1.2664674520492554\n",
      "Iteration: 1524, Loss: 1.2664600610733032\n",
      "Iteration: 1525, Loss: 1.266452670097351\n",
      "Iteration: 1526, Loss: 1.266445279121399\n",
      "Iteration: 1527, Loss: 1.2664378881454468\n",
      "Iteration: 1528, Loss: 1.2664306163787842\n",
      "Iteration: 1529, Loss: 1.266423225402832\n",
      "Iteration: 1530, Loss: 1.2664158344268799\n",
      "Iteration: 1531, Loss: 1.2664084434509277\n",
      "Iteration: 1532, Loss: 1.2664010524749756\n",
      "Iteration: 1533, Loss: 1.266393780708313\n",
      "Iteration: 1534, Loss: 1.2663863897323608\n",
      "Iteration: 1535, Loss: 1.2663789987564087\n",
      "Iteration: 1536, Loss: 1.2663716077804565\n",
      "Iteration: 1537, Loss: 1.266364336013794\n",
      "Iteration: 1538, Loss: 1.2663569450378418\n",
      "Iteration: 1539, Loss: 1.2663496732711792\n",
      "Iteration: 1540, Loss: 1.266342282295227\n",
      "Iteration: 1541, Loss: 1.266334891319275\n",
      "Iteration: 1542, Loss: 1.2663276195526123\n",
      "Iteration: 1543, Loss: 1.2663202285766602\n",
      "Iteration: 1544, Loss: 1.2663129568099976\n",
      "Iteration: 1545, Loss: 1.2663055658340454\n",
      "Iteration: 1546, Loss: 1.2662981748580933\n",
      "Iteration: 1547, Loss: 1.2662909030914307\n",
      "Iteration: 1548, Loss: 1.2662835121154785\n",
      "Iteration: 1549, Loss: 1.266276240348816\n",
      "Iteration: 1550, Loss: 1.2662689685821533\n",
      "Iteration: 1551, Loss: 1.2662615776062012\n",
      "Iteration: 1552, Loss: 1.2662543058395386\n",
      "Iteration: 1553, Loss: 1.2662469148635864\n",
      "Iteration: 1554, Loss: 1.2662396430969238\n",
      "Iteration: 1555, Loss: 1.2662322521209717\n",
      "Iteration: 1556, Loss: 1.266224980354309\n",
      "Iteration: 1557, Loss: 1.2662177085876465\n",
      "Iteration: 1558, Loss: 1.2662103176116943\n",
      "Iteration: 1559, Loss: 1.2662030458450317\n",
      "Iteration: 1560, Loss: 1.2661957740783691\n",
      "Iteration: 1561, Loss: 1.2661885023117065\n",
      "Iteration: 1562, Loss: 1.2661811113357544\n",
      "Iteration: 1563, Loss: 1.2661738395690918\n",
      "Iteration: 1564, Loss: 1.2661665678024292\n",
      "Iteration: 1565, Loss: 1.2661592960357666\n",
      "Iteration: 1566, Loss: 1.2661519050598145\n",
      "Iteration: 1567, Loss: 1.2661446332931519\n",
      "Iteration: 1568, Loss: 1.2661373615264893\n",
      "Iteration: 1569, Loss: 1.2661300897598267\n",
      "Iteration: 1570, Loss: 1.266122817993164\n",
      "Iteration: 1571, Loss: 1.2661155462265015\n",
      "Iteration: 1572, Loss: 1.2661082744598389\n",
      "Iteration: 1573, Loss: 1.2661010026931763\n",
      "Iteration: 1574, Loss: 1.2660937309265137\n",
      "Iteration: 1575, Loss: 1.2660863399505615\n",
      "Iteration: 1576, Loss: 1.266079068183899\n",
      "Iteration: 1577, Loss: 1.2660717964172363\n",
      "Iteration: 1578, Loss: 1.2660645246505737\n",
      "Iteration: 1579, Loss: 1.2660572528839111\n",
      "Iteration: 1580, Loss: 1.266050100326538\n",
      "Iteration: 1581, Loss: 1.2660428285598755\n",
      "Iteration: 1582, Loss: 1.266035556793213\n",
      "Iteration: 1583, Loss: 1.2660282850265503\n",
      "Iteration: 1584, Loss: 1.2660210132598877\n",
      "Iteration: 1585, Loss: 1.266013741493225\n",
      "Iteration: 1586, Loss: 1.2660064697265625\n",
      "Iteration: 1587, Loss: 1.2659991979599\n",
      "Iteration: 1588, Loss: 1.2659919261932373\n",
      "Iteration: 1589, Loss: 1.2659847736358643\n",
      "Iteration: 1590, Loss: 1.2659775018692017\n",
      "Iteration: 1591, Loss: 1.265970230102539\n",
      "Iteration: 1592, Loss: 1.2659629583358765\n",
      "Iteration: 1593, Loss: 1.2659558057785034\n",
      "Iteration: 1594, Loss: 1.2659485340118408\n",
      "Iteration: 1595, Loss: 1.2659412622451782\n",
      "Iteration: 1596, Loss: 1.2659339904785156\n",
      "Iteration: 1597, Loss: 1.2659268379211426\n",
      "Iteration: 1598, Loss: 1.26591956615448\n",
      "Iteration: 1599, Loss: 1.2659122943878174\n",
      "Iteration: 1600, Loss: 1.2659051418304443\n",
      "Iteration: 1601, Loss: 1.2658978700637817\n",
      "Iteration: 1602, Loss: 1.2658907175064087\n",
      "Iteration: 1603, Loss: 1.265883445739746\n",
      "Iteration: 1604, Loss: 1.2658761739730835\n",
      "Iteration: 1605, Loss: 1.2658690214157104\n",
      "Iteration: 1606, Loss: 1.2658617496490479\n",
      "Iteration: 1607, Loss: 1.2658545970916748\n",
      "Iteration: 1608, Loss: 1.2658473253250122\n",
      "Iteration: 1609, Loss: 1.2658401727676392\n",
      "Iteration: 1610, Loss: 1.2658329010009766\n",
      "Iteration: 1611, Loss: 1.2658257484436035\n",
      "Iteration: 1612, Loss: 1.2658185958862305\n",
      "Iteration: 1613, Loss: 1.2658113241195679\n",
      "Iteration: 1614, Loss: 1.2658041715621948\n",
      "Iteration: 1615, Loss: 1.2657968997955322\n",
      "Iteration: 1616, Loss: 1.2657897472381592\n",
      "Iteration: 1617, Loss: 1.2657825946807861\n",
      "Iteration: 1618, Loss: 1.2657753229141235\n",
      "Iteration: 1619, Loss: 1.2657681703567505\n",
      "Iteration: 1620, Loss: 1.2657610177993774\n",
      "Iteration: 1621, Loss: 1.2657537460327148\n",
      "Iteration: 1622, Loss: 1.2657465934753418\n",
      "Iteration: 1623, Loss: 1.2657394409179688\n",
      "Iteration: 1624, Loss: 1.2657322883605957\n",
      "Iteration: 1625, Loss: 1.265725016593933\n",
      "Iteration: 1626, Loss: 1.26571786403656\n",
      "Iteration: 1627, Loss: 1.265710711479187\n",
      "Iteration: 1628, Loss: 1.265703558921814\n",
      "Iteration: 1629, Loss: 1.265696406364441\n",
      "Iteration: 1630, Loss: 1.2656892538070679\n",
      "Iteration: 1631, Loss: 1.2656819820404053\n",
      "Iteration: 1632, Loss: 1.2656748294830322\n",
      "Iteration: 1633, Loss: 1.2656676769256592\n",
      "Iteration: 1634, Loss: 1.2656605243682861\n",
      "Iteration: 1635, Loss: 1.265653371810913\n",
      "Iteration: 1636, Loss: 1.26564621925354\n",
      "Iteration: 1637, Loss: 1.265639066696167\n",
      "Iteration: 1638, Loss: 1.265631914138794\n",
      "Iteration: 1639, Loss: 1.265624761581421\n",
      "Iteration: 1640, Loss: 1.2656176090240479\n",
      "Iteration: 1641, Loss: 1.2656104564666748\n",
      "Iteration: 1642, Loss: 1.2656033039093018\n",
      "Iteration: 1643, Loss: 1.2655961513519287\n",
      "Iteration: 1644, Loss: 1.2655889987945557\n",
      "Iteration: 1645, Loss: 1.2655818462371826\n",
      "Iteration: 1646, Loss: 1.2655748128890991\n",
      "Iteration: 1647, Loss: 1.265567660331726\n",
      "Iteration: 1648, Loss: 1.265560507774353\n",
      "Iteration: 1649, Loss: 1.26555335521698\n",
      "Iteration: 1650, Loss: 1.265546202659607\n",
      "Iteration: 1651, Loss: 1.2655390501022339\n",
      "Iteration: 1652, Loss: 1.2655320167541504\n",
      "Iteration: 1653, Loss: 1.2655248641967773\n",
      "Iteration: 1654, Loss: 1.2655177116394043\n",
      "Iteration: 1655, Loss: 1.2655105590820312\n",
      "Iteration: 1656, Loss: 1.2655035257339478\n",
      "Iteration: 1657, Loss: 1.2654963731765747\n",
      "Iteration: 1658, Loss: 1.2654892206192017\n",
      "Iteration: 1659, Loss: 1.2654821872711182\n",
      "Iteration: 1660, Loss: 1.2654750347137451\n",
      "Iteration: 1661, Loss: 1.265467882156372\n",
      "Iteration: 1662, Loss: 1.2654608488082886\n",
      "Iteration: 1663, Loss: 1.2654536962509155\n",
      "Iteration: 1664, Loss: 1.265446662902832\n",
      "Iteration: 1665, Loss: 1.265439510345459\n",
      "Iteration: 1666, Loss: 1.265432357788086\n",
      "Iteration: 1667, Loss: 1.2654253244400024\n",
      "Iteration: 1668, Loss: 1.2654181718826294\n",
      "Iteration: 1669, Loss: 1.265411138534546\n",
      "Iteration: 1670, Loss: 1.2654039859771729\n",
      "Iteration: 1671, Loss: 1.2653969526290894\n",
      "Iteration: 1672, Loss: 1.2653898000717163\n",
      "Iteration: 1673, Loss: 1.2653827667236328\n",
      "Iteration: 1674, Loss: 1.2653757333755493\n",
      "Iteration: 1675, Loss: 1.2653685808181763\n",
      "Iteration: 1676, Loss: 1.2653615474700928\n",
      "Iteration: 1677, Loss: 1.2653543949127197\n",
      "Iteration: 1678, Loss: 1.2653473615646362\n",
      "Iteration: 1679, Loss: 1.2653403282165527\n",
      "Iteration: 1680, Loss: 1.2653331756591797\n",
      "Iteration: 1681, Loss: 1.2653261423110962\n",
      "Iteration: 1682, Loss: 1.2653191089630127\n",
      "Iteration: 1683, Loss: 1.2653119564056396\n",
      "Iteration: 1684, Loss: 1.2653049230575562\n",
      "Iteration: 1685, Loss: 1.2652978897094727\n",
      "Iteration: 1686, Loss: 1.2652908563613892\n",
      "Iteration: 1687, Loss: 1.2652837038040161\n",
      "Iteration: 1688, Loss: 1.2652766704559326\n",
      "Iteration: 1689, Loss: 1.2652696371078491\n",
      "Iteration: 1690, Loss: 1.2652626037597656\n",
      "Iteration: 1691, Loss: 1.2652555704116821\n",
      "Iteration: 1692, Loss: 1.2652485370635986\n",
      "Iteration: 1693, Loss: 1.2652415037155151\n",
      "Iteration: 1694, Loss: 1.265234351158142\n",
      "Iteration: 1695, Loss: 1.2652273178100586\n",
      "Iteration: 1696, Loss: 1.265220284461975\n",
      "Iteration: 1697, Loss: 1.2652132511138916\n",
      "Iteration: 1698, Loss: 1.265206217765808\n",
      "Iteration: 1699, Loss: 1.2651991844177246\n",
      "Iteration: 1700, Loss: 1.2651921510696411\n",
      "Iteration: 1701, Loss: 1.2651851177215576\n",
      "Iteration: 1702, Loss: 1.2651780843734741\n",
      "Iteration: 1703, Loss: 1.2651710510253906\n",
      "Iteration: 1704, Loss: 1.2651640176773071\n",
      "Iteration: 1705, Loss: 1.2651569843292236\n",
      "Iteration: 1706, Loss: 1.2651499509811401\n",
      "Iteration: 1707, Loss: 1.2651430368423462\n",
      "Iteration: 1708, Loss: 1.2651360034942627\n",
      "Iteration: 1709, Loss: 1.2651289701461792\n",
      "Iteration: 1710, Loss: 1.2651219367980957\n",
      "Iteration: 1711, Loss: 1.2651149034500122\n",
      "Iteration: 1712, Loss: 1.2651078701019287\n",
      "Iteration: 1713, Loss: 1.2651008367538452\n",
      "Iteration: 1714, Loss: 1.2650939226150513\n",
      "Iteration: 1715, Loss: 1.2650868892669678\n",
      "Iteration: 1716, Loss: 1.2650798559188843\n",
      "Iteration: 1717, Loss: 1.2650728225708008\n",
      "Iteration: 1718, Loss: 1.2650659084320068\n",
      "Iteration: 1719, Loss: 1.2650588750839233\n",
      "Iteration: 1720, Loss: 1.2650518417358398\n",
      "Iteration: 1721, Loss: 1.265044927597046\n",
      "Iteration: 1722, Loss: 1.2650378942489624\n",
      "Iteration: 1723, Loss: 1.265030860900879\n",
      "Iteration: 1724, Loss: 1.265023946762085\n",
      "Iteration: 1725, Loss: 1.2650169134140015\n",
      "Iteration: 1726, Loss: 1.2650099992752075\n",
      "Iteration: 1727, Loss: 1.265002965927124\n",
      "Iteration: 1728, Loss: 1.2649959325790405\n",
      "Iteration: 1729, Loss: 1.2649890184402466\n",
      "Iteration: 1730, Loss: 1.264981985092163\n",
      "Iteration: 1731, Loss: 1.2649750709533691\n",
      "Iteration: 1732, Loss: 1.2649680376052856\n",
      "Iteration: 1733, Loss: 1.2649611234664917\n",
      "Iteration: 1734, Loss: 1.2649540901184082\n",
      "Iteration: 1735, Loss: 1.2649471759796143\n",
      "Iteration: 1736, Loss: 1.2649401426315308\n",
      "Iteration: 1737, Loss: 1.2649332284927368\n",
      "Iteration: 1738, Loss: 1.2649263143539429\n",
      "Iteration: 1739, Loss: 1.2649192810058594\n",
      "Iteration: 1740, Loss: 1.2649123668670654\n",
      "Iteration: 1741, Loss: 1.264905333518982\n",
      "Iteration: 1742, Loss: 1.264898419380188\n",
      "Iteration: 1743, Loss: 1.264891505241394\n",
      "Iteration: 1744, Loss: 1.2648844718933105\n",
      "Iteration: 1745, Loss: 1.2648775577545166\n",
      "Iteration: 1746, Loss: 1.2648706436157227\n",
      "Iteration: 1747, Loss: 1.2648637294769287\n",
      "Iteration: 1748, Loss: 1.2648566961288452\n",
      "Iteration: 1749, Loss: 1.2648497819900513\n",
      "Iteration: 1750, Loss: 1.2648428678512573\n",
      "Iteration: 1751, Loss: 1.2648359537124634\n",
      "Iteration: 1752, Loss: 1.2648290395736694\n",
      "Iteration: 1753, Loss: 1.264822006225586\n",
      "Iteration: 1754, Loss: 1.264815092086792\n",
      "Iteration: 1755, Loss: 1.264808177947998\n",
      "Iteration: 1756, Loss: 1.264801263809204\n",
      "Iteration: 1757, Loss: 1.2647943496704102\n",
      "Iteration: 1758, Loss: 1.2647874355316162\n",
      "Iteration: 1759, Loss: 1.2647805213928223\n",
      "Iteration: 1760, Loss: 1.2647736072540283\n",
      "Iteration: 1761, Loss: 1.2647666931152344\n",
      "Iteration: 1762, Loss: 1.2647597789764404\n",
      "Iteration: 1763, Loss: 1.2647528648376465\n",
      "Iteration: 1764, Loss: 1.2647459506988525\n",
      "Iteration: 1765, Loss: 1.2647390365600586\n",
      "Iteration: 1766, Loss: 1.2647321224212646\n",
      "Iteration: 1767, Loss: 1.2647252082824707\n",
      "Iteration: 1768, Loss: 1.2647182941436768\n",
      "Iteration: 1769, Loss: 1.2647113800048828\n",
      "Iteration: 1770, Loss: 1.2647044658660889\n",
      "Iteration: 1771, Loss: 1.264697551727295\n",
      "Iteration: 1772, Loss: 1.264690637588501\n",
      "Iteration: 1773, Loss: 1.264683723449707\n",
      "Iteration: 1774, Loss: 1.264676809310913\n",
      "Iteration: 1775, Loss: 1.2646700143814087\n",
      "Iteration: 1776, Loss: 1.2646631002426147\n",
      "Iteration: 1777, Loss: 1.2646561861038208\n",
      "Iteration: 1778, Loss: 1.2646492719650269\n",
      "Iteration: 1779, Loss: 1.264642357826233\n",
      "Iteration: 1780, Loss: 1.2646355628967285\n",
      "Iteration: 1781, Loss: 1.264628529548645\n",
      "Iteration: 1782, Loss: 1.264621615409851\n",
      "Iteration: 1783, Loss: 1.2646147012710571\n",
      "Iteration: 1784, Loss: 1.2646076679229736\n",
      "Iteration: 1785, Loss: 1.2646005153656006\n",
      "Iteration: 1786, Loss: 1.2645931243896484\n",
      "Iteration: 1787, Loss: 1.2645856142044067\n",
      "Iteration: 1788, Loss: 1.2645771503448486\n",
      "Iteration: 1789, Loss: 1.264563798904419\n",
      "Iteration: 1790, Loss: 1.2644857168197632\n",
      "Iteration: 1791, Loss: 1.2638444900512695\n",
      "Iteration: 1792, Loss: 1.2645654678344727\n",
      "Iteration: 1793, Loss: 1.2645740509033203\n",
      "Iteration: 1794, Loss: 1.2645819187164307\n",
      "Iteration: 1795, Loss: 1.2645891904830933\n",
      "Iteration: 1796, Loss: 1.264595627784729\n",
      "Iteration: 1797, Loss: 1.2646013498306274\n",
      "Iteration: 1798, Loss: 1.2646033763885498\n",
      "Iteration: 1799, Loss: 1.2645801305770874\n",
      "Iteration: 1800, Loss: 1.2646129131317139\n",
      "Iteration: 1801, Loss: 1.2646149396896362\n",
      "Iteration: 1802, Loss: 1.2646161317825317\n",
      "Iteration: 1803, Loss: 1.264616847038269\n",
      "Iteration: 1804, Loss: 1.2646167278289795\n",
      "Iteration: 1805, Loss: 1.2646161317825317\n",
      "Iteration: 1806, Loss: 1.2646149396896362\n",
      "Iteration: 1807, Loss: 1.264613151550293\n",
      "Iteration: 1808, Loss: 1.264611005783081\n",
      "Iteration: 1809, Loss: 1.2646082639694214\n",
      "Iteration: 1810, Loss: 1.2646052837371826\n",
      "Iteration: 1811, Loss: 1.2646018266677856\n",
      "Iteration: 1812, Loss: 1.2645981311798096\n",
      "Iteration: 1813, Loss: 1.2645940780639648\n",
      "Iteration: 1814, Loss: 1.2645896673202515\n",
      "Iteration: 1815, Loss: 1.2645851373672485\n",
      "Iteration: 1816, Loss: 1.2645803689956665\n",
      "Iteration: 1817, Loss: 1.2645752429962158\n",
      "Iteration: 1818, Loss: 1.2645701169967651\n",
      "Iteration: 1819, Loss: 1.2645647525787354\n",
      "Iteration: 1820, Loss: 1.2645591497421265\n",
      "Iteration: 1821, Loss: 1.2645535469055176\n",
      "Iteration: 1822, Loss: 1.2645477056503296\n",
      "Iteration: 1823, Loss: 1.264541745185852\n",
      "Iteration: 1824, Loss: 1.2645357847213745\n",
      "Iteration: 1825, Loss: 1.2645297050476074\n",
      "Iteration: 1826, Loss: 1.2645235061645508\n",
      "Iteration: 1827, Loss: 1.2645171880722046\n",
      "Iteration: 1828, Loss: 1.2645108699798584\n",
      "Iteration: 1829, Loss: 1.2645044326782227\n",
      "Iteration: 1830, Loss: 1.264497995376587\n",
      "Iteration: 1831, Loss: 1.2644915580749512\n",
      "Iteration: 1832, Loss: 1.2644850015640259\n",
      "Iteration: 1833, Loss: 1.2644784450531006\n",
      "Iteration: 1834, Loss: 1.2644717693328857\n",
      "Iteration: 1835, Loss: 1.2644652128219604\n",
      "Iteration: 1836, Loss: 1.2644585371017456\n",
      "Iteration: 1837, Loss: 1.2644517421722412\n",
      "Iteration: 1838, Loss: 1.2644450664520264\n",
      "Iteration: 1839, Loss: 1.264438271522522\n",
      "Iteration: 1840, Loss: 1.2644315958023071\n",
      "Iteration: 1841, Loss: 1.2644248008728027\n",
      "Iteration: 1842, Loss: 1.2644180059432983\n",
      "Iteration: 1843, Loss: 1.264411211013794\n",
      "Iteration: 1844, Loss: 1.2644044160842896\n",
      "Iteration: 1845, Loss: 1.2643975019454956\n",
      "Iteration: 1846, Loss: 1.2643907070159912\n",
      "Iteration: 1847, Loss: 1.2643839120864868\n",
      "Iteration: 1848, Loss: 1.2643769979476929\n",
      "Iteration: 1849, Loss: 1.2643702030181885\n",
      "Iteration: 1850, Loss: 1.2643632888793945\n",
      "Iteration: 1851, Loss: 1.2643564939498901\n",
      "Iteration: 1852, Loss: 1.2643495798110962\n",
      "Iteration: 1853, Loss: 1.2643426656723022\n",
      "Iteration: 1854, Loss: 1.2643358707427979\n",
      "Iteration: 1855, Loss: 1.264328956604004\n",
      "Iteration: 1856, Loss: 1.26432204246521\n",
      "Iteration: 1857, Loss: 1.2643152475357056\n",
      "Iteration: 1858, Loss: 1.2643083333969116\n",
      "Iteration: 1859, Loss: 1.2643014192581177\n",
      "Iteration: 1860, Loss: 1.2642946243286133\n",
      "Iteration: 1861, Loss: 1.2642877101898193\n",
      "Iteration: 1862, Loss: 1.2642807960510254\n",
      "Iteration: 1863, Loss: 1.2642738819122314\n",
      "Iteration: 1864, Loss: 1.264267086982727\n",
      "Iteration: 1865, Loss: 1.264260172843933\n",
      "Iteration: 1866, Loss: 1.2642532587051392\n",
      "Iteration: 1867, Loss: 1.2642464637756348\n",
      "Iteration: 1868, Loss: 1.2642395496368408\n",
      "Iteration: 1869, Loss: 1.2642326354980469\n",
      "Iteration: 1870, Loss: 1.2642258405685425\n",
      "Iteration: 1871, Loss: 1.2642189264297485\n",
      "Iteration: 1872, Loss: 1.2642120122909546\n",
      "Iteration: 1873, Loss: 1.2642052173614502\n",
      "Iteration: 1874, Loss: 1.2641983032226562\n",
      "Iteration: 1875, Loss: 1.2641913890838623\n",
      "Iteration: 1876, Loss: 1.264184594154358\n",
      "Iteration: 1877, Loss: 1.264177680015564\n",
      "Iteration: 1878, Loss: 1.2641708850860596\n",
      "Iteration: 1879, Loss: 1.2641639709472656\n",
      "Iteration: 1880, Loss: 1.2641570568084717\n",
      "Iteration: 1881, Loss: 1.2641502618789673\n",
      "Iteration: 1882, Loss: 1.2641433477401733\n",
      "Iteration: 1883, Loss: 1.264136552810669\n",
      "Iteration: 1884, Loss: 1.264129638671875\n",
      "Iteration: 1885, Loss: 1.2641228437423706\n",
      "Iteration: 1886, Loss: 1.2641159296035767\n",
      "Iteration: 1887, Loss: 1.2641091346740723\n",
      "Iteration: 1888, Loss: 1.2641022205352783\n",
      "Iteration: 1889, Loss: 1.264095425605774\n",
      "Iteration: 1890, Loss: 1.2640886306762695\n",
      "Iteration: 1891, Loss: 1.2640817165374756\n",
      "Iteration: 1892, Loss: 1.2640749216079712\n",
      "Iteration: 1893, Loss: 1.2640680074691772\n",
      "Iteration: 1894, Loss: 1.2640612125396729\n",
      "Iteration: 1895, Loss: 1.2640544176101685\n",
      "Iteration: 1896, Loss: 1.2640475034713745\n",
      "Iteration: 1897, Loss: 1.2640407085418701\n",
      "Iteration: 1898, Loss: 1.2640339136123657\n",
      "Iteration: 1899, Loss: 1.2640271186828613\n",
      "Iteration: 1900, Loss: 1.2640202045440674\n",
      "Iteration: 1901, Loss: 1.264013409614563\n",
      "Iteration: 1902, Loss: 1.2640066146850586\n",
      "Iteration: 1903, Loss: 1.2639998197555542\n",
      "Iteration: 1904, Loss: 1.2639929056167603\n",
      "Iteration: 1905, Loss: 1.2639861106872559\n",
      "Iteration: 1906, Loss: 1.2639793157577515\n",
      "Iteration: 1907, Loss: 1.263972520828247\n",
      "Iteration: 1908, Loss: 1.2639657258987427\n",
      "Iteration: 1909, Loss: 1.2639589309692383\n",
      "Iteration: 1910, Loss: 1.2639520168304443\n",
      "Iteration: 1911, Loss: 1.26394522190094\n",
      "Iteration: 1912, Loss: 1.2639384269714355\n",
      "Iteration: 1913, Loss: 1.2639316320419312\n",
      "Iteration: 1914, Loss: 1.2639248371124268\n",
      "Iteration: 1915, Loss: 1.2639180421829224\n",
      "Iteration: 1916, Loss: 1.263911247253418\n",
      "Iteration: 1917, Loss: 1.2639044523239136\n",
      "Iteration: 1918, Loss: 1.2638976573944092\n",
      "Iteration: 1919, Loss: 1.2638908624649048\n",
      "Iteration: 1920, Loss: 1.2638840675354004\n",
      "Iteration: 1921, Loss: 1.263877272605896\n",
      "Iteration: 1922, Loss: 1.2638704776763916\n",
      "Iteration: 1923, Loss: 1.2638636827468872\n",
      "Iteration: 1924, Loss: 1.2638570070266724\n",
      "Iteration: 1925, Loss: 1.263850212097168\n",
      "Iteration: 1926, Loss: 1.2638434171676636\n",
      "Iteration: 1927, Loss: 1.2638366222381592\n",
      "Iteration: 1928, Loss: 1.2638298273086548\n",
      "Iteration: 1929, Loss: 1.2638230323791504\n",
      "Iteration: 1930, Loss: 1.2638163566589355\n",
      "Iteration: 1931, Loss: 1.2638095617294312\n",
      "Iteration: 1932, Loss: 1.2638027667999268\n",
      "Iteration: 1933, Loss: 1.2637959718704224\n",
      "Iteration: 1934, Loss: 1.2637892961502075\n",
      "Iteration: 1935, Loss: 1.2637825012207031\n",
      "Iteration: 1936, Loss: 1.2637757062911987\n",
      "Iteration: 1937, Loss: 1.2637689113616943\n",
      "Iteration: 1938, Loss: 1.2637622356414795\n",
      "Iteration: 1939, Loss: 1.263755440711975\n",
      "Iteration: 1940, Loss: 1.2637486457824707\n",
      "Iteration: 1941, Loss: 1.2637419700622559\n",
      "Iteration: 1942, Loss: 1.2637351751327515\n",
      "Iteration: 1943, Loss: 1.2637284994125366\n",
      "Iteration: 1944, Loss: 1.2637217044830322\n",
      "Iteration: 1945, Loss: 1.2637149095535278\n",
      "Iteration: 1946, Loss: 1.263708233833313\n",
      "Iteration: 1947, Loss: 1.2637014389038086\n",
      "Iteration: 1948, Loss: 1.2636947631835938\n",
      "Iteration: 1949, Loss: 1.2636879682540894\n",
      "Iteration: 1950, Loss: 1.2636812925338745\n",
      "Iteration: 1951, Loss: 1.2636744976043701\n",
      "Iteration: 1952, Loss: 1.2636678218841553\n",
      "Iteration: 1953, Loss: 1.2636610269546509\n",
      "Iteration: 1954, Loss: 1.263654351234436\n",
      "Iteration: 1955, Loss: 1.2636476755142212\n",
      "Iteration: 1956, Loss: 1.2636408805847168\n",
      "Iteration: 1957, Loss: 1.263634204864502\n",
      "Iteration: 1958, Loss: 1.2636274099349976\n",
      "Iteration: 1959, Loss: 1.2636207342147827\n",
      "Iteration: 1960, Loss: 1.2636140584945679\n",
      "Iteration: 1961, Loss: 1.2636072635650635\n",
      "Iteration: 1962, Loss: 1.2636005878448486\n",
      "Iteration: 1963, Loss: 1.2635939121246338\n",
      "Iteration: 1964, Loss: 1.263587236404419\n",
      "Iteration: 1965, Loss: 1.2635804414749146\n",
      "Iteration: 1966, Loss: 1.2635737657546997\n",
      "Iteration: 1967, Loss: 1.2635670900344849\n",
      "Iteration: 1968, Loss: 1.26356041431427\n",
      "Iteration: 1969, Loss: 1.2635536193847656\n",
      "Iteration: 1970, Loss: 1.2635469436645508\n",
      "Iteration: 1971, Loss: 1.263540267944336\n",
      "Iteration: 1972, Loss: 1.263533592224121\n",
      "Iteration: 1973, Loss: 1.2635269165039062\n",
      "Iteration: 1974, Loss: 1.2635202407836914\n",
      "Iteration: 1975, Loss: 1.2635135650634766\n",
      "Iteration: 1976, Loss: 1.2635067701339722\n",
      "Iteration: 1977, Loss: 1.2635000944137573\n",
      "Iteration: 1978, Loss: 1.2634934186935425\n",
      "Iteration: 1979, Loss: 1.2634867429733276\n",
      "Iteration: 1980, Loss: 1.2634800672531128\n",
      "Iteration: 1981, Loss: 1.263473391532898\n",
      "Iteration: 1982, Loss: 1.263466715812683\n",
      "Iteration: 1983, Loss: 1.2634600400924683\n",
      "Iteration: 1984, Loss: 1.2634533643722534\n",
      "Iteration: 1985, Loss: 1.2634466886520386\n",
      "Iteration: 1986, Loss: 1.2634400129318237\n",
      "Iteration: 1987, Loss: 1.2634333372116089\n",
      "Iteration: 1988, Loss: 1.2634267807006836\n",
      "Iteration: 1989, Loss: 1.2634201049804688\n",
      "Iteration: 1990, Loss: 1.263413429260254\n",
      "Iteration: 1991, Loss: 1.263406753540039\n",
      "Iteration: 1992, Loss: 1.2634000778198242\n",
      "Iteration: 1993, Loss: 1.2633934020996094\n",
      "Iteration: 1994, Loss: 1.2633867263793945\n",
      "Iteration: 1995, Loss: 1.2633801698684692\n",
      "Iteration: 1996, Loss: 1.2633734941482544\n",
      "Iteration: 1997, Loss: 1.2633668184280396\n",
      "Iteration: 1998, Loss: 1.2633601427078247\n",
      "Iteration: 1999, Loss: 1.2633535861968994\n",
      "Iteration: 2000, Loss: 1.2633469104766846\n",
      "Iteration: 2001, Loss: 1.2633402347564697\n",
      "Iteration: 2002, Loss: 1.2633335590362549\n",
      "Iteration: 2003, Loss: 1.2633270025253296\n",
      "Iteration: 2004, Loss: 1.2633203268051147\n",
      "Iteration: 2005, Loss: 1.2633136510849\n",
      "Iteration: 2006, Loss: 1.2633070945739746\n",
      "Iteration: 2007, Loss: 1.2633004188537598\n",
      "Iteration: 2008, Loss: 1.2632938623428345\n",
      "Iteration: 2009, Loss: 1.2632871866226196\n",
      "Iteration: 2010, Loss: 1.2632805109024048\n",
      "Iteration: 2011, Loss: 1.2632739543914795\n",
      "Iteration: 2012, Loss: 1.2632672786712646\n",
      "Iteration: 2013, Loss: 1.2632607221603394\n",
      "Iteration: 2014, Loss: 1.2632540464401245\n",
      "Iteration: 2015, Loss: 1.2632474899291992\n",
      "Iteration: 2016, Loss: 1.2632408142089844\n",
      "Iteration: 2017, Loss: 1.263234257698059\n",
      "Iteration: 2018, Loss: 1.2632275819778442\n",
      "Iteration: 2019, Loss: 1.263221025466919\n",
      "Iteration: 2020, Loss: 1.2632144689559937\n",
      "Iteration: 2021, Loss: 1.2632077932357788\n",
      "Iteration: 2022, Loss: 1.2632012367248535\n",
      "Iteration: 2023, Loss: 1.2631945610046387\n",
      "Iteration: 2024, Loss: 1.2631880044937134\n",
      "Iteration: 2025, Loss: 1.263181447982788\n",
      "Iteration: 2026, Loss: 1.2631747722625732\n",
      "Iteration: 2027, Loss: 1.263168215751648\n",
      "Iteration: 2028, Loss: 1.2631616592407227\n",
      "Iteration: 2029, Loss: 1.2631549835205078\n",
      "Iteration: 2030, Loss: 1.2631484270095825\n",
      "Iteration: 2031, Loss: 1.2631418704986572\n",
      "Iteration: 2032, Loss: 1.263135313987732\n",
      "Iteration: 2033, Loss: 1.263128638267517\n",
      "Iteration: 2034, Loss: 1.2631220817565918\n",
      "Iteration: 2035, Loss: 1.2631155252456665\n",
      "Iteration: 2036, Loss: 1.2631089687347412\n",
      "Iteration: 2037, Loss: 1.263102412223816\n",
      "Iteration: 2038, Loss: 1.263095736503601\n",
      "Iteration: 2039, Loss: 1.2630891799926758\n",
      "Iteration: 2040, Loss: 1.2630826234817505\n",
      "Iteration: 2041, Loss: 1.2630760669708252\n",
      "Iteration: 2042, Loss: 1.2630695104599\n",
      "Iteration: 2043, Loss: 1.2630629539489746\n",
      "Iteration: 2044, Loss: 1.2630563974380493\n",
      "Iteration: 2045, Loss: 1.263049840927124\n",
      "Iteration: 2046, Loss: 1.2630432844161987\n",
      "Iteration: 2047, Loss: 1.2630367279052734\n",
      "Iteration: 2048, Loss: 1.2630301713943481\n",
      "Iteration: 2049, Loss: 1.2630236148834229\n",
      "Iteration: 2050, Loss: 1.2630170583724976\n",
      "Iteration: 2051, Loss: 1.2630105018615723\n",
      "Iteration: 2052, Loss: 1.263003945350647\n",
      "Iteration: 2053, Loss: 1.2629973888397217\n",
      "Iteration: 2054, Loss: 1.2629908323287964\n",
      "Iteration: 2055, Loss: 1.262984275817871\n",
      "Iteration: 2056, Loss: 1.2629777193069458\n",
      "Iteration: 2057, Loss: 1.2629711627960205\n",
      "Iteration: 2058, Loss: 1.2629646062850952\n",
      "Iteration: 2059, Loss: 1.2629581689834595\n",
      "Iteration: 2060, Loss: 1.2629516124725342\n",
      "Iteration: 2061, Loss: 1.2629450559616089\n",
      "Iteration: 2062, Loss: 1.2629384994506836\n",
      "Iteration: 2063, Loss: 1.2629319429397583\n",
      "Iteration: 2064, Loss: 1.2629255056381226\n",
      "Iteration: 2065, Loss: 1.2629189491271973\n",
      "Iteration: 2066, Loss: 1.262912392616272\n",
      "Iteration: 2067, Loss: 1.2629058361053467\n",
      "Iteration: 2068, Loss: 1.262899398803711\n",
      "Iteration: 2069, Loss: 1.2628928422927856\n",
      "Iteration: 2070, Loss: 1.2628862857818604\n",
      "Iteration: 2071, Loss: 1.2628798484802246\n",
      "Iteration: 2072, Loss: 1.2628732919692993\n",
      "Iteration: 2073, Loss: 1.262866735458374\n",
      "Iteration: 2074, Loss: 1.2628602981567383\n",
      "Iteration: 2075, Loss: 1.262853741645813\n",
      "Iteration: 2076, Loss: 1.2628473043441772\n",
      "Iteration: 2077, Loss: 1.262840747833252\n",
      "Iteration: 2078, Loss: 1.2628341913223267\n",
      "Iteration: 2079, Loss: 1.262827754020691\n",
      "Iteration: 2080, Loss: 1.2628211975097656\n",
      "Iteration: 2081, Loss: 1.2628147602081299\n",
      "Iteration: 2082, Loss: 1.2628082036972046\n",
      "Iteration: 2083, Loss: 1.2628017663955688\n",
      "Iteration: 2084, Loss: 1.2627952098846436\n",
      "Iteration: 2085, Loss: 1.2627887725830078\n",
      "Iteration: 2086, Loss: 1.2627822160720825\n",
      "Iteration: 2087, Loss: 1.2627757787704468\n",
      "Iteration: 2088, Loss: 1.262769341468811\n",
      "Iteration: 2089, Loss: 1.2627627849578857\n",
      "Iteration: 2090, Loss: 1.26275634765625\n",
      "Iteration: 2091, Loss: 1.2627497911453247\n",
      "Iteration: 2092, Loss: 1.262743353843689\n",
      "Iteration: 2093, Loss: 1.2627369165420532\n",
      "Iteration: 2094, Loss: 1.262730360031128\n",
      "Iteration: 2095, Loss: 1.2627239227294922\n",
      "Iteration: 2096, Loss: 1.2627174854278564\n",
      "Iteration: 2097, Loss: 1.2627109289169312\n",
      "Iteration: 2098, Loss: 1.2627044916152954\n",
      "Iteration: 2099, Loss: 1.2626980543136597\n",
      "Iteration: 2100, Loss: 1.262691617012024\n",
      "Iteration: 2101, Loss: 1.2626851797103882\n",
      "Iteration: 2102, Loss: 1.262678623199463\n",
      "Iteration: 2103, Loss: 1.2626721858978271\n",
      "Iteration: 2104, Loss: 1.2626657485961914\n",
      "Iteration: 2105, Loss: 1.2626593112945557\n",
      "Iteration: 2106, Loss: 1.26265287399292\n",
      "Iteration: 2107, Loss: 1.2626463174819946\n",
      "Iteration: 2108, Loss: 1.2626398801803589\n",
      "Iteration: 2109, Loss: 1.2626334428787231\n",
      "Iteration: 2110, Loss: 1.2626270055770874\n",
      "Iteration: 2111, Loss: 1.2626205682754517\n",
      "Iteration: 2112, Loss: 1.262614130973816\n",
      "Iteration: 2113, Loss: 1.2626076936721802\n",
      "Iteration: 2114, Loss: 1.2626012563705444\n",
      "Iteration: 2115, Loss: 1.2625948190689087\n",
      "Iteration: 2116, Loss: 1.262588381767273\n",
      "Iteration: 2117, Loss: 1.2625819444656372\n",
      "Iteration: 2118, Loss: 1.2625755071640015\n",
      "Iteration: 2119, Loss: 1.2625690698623657\n",
      "Iteration: 2120, Loss: 1.26256263256073\n",
      "Iteration: 2121, Loss: 1.2625561952590942\n",
      "Iteration: 2122, Loss: 1.2625497579574585\n",
      "Iteration: 2123, Loss: 1.2625433206558228\n",
      "Iteration: 2124, Loss: 1.262536883354187\n",
      "Iteration: 2125, Loss: 1.2625304460525513\n",
      "Iteration: 2126, Loss: 1.262524127960205\n",
      "Iteration: 2127, Loss: 1.2625176906585693\n",
      "Iteration: 2128, Loss: 1.2625112533569336\n",
      "Iteration: 2129, Loss: 1.2625048160552979\n",
      "Iteration: 2130, Loss: 1.262498378753662\n",
      "Iteration: 2131, Loss: 1.262492060661316\n",
      "Iteration: 2132, Loss: 1.2624856233596802\n",
      "Iteration: 2133, Loss: 1.2624791860580444\n",
      "Iteration: 2134, Loss: 1.2624727487564087\n",
      "Iteration: 2135, Loss: 1.2624664306640625\n",
      "Iteration: 2136, Loss: 1.2624599933624268\n",
      "Iteration: 2137, Loss: 1.262453556060791\n",
      "Iteration: 2138, Loss: 1.2624471187591553\n",
      "Iteration: 2139, Loss: 1.262440800666809\n",
      "Iteration: 2140, Loss: 1.2624343633651733\n",
      "Iteration: 2141, Loss: 1.2624279260635376\n",
      "Iteration: 2142, Loss: 1.2624216079711914\n",
      "Iteration: 2143, Loss: 1.2624151706695557\n",
      "Iteration: 2144, Loss: 1.2624088525772095\n",
      "Iteration: 2145, Loss: 1.2624024152755737\n",
      "Iteration: 2146, Loss: 1.262395977973938\n",
      "Iteration: 2147, Loss: 1.2623896598815918\n",
      "Iteration: 2148, Loss: 1.262383222579956\n",
      "Iteration: 2149, Loss: 1.2623769044876099\n",
      "Iteration: 2150, Loss: 1.2623704671859741\n",
      "Iteration: 2151, Loss: 1.262364149093628\n",
      "Iteration: 2152, Loss: 1.2623577117919922\n",
      "Iteration: 2153, Loss: 1.262351393699646\n",
      "Iteration: 2154, Loss: 1.2623449563980103\n",
      "Iteration: 2155, Loss: 1.262338638305664\n",
      "Iteration: 2156, Loss: 1.2623323202133179\n",
      "Iteration: 2157, Loss: 1.2623258829116821\n",
      "Iteration: 2158, Loss: 1.262319564819336\n",
      "Iteration: 2159, Loss: 1.2623131275177002\n",
      "Iteration: 2160, Loss: 1.262306809425354\n",
      "Iteration: 2161, Loss: 1.2623004913330078\n",
      "Iteration: 2162, Loss: 1.262294054031372\n",
      "Iteration: 2163, Loss: 1.2622877359390259\n",
      "Iteration: 2164, Loss: 1.2622814178466797\n",
      "Iteration: 2165, Loss: 1.262274980545044\n",
      "Iteration: 2166, Loss: 1.2622686624526978\n",
      "Iteration: 2167, Loss: 1.2622623443603516\n",
      "Iteration: 2168, Loss: 1.2622559070587158\n",
      "Iteration: 2169, Loss: 1.2622495889663696\n",
      "Iteration: 2170, Loss: 1.2622432708740234\n",
      "Iteration: 2171, Loss: 1.2622369527816772\n",
      "Iteration: 2172, Loss: 1.262230634689331\n",
      "Iteration: 2173, Loss: 1.2622241973876953\n",
      "Iteration: 2174, Loss: 1.2622178792953491\n",
      "Iteration: 2175, Loss: 1.262211561203003\n",
      "Iteration: 2176, Loss: 1.2622052431106567\n",
      "Iteration: 2177, Loss: 1.2621989250183105\n",
      "Iteration: 2178, Loss: 1.2621926069259644\n",
      "Iteration: 2179, Loss: 1.2621862888336182\n",
      "Iteration: 2180, Loss: 1.2621798515319824\n",
      "Iteration: 2181, Loss: 1.2621735334396362\n",
      "Iteration: 2182, Loss: 1.26216721534729\n",
      "Iteration: 2183, Loss: 1.2621608972549438\n",
      "Iteration: 2184, Loss: 1.2621545791625977\n",
      "Iteration: 2185, Loss: 1.2621482610702515\n",
      "Iteration: 2186, Loss: 1.2621419429779053\n",
      "Iteration: 2187, Loss: 1.262135624885559\n",
      "Iteration: 2188, Loss: 1.262129306793213\n",
      "Iteration: 2189, Loss: 1.2621229887008667\n",
      "Iteration: 2190, Loss: 1.2621166706085205\n",
      "Iteration: 2191, Loss: 1.2621104717254639\n",
      "Iteration: 2192, Loss: 1.2621041536331177\n",
      "Iteration: 2193, Loss: 1.2620978355407715\n",
      "Iteration: 2194, Loss: 1.2620915174484253\n",
      "Iteration: 2195, Loss: 1.262085199356079\n",
      "Iteration: 2196, Loss: 1.262078881263733\n",
      "Iteration: 2197, Loss: 1.2620725631713867\n",
      "Iteration: 2198, Loss: 1.2620662450790405\n",
      "Iteration: 2199, Loss: 1.2620600461959839\n",
      "Iteration: 2200, Loss: 1.2620537281036377\n",
      "Iteration: 2201, Loss: 1.2620474100112915\n",
      "Iteration: 2202, Loss: 1.2620410919189453\n",
      "Iteration: 2203, Loss: 1.2620347738265991\n",
      "Iteration: 2204, Loss: 1.2620285749435425\n",
      "Iteration: 2205, Loss: 1.2620222568511963\n",
      "Iteration: 2206, Loss: 1.26201593875885\n",
      "Iteration: 2207, Loss: 1.2620097398757935\n",
      "Iteration: 2208, Loss: 1.2620034217834473\n",
      "Iteration: 2209, Loss: 1.261997103691101\n",
      "Iteration: 2210, Loss: 1.2619909048080444\n",
      "Iteration: 2211, Loss: 1.2619845867156982\n",
      "Iteration: 2212, Loss: 1.261978268623352\n",
      "Iteration: 2213, Loss: 1.2619720697402954\n",
      "Iteration: 2214, Loss: 1.2619657516479492\n",
      "Iteration: 2215, Loss: 1.261959433555603\n",
      "Iteration: 2216, Loss: 1.2619532346725464\n",
      "Iteration: 2217, Loss: 1.2619469165802002\n",
      "Iteration: 2218, Loss: 1.2619407176971436\n",
      "Iteration: 2219, Loss: 1.2619343996047974\n",
      "Iteration: 2220, Loss: 1.2619282007217407\n",
      "Iteration: 2221, Loss: 1.2619218826293945\n",
      "Iteration: 2222, Loss: 1.261915683746338\n",
      "Iteration: 2223, Loss: 1.2619093656539917\n",
      "Iteration: 2224, Loss: 1.261903166770935\n",
      "Iteration: 2225, Loss: 1.2618968486785889\n",
      "Iteration: 2226, Loss: 1.2618906497955322\n",
      "Iteration: 2227, Loss: 1.261884331703186\n",
      "Iteration: 2228, Loss: 1.2618781328201294\n",
      "Iteration: 2229, Loss: 1.2618719339370728\n",
      "Iteration: 2230, Loss: 1.2618656158447266\n",
      "Iteration: 2231, Loss: 1.26185941696167\n",
      "Iteration: 2232, Loss: 1.2618532180786133\n",
      "Iteration: 2233, Loss: 1.261846899986267\n",
      "Iteration: 2234, Loss: 1.2618407011032104\n",
      "Iteration: 2235, Loss: 1.2618345022201538\n",
      "Iteration: 2236, Loss: 1.2618281841278076\n",
      "Iteration: 2237, Loss: 1.261821985244751\n",
      "Iteration: 2238, Loss: 1.2618157863616943\n",
      "Iteration: 2239, Loss: 1.2618094682693481\n",
      "Iteration: 2240, Loss: 1.2618032693862915\n",
      "Iteration: 2241, Loss: 1.2617970705032349\n",
      "Iteration: 2242, Loss: 1.2617908716201782\n",
      "Iteration: 2243, Loss: 1.2617846727371216\n",
      "Iteration: 2244, Loss: 1.2617783546447754\n",
      "Iteration: 2245, Loss: 1.2617721557617188\n",
      "Iteration: 2246, Loss: 1.261765956878662\n",
      "Iteration: 2247, Loss: 1.2617597579956055\n",
      "Iteration: 2248, Loss: 1.2617535591125488\n",
      "Iteration: 2249, Loss: 1.2617473602294922\n",
      "Iteration: 2250, Loss: 1.2617411613464355\n",
      "Iteration: 2251, Loss: 1.2617348432540894\n",
      "Iteration: 2252, Loss: 1.2617286443710327\n",
      "Iteration: 2253, Loss: 1.261722445487976\n",
      "Iteration: 2254, Loss: 1.2617162466049194\n",
      "Iteration: 2255, Loss: 1.2617100477218628\n",
      "Iteration: 2256, Loss: 1.2617038488388062\n",
      "Iteration: 2257, Loss: 1.2616976499557495\n",
      "Iteration: 2258, Loss: 1.2616914510726929\n",
      "Iteration: 2259, Loss: 1.2616852521896362\n",
      "Iteration: 2260, Loss: 1.2616790533065796\n",
      "Iteration: 2261, Loss: 1.261672854423523\n",
      "Iteration: 2262, Loss: 1.2616666555404663\n",
      "Iteration: 2263, Loss: 1.2616604566574097\n",
      "Iteration: 2264, Loss: 1.2616543769836426\n",
      "Iteration: 2265, Loss: 1.261648178100586\n",
      "Iteration: 2266, Loss: 1.2616419792175293\n",
      "Iteration: 2267, Loss: 1.2616357803344727\n",
      "Iteration: 2268, Loss: 1.261629581451416\n",
      "Iteration: 2269, Loss: 1.2616233825683594\n",
      "Iteration: 2270, Loss: 1.2616171836853027\n",
      "Iteration: 2271, Loss: 1.2616111040115356\n",
      "Iteration: 2272, Loss: 1.261604905128479\n",
      "Iteration: 2273, Loss: 1.2615987062454224\n",
      "Iteration: 2274, Loss: 1.2615925073623657\n",
      "Iteration: 2275, Loss: 1.261586308479309\n",
      "Iteration: 2276, Loss: 1.261580228805542\n",
      "Iteration: 2277, Loss: 1.2615740299224854\n",
      "Iteration: 2278, Loss: 1.2615678310394287\n",
      "Iteration: 2279, Loss: 1.2615617513656616\n",
      "Iteration: 2280, Loss: 1.261555552482605\n",
      "Iteration: 2281, Loss: 1.2615493535995483\n",
      "Iteration: 2282, Loss: 1.2615431547164917\n",
      "Iteration: 2283, Loss: 1.2615370750427246\n",
      "Iteration: 2284, Loss: 1.261530876159668\n",
      "Iteration: 2285, Loss: 1.2615247964859009\n",
      "Iteration: 2286, Loss: 1.2615185976028442\n",
      "Iteration: 2287, Loss: 1.2615123987197876\n",
      "Iteration: 2288, Loss: 1.2615063190460205\n",
      "Iteration: 2289, Loss: 1.2615001201629639\n",
      "Iteration: 2290, Loss: 1.2614940404891968\n",
      "Iteration: 2291, Loss: 1.2614878416061401\n",
      "Iteration: 2292, Loss: 1.261481761932373\n",
      "Iteration: 2293, Loss: 1.2614755630493164\n",
      "Iteration: 2294, Loss: 1.2614694833755493\n",
      "Iteration: 2295, Loss: 1.2614632844924927\n",
      "Iteration: 2296, Loss: 1.2614572048187256\n",
      "Iteration: 2297, Loss: 1.261451005935669\n",
      "Iteration: 2298, Loss: 1.2614449262619019\n",
      "Iteration: 2299, Loss: 1.2614387273788452\n",
      "Iteration: 2300, Loss: 1.2614326477050781\n",
      "Iteration: 2301, Loss: 1.2614264488220215\n",
      "Iteration: 2302, Loss: 1.2614203691482544\n",
      "Iteration: 2303, Loss: 1.2614142894744873\n",
      "Iteration: 2304, Loss: 1.2614080905914307\n",
      "Iteration: 2305, Loss: 1.2614020109176636\n",
      "Iteration: 2306, Loss: 1.2613959312438965\n",
      "Iteration: 2307, Loss: 1.2613897323608398\n",
      "Iteration: 2308, Loss: 1.2613836526870728\n",
      "Iteration: 2309, Loss: 1.2613775730133057\n",
      "Iteration: 2310, Loss: 1.261371374130249\n",
      "Iteration: 2311, Loss: 1.261365294456482\n",
      "Iteration: 2312, Loss: 1.2613592147827148\n",
      "Iteration: 2313, Loss: 1.2613531351089478\n",
      "Iteration: 2314, Loss: 1.2613469362258911\n",
      "Iteration: 2315, Loss: 1.261340856552124\n",
      "Iteration: 2316, Loss: 1.261334776878357\n",
      "Iteration: 2317, Loss: 1.2613286972045898\n",
      "Iteration: 2318, Loss: 1.2613226175308228\n",
      "Iteration: 2319, Loss: 1.2613165378570557\n",
      "Iteration: 2320, Loss: 1.261310338973999\n",
      "Iteration: 2321, Loss: 1.261304259300232\n",
      "Iteration: 2322, Loss: 1.2612981796264648\n",
      "Iteration: 2323, Loss: 1.2612920999526978\n",
      "Iteration: 2324, Loss: 1.2612860202789307\n",
      "Iteration: 2325, Loss: 1.2612799406051636\n",
      "Iteration: 2326, Loss: 1.2612738609313965\n",
      "Iteration: 2327, Loss: 1.2612677812576294\n",
      "Iteration: 2328, Loss: 1.2612617015838623\n",
      "Iteration: 2329, Loss: 1.2612556219100952\n",
      "Iteration: 2330, Loss: 1.2612495422363281\n",
      "Iteration: 2331, Loss: 1.261243462562561\n",
      "Iteration: 2332, Loss: 1.261237382888794\n",
      "Iteration: 2333, Loss: 1.2612313032150269\n",
      "Iteration: 2334, Loss: 1.2612252235412598\n",
      "Iteration: 2335, Loss: 1.2612191438674927\n",
      "Iteration: 2336, Loss: 1.2612130641937256\n",
      "Iteration: 2337, Loss: 1.2612069845199585\n",
      "Iteration: 2338, Loss: 1.2612009048461914\n",
      "Iteration: 2339, Loss: 1.2611948251724243\n",
      "Iteration: 2340, Loss: 1.2611887454986572\n",
      "Iteration: 2341, Loss: 1.2611826658248901\n",
      "Iteration: 2342, Loss: 1.2611767053604126\n",
      "Iteration: 2343, Loss: 1.2611706256866455\n",
      "Iteration: 2344, Loss: 1.2611645460128784\n",
      "Iteration: 2345, Loss: 1.2611584663391113\n",
      "Iteration: 2346, Loss: 1.2611523866653442\n",
      "Iteration: 2347, Loss: 1.2611464262008667\n",
      "Iteration: 2348, Loss: 1.2611403465270996\n",
      "Iteration: 2349, Loss: 1.2611342668533325\n",
      "Iteration: 2350, Loss: 1.2611281871795654\n",
      "Iteration: 2351, Loss: 1.261122226715088\n",
      "Iteration: 2352, Loss: 1.2611161470413208\n",
      "Iteration: 2353, Loss: 1.2611100673675537\n",
      "Iteration: 2354, Loss: 1.2611039876937866\n",
      "Iteration: 2355, Loss: 1.261098027229309\n",
      "Iteration: 2356, Loss: 1.261091947555542\n",
      "Iteration: 2357, Loss: 1.261085867881775\n",
      "Iteration: 2358, Loss: 1.2610799074172974\n",
      "Iteration: 2359, Loss: 1.2610738277435303\n",
      "Iteration: 2360, Loss: 1.2610678672790527\n",
      "Iteration: 2361, Loss: 1.2610617876052856\n",
      "Iteration: 2362, Loss: 1.2610557079315186\n",
      "Iteration: 2363, Loss: 1.261049747467041\n",
      "Iteration: 2364, Loss: 1.261043667793274\n",
      "Iteration: 2365, Loss: 1.2610377073287964\n",
      "Iteration: 2366, Loss: 1.2610316276550293\n",
      "Iteration: 2367, Loss: 1.2610256671905518\n",
      "Iteration: 2368, Loss: 1.2610195875167847\n",
      "Iteration: 2369, Loss: 1.2610136270523071\n",
      "Iteration: 2370, Loss: 1.26100754737854\n",
      "Iteration: 2371, Loss: 1.2610015869140625\n",
      "Iteration: 2372, Loss: 1.2609955072402954\n",
      "Iteration: 2373, Loss: 1.2609895467758179\n",
      "Iteration: 2374, Loss: 1.2609834671020508\n",
      "Iteration: 2375, Loss: 1.2609775066375732\n",
      "Iteration: 2376, Loss: 1.2609715461730957\n",
      "Iteration: 2377, Loss: 1.2609654664993286\n",
      "Iteration: 2378, Loss: 1.260959506034851\n",
      "Iteration: 2379, Loss: 1.260953426361084\n",
      "Iteration: 2380, Loss: 1.2609474658966064\n",
      "Iteration: 2381, Loss: 1.260941505432129\n",
      "Iteration: 2382, Loss: 1.2609354257583618\n",
      "Iteration: 2383, Loss: 1.2609294652938843\n",
      "Iteration: 2384, Loss: 1.2609235048294067\n",
      "Iteration: 2385, Loss: 1.2609175443649292\n",
      "Iteration: 2386, Loss: 1.260911464691162\n",
      "Iteration: 2387, Loss: 1.2609055042266846\n",
      "Iteration: 2388, Loss: 1.260899543762207\n",
      "Iteration: 2389, Loss: 1.2608935832977295\n",
      "Iteration: 2390, Loss: 1.2608875036239624\n",
      "Iteration: 2391, Loss: 1.2608815431594849\n",
      "Iteration: 2392, Loss: 1.2608755826950073\n",
      "Iteration: 2393, Loss: 1.2608696222305298\n",
      "Iteration: 2394, Loss: 1.2608636617660522\n",
      "Iteration: 2395, Loss: 1.2608575820922852\n",
      "Iteration: 2396, Loss: 1.2608516216278076\n",
      "Iteration: 2397, Loss: 1.26084566116333\n",
      "Iteration: 2398, Loss: 1.2608397006988525\n",
      "Iteration: 2399, Loss: 1.260833740234375\n",
      "Iteration: 2400, Loss: 1.2608277797698975\n",
      "Iteration: 2401, Loss: 1.26082181930542\n",
      "Iteration: 2402, Loss: 1.2608158588409424\n",
      "Iteration: 2403, Loss: 1.2608098983764648\n",
      "Iteration: 2404, Loss: 1.2608039379119873\n",
      "Iteration: 2405, Loss: 1.2607979774475098\n",
      "Iteration: 2406, Loss: 1.2607920169830322\n",
      "Iteration: 2407, Loss: 1.2607860565185547\n",
      "Iteration: 2408, Loss: 1.2607800960540771\n",
      "Iteration: 2409, Loss: 1.2607741355895996\n",
      "Iteration: 2410, Loss: 1.260768175125122\n",
      "Iteration: 2411, Loss: 1.2607622146606445\n",
      "Iteration: 2412, Loss: 1.260756254196167\n",
      "Iteration: 2413, Loss: 1.2607502937316895\n",
      "Iteration: 2414, Loss: 1.260744333267212\n",
      "Iteration: 2415, Loss: 1.2607383728027344\n",
      "Iteration: 2416, Loss: 1.2607324123382568\n",
      "Iteration: 2417, Loss: 1.2607264518737793\n",
      "Iteration: 2418, Loss: 1.2607206106185913\n",
      "Iteration: 2419, Loss: 1.2607146501541138\n",
      "Iteration: 2420, Loss: 1.2607086896896362\n",
      "Iteration: 2421, Loss: 1.2607027292251587\n",
      "Iteration: 2422, Loss: 1.2606967687606812\n",
      "Iteration: 2423, Loss: 1.2606908082962036\n",
      "Iteration: 2424, Loss: 1.2606849670410156\n",
      "Iteration: 2425, Loss: 1.260679006576538\n",
      "Iteration: 2426, Loss: 1.2606730461120605\n",
      "Iteration: 2427, Loss: 1.260667085647583\n",
      "Iteration: 2428, Loss: 1.260661244392395\n",
      "Iteration: 2429, Loss: 1.2606552839279175\n",
      "Iteration: 2430, Loss: 1.26064932346344\n",
      "Iteration: 2431, Loss: 1.260643482208252\n",
      "Iteration: 2432, Loss: 1.2606375217437744\n",
      "Iteration: 2433, Loss: 1.2606315612792969\n",
      "Iteration: 2434, Loss: 1.2606257200241089\n",
      "Iteration: 2435, Loss: 1.2606197595596313\n",
      "Iteration: 2436, Loss: 1.2606137990951538\n",
      "Iteration: 2437, Loss: 1.2606079578399658\n",
      "Iteration: 2438, Loss: 1.2606019973754883\n",
      "Iteration: 2439, Loss: 1.2605961561203003\n",
      "Iteration: 2440, Loss: 1.2605901956558228\n",
      "Iteration: 2441, Loss: 1.2605843544006348\n",
      "Iteration: 2442, Loss: 1.2605783939361572\n",
      "Iteration: 2443, Loss: 1.2605724334716797\n",
      "Iteration: 2444, Loss: 1.2605665922164917\n",
      "Iteration: 2445, Loss: 1.2605606317520142\n",
      "Iteration: 2446, Loss: 1.2605547904968262\n",
      "Iteration: 2447, Loss: 1.2605488300323486\n",
      "Iteration: 2448, Loss: 1.2605429887771606\n",
      "Iteration: 2449, Loss: 1.2605371475219727\n",
      "Iteration: 2450, Loss: 1.2605311870574951\n",
      "Iteration: 2451, Loss: 1.2605253458023071\n",
      "Iteration: 2452, Loss: 1.2605193853378296\n",
      "Iteration: 2453, Loss: 1.2605135440826416\n",
      "Iteration: 2454, Loss: 1.260507583618164\n",
      "Iteration: 2455, Loss: 1.260501742362976\n",
      "Iteration: 2456, Loss: 1.260495901107788\n",
      "Iteration: 2457, Loss: 1.2604899406433105\n",
      "Iteration: 2458, Loss: 1.2604840993881226\n",
      "Iteration: 2459, Loss: 1.2604782581329346\n",
      "Iteration: 2460, Loss: 1.260472297668457\n",
      "Iteration: 2461, Loss: 1.260466456413269\n",
      "Iteration: 2462, Loss: 1.260460615158081\n",
      "Iteration: 2463, Loss: 1.260454773902893\n",
      "Iteration: 2464, Loss: 1.2604488134384155\n",
      "Iteration: 2465, Loss: 1.2604429721832275\n",
      "Iteration: 2466, Loss: 1.2604371309280396\n",
      "Iteration: 2467, Loss: 1.2604312896728516\n",
      "Iteration: 2468, Loss: 1.260425329208374\n",
      "Iteration: 2469, Loss: 1.260419487953186\n",
      "Iteration: 2470, Loss: 1.260413646697998\n",
      "Iteration: 2471, Loss: 1.26040780544281\n",
      "Iteration: 2472, Loss: 1.260401964187622\n",
      "Iteration: 2473, Loss: 1.2603960037231445\n",
      "Iteration: 2474, Loss: 1.2603901624679565\n",
      "Iteration: 2475, Loss: 1.2603843212127686\n",
      "Iteration: 2476, Loss: 1.2603784799575806\n",
      "Iteration: 2477, Loss: 1.2603726387023926\n",
      "Iteration: 2478, Loss: 1.2603667974472046\n",
      "Iteration: 2479, Loss: 1.2603609561920166\n",
      "Iteration: 2480, Loss: 1.2603551149368286\n",
      "Iteration: 2481, Loss: 1.2603492736816406\n",
      "Iteration: 2482, Loss: 1.2603434324264526\n",
      "Iteration: 2483, Loss: 1.2603375911712646\n",
      "Iteration: 2484, Loss: 1.2603317499160767\n",
      "Iteration: 2485, Loss: 1.2603259086608887\n",
      "Iteration: 2486, Loss: 1.2603200674057007\n",
      "Iteration: 2487, Loss: 1.2603142261505127\n",
      "Iteration: 2488, Loss: 1.2603083848953247\n",
      "Iteration: 2489, Loss: 1.2603025436401367\n",
      "Iteration: 2490, Loss: 1.2602967023849487\n",
      "Iteration: 2491, Loss: 1.2602908611297607\n",
      "Iteration: 2492, Loss: 1.2602850198745728\n",
      "Iteration: 2493, Loss: 1.2602791786193848\n",
      "Iteration: 2494, Loss: 1.2602733373641968\n",
      "Iteration: 2495, Loss: 1.2602674961090088\n",
      "Iteration: 2496, Loss: 1.2602617740631104\n",
      "Iteration: 2497, Loss: 1.2602559328079224\n",
      "Iteration: 2498, Loss: 1.2602500915527344\n",
      "Iteration: 2499, Loss: 1.2602442502975464\n",
      "Iteration: 2500, Loss: 1.2602384090423584\n",
      "Iteration: 2501, Loss: 1.2602325677871704\n",
      "Iteration: 2502, Loss: 1.260226845741272\n",
      "Iteration: 2503, Loss: 1.260221004486084\n",
      "Iteration: 2504, Loss: 1.260215163230896\n",
      "Iteration: 2505, Loss: 1.260209321975708\n",
      "Iteration: 2506, Loss: 1.2602035999298096\n",
      "Iteration: 2507, Loss: 1.2601977586746216\n",
      "Iteration: 2508, Loss: 1.2601919174194336\n",
      "Iteration: 2509, Loss: 1.2601860761642456\n",
      "Iteration: 2510, Loss: 1.2601803541183472\n",
      "Iteration: 2511, Loss: 1.2601745128631592\n",
      "Iteration: 2512, Loss: 1.2601686716079712\n",
      "Iteration: 2513, Loss: 1.2601629495620728\n",
      "Iteration: 2514, Loss: 1.2601571083068848\n",
      "Iteration: 2515, Loss: 1.2601513862609863\n",
      "Iteration: 2516, Loss: 1.2601455450057983\n",
      "Iteration: 2517, Loss: 1.2601397037506104\n",
      "Iteration: 2518, Loss: 1.260133981704712\n",
      "Iteration: 2519, Loss: 1.260128140449524\n",
      "Iteration: 2520, Loss: 1.2601224184036255\n",
      "Iteration: 2521, Loss: 1.2601165771484375\n",
      "Iteration: 2522, Loss: 1.260110855102539\n",
      "Iteration: 2523, Loss: 1.260105013847351\n",
      "Iteration: 2524, Loss: 1.260099172592163\n",
      "Iteration: 2525, Loss: 1.2600934505462646\n",
      "Iteration: 2526, Loss: 1.2600877285003662\n",
      "Iteration: 2527, Loss: 1.2600818872451782\n",
      "Iteration: 2528, Loss: 1.2600761651992798\n",
      "Iteration: 2529, Loss: 1.2600703239440918\n",
      "Iteration: 2530, Loss: 1.2600646018981934\n",
      "Iteration: 2531, Loss: 1.2600587606430054\n",
      "Iteration: 2532, Loss: 1.260053038597107\n",
      "Iteration: 2533, Loss: 1.260047197341919\n",
      "Iteration: 2534, Loss: 1.2600414752960205\n",
      "Iteration: 2535, Loss: 1.260035753250122\n",
      "Iteration: 2536, Loss: 1.260029911994934\n",
      "Iteration: 2537, Loss: 1.2600241899490356\n",
      "Iteration: 2538, Loss: 1.2600184679031372\n",
      "Iteration: 2539, Loss: 1.2600126266479492\n",
      "Iteration: 2540, Loss: 1.2600069046020508\n",
      "Iteration: 2541, Loss: 1.2600011825561523\n",
      "Iteration: 2542, Loss: 1.2599953413009644\n",
      "Iteration: 2543, Loss: 1.259989619255066\n",
      "Iteration: 2544, Loss: 1.2599838972091675\n",
      "Iteration: 2545, Loss: 1.259978175163269\n",
      "Iteration: 2546, Loss: 1.259972333908081\n",
      "Iteration: 2547, Loss: 1.2599666118621826\n",
      "Iteration: 2548, Loss: 1.2599608898162842\n",
      "Iteration: 2549, Loss: 1.2599551677703857\n",
      "Iteration: 2550, Loss: 1.2599493265151978\n",
      "Iteration: 2551, Loss: 1.2599436044692993\n",
      "Iteration: 2552, Loss: 1.2599378824234009\n",
      "Iteration: 2553, Loss: 1.2599321603775024\n",
      "Iteration: 2554, Loss: 1.259926438331604\n",
      "Iteration: 2555, Loss: 1.2599207162857056\n",
      "Iteration: 2556, Loss: 1.2599149942398071\n",
      "Iteration: 2557, Loss: 1.2599091529846191\n",
      "Iteration: 2558, Loss: 1.2599034309387207\n",
      "Iteration: 2559, Loss: 1.2598977088928223\n",
      "Iteration: 2560, Loss: 1.2598919868469238\n",
      "Iteration: 2561, Loss: 1.2598862648010254\n",
      "Iteration: 2562, Loss: 1.259880542755127\n",
      "Iteration: 2563, Loss: 1.2598748207092285\n",
      "Iteration: 2564, Loss: 1.25986909866333\n",
      "Iteration: 2565, Loss: 1.2598633766174316\n",
      "Iteration: 2566, Loss: 1.2598576545715332\n",
      "Iteration: 2567, Loss: 1.2598519325256348\n",
      "Iteration: 2568, Loss: 1.2598462104797363\n",
      "Iteration: 2569, Loss: 1.259840488433838\n",
      "Iteration: 2570, Loss: 1.2598347663879395\n",
      "Iteration: 2571, Loss: 1.259829044342041\n",
      "Iteration: 2572, Loss: 1.2598233222961426\n",
      "Iteration: 2573, Loss: 1.2598176002502441\n",
      "Iteration: 2574, Loss: 1.2598119974136353\n",
      "Iteration: 2575, Loss: 1.2598062753677368\n",
      "Iteration: 2576, Loss: 1.2598005533218384\n",
      "Iteration: 2577, Loss: 1.25979483127594\n",
      "Iteration: 2578, Loss: 1.2597891092300415\n",
      "Iteration: 2579, Loss: 1.259783387184143\n",
      "Iteration: 2580, Loss: 1.2597776651382446\n",
      "Iteration: 2581, Loss: 1.2597720623016357\n",
      "Iteration: 2582, Loss: 1.2597663402557373\n",
      "Iteration: 2583, Loss: 1.2597606182098389\n",
      "Iteration: 2584, Loss: 1.2597548961639404\n",
      "Iteration: 2585, Loss: 1.259749174118042\n",
      "Iteration: 2586, Loss: 1.259743571281433\n",
      "Iteration: 2587, Loss: 1.2597378492355347\n",
      "Iteration: 2588, Loss: 1.2597321271896362\n",
      "Iteration: 2589, Loss: 1.2597264051437378\n",
      "Iteration: 2590, Loss: 1.259720802307129\n",
      "Iteration: 2591, Loss: 1.2597150802612305\n",
      "Iteration: 2592, Loss: 1.259709358215332\n",
      "Iteration: 2593, Loss: 1.2597037553787231\n",
      "Iteration: 2594, Loss: 1.2596980333328247\n",
      "Iteration: 2595, Loss: 1.2596923112869263\n",
      "Iteration: 2596, Loss: 1.2596867084503174\n",
      "Iteration: 2597, Loss: 1.259680986404419\n",
      "Iteration: 2598, Loss: 1.2596752643585205\n",
      "Iteration: 2599, Loss: 1.2596696615219116\n",
      "Iteration: 2600, Loss: 1.2596639394760132\n",
      "Iteration: 2601, Loss: 1.2596583366394043\n",
      "Iteration: 2602, Loss: 1.2596526145935059\n",
      "Iteration: 2603, Loss: 1.259647011756897\n",
      "Iteration: 2604, Loss: 1.2596412897109985\n",
      "Iteration: 2605, Loss: 1.2596356868743896\n",
      "Iteration: 2606, Loss: 1.2596299648284912\n",
      "Iteration: 2607, Loss: 1.2596242427825928\n",
      "Iteration: 2608, Loss: 1.2596186399459839\n",
      "Iteration: 2609, Loss: 1.2596129179000854\n",
      "Iteration: 2610, Loss: 1.2596073150634766\n",
      "Iteration: 2611, Loss: 1.2596017122268677\n",
      "Iteration: 2612, Loss: 1.2595959901809692\n",
      "Iteration: 2613, Loss: 1.2595903873443604\n",
      "Iteration: 2614, Loss: 1.259584665298462\n",
      "Iteration: 2615, Loss: 1.259579062461853\n",
      "Iteration: 2616, Loss: 1.2595733404159546\n",
      "Iteration: 2617, Loss: 1.2595677375793457\n",
      "Iteration: 2618, Loss: 1.2595621347427368\n",
      "Iteration: 2619, Loss: 1.2595564126968384\n",
      "Iteration: 2620, Loss: 1.2595508098602295\n",
      "Iteration: 2621, Loss: 1.2595452070236206\n",
      "Iteration: 2622, Loss: 1.2595394849777222\n",
      "Iteration: 2623, Loss: 1.2595338821411133\n",
      "Iteration: 2624, Loss: 1.2595282793045044\n",
      "Iteration: 2625, Loss: 1.259522557258606\n",
      "Iteration: 2626, Loss: 1.259516954421997\n",
      "Iteration: 2627, Loss: 1.2595113515853882\n",
      "Iteration: 2628, Loss: 1.2595057487487793\n",
      "Iteration: 2629, Loss: 1.2595000267028809\n",
      "Iteration: 2630, Loss: 1.259494423866272\n",
      "Iteration: 2631, Loss: 1.259488821029663\n",
      "Iteration: 2632, Loss: 1.2594832181930542\n",
      "Iteration: 2633, Loss: 1.2594774961471558\n",
      "Iteration: 2634, Loss: 1.2594718933105469\n",
      "Iteration: 2635, Loss: 1.259466290473938\n",
      "Iteration: 2636, Loss: 1.259460687637329\n",
      "Iteration: 2637, Loss: 1.2594550848007202\n",
      "Iteration: 2638, Loss: 1.2594494819641113\n",
      "Iteration: 2639, Loss: 1.259443759918213\n",
      "Iteration: 2640, Loss: 1.259438157081604\n",
      "Iteration: 2641, Loss: 1.2594325542449951\n",
      "Iteration: 2642, Loss: 1.2594269514083862\n",
      "Iteration: 2643, Loss: 1.2594213485717773\n",
      "Iteration: 2644, Loss: 1.2594157457351685\n",
      "Iteration: 2645, Loss: 1.2594101428985596\n",
      "Iteration: 2646, Loss: 1.2594045400619507\n",
      "Iteration: 2647, Loss: 1.2593989372253418\n",
      "Iteration: 2648, Loss: 1.259393334388733\n",
      "Iteration: 2649, Loss: 1.259387731552124\n",
      "Iteration: 2650, Loss: 1.2593821287155151\n",
      "Iteration: 2651, Loss: 1.2593765258789062\n",
      "Iteration: 2652, Loss: 1.2593709230422974\n",
      "Iteration: 2653, Loss: 1.2593653202056885\n",
      "Iteration: 2654, Loss: 1.2593597173690796\n",
      "Iteration: 2655, Loss: 1.2593541145324707\n",
      "Iteration: 2656, Loss: 1.2593485116958618\n",
      "Iteration: 2657, Loss: 1.259342908859253\n",
      "Iteration: 2658, Loss: 1.259337306022644\n",
      "Iteration: 2659, Loss: 1.2593317031860352\n",
      "Iteration: 2660, Loss: 1.2593261003494263\n",
      "Iteration: 2661, Loss: 1.2593204975128174\n",
      "Iteration: 2662, Loss: 1.259315013885498\n",
      "Iteration: 2663, Loss: 1.2593094110488892\n",
      "Iteration: 2664, Loss: 1.2593038082122803\n",
      "Iteration: 2665, Loss: 1.2592982053756714\n",
      "Iteration: 2666, Loss: 1.2592926025390625\n",
      "Iteration: 2667, Loss: 1.2592869997024536\n",
      "Iteration: 2668, Loss: 1.2592815160751343\n",
      "Iteration: 2669, Loss: 1.2592759132385254\n",
      "Iteration: 2670, Loss: 1.2592703104019165\n",
      "Iteration: 2671, Loss: 1.2592647075653076\n",
      "Iteration: 2672, Loss: 1.2592592239379883\n",
      "Iteration: 2673, Loss: 1.2592536211013794\n",
      "Iteration: 2674, Loss: 1.2592480182647705\n",
      "Iteration: 2675, Loss: 1.2592424154281616\n",
      "Iteration: 2676, Loss: 1.2592369318008423\n",
      "Iteration: 2677, Loss: 1.2592313289642334\n",
      "Iteration: 2678, Loss: 1.2592257261276245\n",
      "Iteration: 2679, Loss: 1.2592202425003052\n",
      "Iteration: 2680, Loss: 1.2592146396636963\n",
      "Iteration: 2681, Loss: 1.2592090368270874\n",
      "Iteration: 2682, Loss: 1.259203553199768\n",
      "Iteration: 2683, Loss: 1.2591979503631592\n",
      "Iteration: 2684, Loss: 1.2591923475265503\n",
      "Iteration: 2685, Loss: 1.259186863899231\n",
      "Iteration: 2686, Loss: 1.259181261062622\n",
      "Iteration: 2687, Loss: 1.2591757774353027\n",
      "Iteration: 2688, Loss: 1.2591701745986938\n",
      "Iteration: 2689, Loss: 1.259164571762085\n",
      "Iteration: 2690, Loss: 1.2591590881347656\n",
      "Iteration: 2691, Loss: 1.2591534852981567\n",
      "Iteration: 2692, Loss: 1.2591480016708374\n",
      "Iteration: 2693, Loss: 1.2591423988342285\n",
      "Iteration: 2694, Loss: 1.2591369152069092\n",
      "Iteration: 2695, Loss: 1.2591313123703003\n",
      "Iteration: 2696, Loss: 1.259125828742981\n",
      "Iteration: 2697, Loss: 1.259120225906372\n",
      "Iteration: 2698, Loss: 1.2591147422790527\n",
      "Iteration: 2699, Loss: 1.2591091394424438\n",
      "Iteration: 2700, Loss: 1.2591036558151245\n",
      "Iteration: 2701, Loss: 1.259097933769226\n",
      "Iteration: 2702, Loss: 1.2590924501419067\n",
      "Iteration: 2703, Loss: 1.2590868473052979\n",
      "Iteration: 2704, Loss: 1.2590813636779785\n",
      "Iteration: 2705, Loss: 1.2590758800506592\n",
      "Iteration: 2706, Loss: 1.2590702772140503\n",
      "Iteration: 2707, Loss: 1.259064793586731\n",
      "Iteration: 2708, Loss: 1.259059190750122\n",
      "Iteration: 2709, Loss: 1.2590535879135132\n",
      "Iteration: 2710, Loss: 1.2590481042861938\n",
      "Iteration: 2711, Loss: 1.259042501449585\n",
      "Iteration: 2712, Loss: 1.2590370178222656\n",
      "Iteration: 2713, Loss: 1.2590315341949463\n",
      "Iteration: 2714, Loss: 1.2590258121490479\n",
      "Iteration: 2715, Loss: 1.2590203285217285\n",
      "Iteration: 2716, Loss: 1.2590148448944092\n",
      "Iteration: 2717, Loss: 1.2590092420578003\n",
      "Iteration: 2718, Loss: 1.259003758430481\n",
      "Iteration: 2719, Loss: 1.2589982748031616\n",
      "Iteration: 2720, Loss: 1.2589925527572632\n",
      "Iteration: 2721, Loss: 1.2589870691299438\n",
      "Iteration: 2722, Loss: 1.258981466293335\n",
      "Iteration: 2723, Loss: 1.2589757442474365\n",
      "Iteration: 2724, Loss: 1.2589702606201172\n",
      "Iteration: 2725, Loss: 1.2589646577835083\n",
      "Iteration: 2726, Loss: 1.2589589357376099\n",
      "Iteration: 2727, Loss: 1.2589534521102905\n",
      "Iteration: 2728, Loss: 1.258947730064392\n",
      "Iteration: 2729, Loss: 1.2589420080184937\n",
      "Iteration: 2730, Loss: 1.2589364051818848\n",
      "Iteration: 2731, Loss: 1.2589304447174072\n",
      "Iteration: 2732, Loss: 1.2589246034622192\n",
      "Iteration: 2733, Loss: 1.2589187622070312\n",
      "Iteration: 2734, Loss: 1.2589125633239746\n",
      "Iteration: 2735, Loss: 1.2589061260223389\n",
      "Iteration: 2736, Loss: 1.2588995695114136\n",
      "Iteration: 2737, Loss: 1.258892297744751\n",
      "Iteration: 2738, Loss: 1.2588837146759033\n",
      "Iteration: 2739, Loss: 1.2588725090026855\n",
      "Iteration: 2740, Loss: 1.2588542699813843\n",
      "Iteration: 2741, Loss: 1.2588075399398804\n",
      "Iteration: 2742, Loss: 1.2584949731826782\n",
      "Iteration: 2743, Loss: 1.2436206340789795\n",
      "Iteration: 2744, Loss: 1.2588891983032227\n",
      "Iteration: 2745, Loss: 1.2589335441589355\n",
      "Iteration: 2746, Loss: 1.5089683532714844\n",
      "Iteration: 2747, Loss: 1.5090175867080688\n",
      "Iteration: 2748, Loss: 1.5090670585632324\n",
      "Iteration: 2749, Loss: 1.509116768836975\n",
      "Iteration: 2750, Loss: 1.5091655254364014\n",
      "Iteration: 2751, Loss: 1.5092124938964844\n",
      "Iteration: 2752, Loss: 1.5092570781707764\n",
      "Iteration: 2753, Loss: 1.5092990398406982\n",
      "Iteration: 2754, Loss: 1.5093380212783813\n",
      "Iteration: 2755, Loss: 1.5093741416931152\n",
      "Iteration: 2756, Loss: 1.5094072818756104\n",
      "Iteration: 2757, Loss: 1.5094374418258667\n",
      "Iteration: 2758, Loss: 1.5094647407531738\n",
      "Iteration: 2759, Loss: 1.5094894170761108\n",
      "Iteration: 2760, Loss: 1.5095114707946777\n",
      "Iteration: 2761, Loss: 1.5095312595367432\n",
      "Iteration: 2762, Loss: 1.5095486640930176\n",
      "Iteration: 2763, Loss: 1.5095640420913696\n",
      "Iteration: 2764, Loss: 1.5095775127410889\n",
      "Iteration: 2765, Loss: 1.5095891952514648\n",
      "Iteration: 2766, Loss: 1.509599208831787\n",
      "Iteration: 2767, Loss: 1.5096076726913452\n",
      "Iteration: 2768, Loss: 1.5096148252487183\n",
      "Iteration: 2769, Loss: 1.5096207857131958\n",
      "Iteration: 2770, Loss: 1.5096255540847778\n",
      "Iteration: 2771, Loss: 1.509629249572754\n",
      "Iteration: 2772, Loss: 1.5096319913864136\n",
      "Iteration: 2773, Loss: 1.5096338987350464\n",
      "Iteration: 2774, Loss: 1.509635090827942\n",
      "Iteration: 2775, Loss: 1.5096355676651\n",
      "Iteration: 2776, Loss: 1.509635329246521\n",
      "Iteration: 2777, Loss: 1.5096344947814941\n",
      "Iteration: 2778, Loss: 1.5096333026885986\n",
      "Iteration: 2779, Loss: 1.5096315145492554\n",
      "Iteration: 2780, Loss: 1.509629249572754\n",
      "Iteration: 2781, Loss: 1.5096267461776733\n",
      "Iteration: 2782, Loss: 1.5096237659454346\n",
      "Iteration: 2783, Loss: 1.5096205472946167\n",
      "Iteration: 2784, Loss: 1.5096169710159302\n",
      "Iteration: 2785, Loss: 1.5096131563186646\n",
      "Iteration: 2786, Loss: 1.5096092224121094\n",
      "Iteration: 2787, Loss: 1.5096049308776855\n",
      "Iteration: 2788, Loss: 1.5096006393432617\n",
      "Iteration: 2789, Loss: 1.5095959901809692\n",
      "Iteration: 2790, Loss: 1.5095913410186768\n",
      "Iteration: 2791, Loss: 1.5095864534378052\n",
      "Iteration: 2792, Loss: 1.509581446647644\n",
      "Iteration: 2793, Loss: 1.509576439857483\n",
      "Iteration: 2794, Loss: 1.5095711946487427\n",
      "Iteration: 2795, Loss: 1.5095659494400024\n",
      "Iteration: 2796, Loss: 1.5095605850219727\n",
      "Iteration: 2797, Loss: 1.5095552206039429\n",
      "Iteration: 2798, Loss: 1.5095497369766235\n",
      "Iteration: 2799, Loss: 1.5095441341400146\n",
      "Iteration: 2800, Loss: 1.5095385313034058\n",
      "Iteration: 2801, Loss: 1.5095329284667969\n",
      "Iteration: 2802, Loss: 1.5095272064208984\n",
      "Iteration: 2803, Loss: 1.509521484375\n",
      "Iteration: 2804, Loss: 1.5095157623291016\n",
      "Iteration: 2805, Loss: 1.5095100402832031\n",
      "Iteration: 2806, Loss: 1.5095041990280151\n",
      "Iteration: 2807, Loss: 1.5094983577728271\n",
      "Iteration: 2808, Loss: 1.5094925165176392\n",
      "Iteration: 2809, Loss: 1.5094866752624512\n",
      "Iteration: 2810, Loss: 1.5094808340072632\n",
      "Iteration: 2811, Loss: 1.5094748735427856\n",
      "Iteration: 2812, Loss: 1.5094690322875977\n",
      "Iteration: 2813, Loss: 1.5094630718231201\n",
      "Iteration: 2814, Loss: 1.5094571113586426\n",
      "Iteration: 2815, Loss: 1.5094512701034546\n",
      "Iteration: 2816, Loss: 1.509445309638977\n",
      "Iteration: 2817, Loss: 1.5094393491744995\n",
      "Iteration: 2818, Loss: 1.509433388710022\n",
      "Iteration: 2819, Loss: 1.5094274282455444\n",
      "Iteration: 2820, Loss: 1.509421467781067\n",
      "Iteration: 2821, Loss: 1.5094155073165894\n",
      "Iteration: 2822, Loss: 1.5094095468521118\n",
      "Iteration: 2823, Loss: 1.5094035863876343\n",
      "Iteration: 2824, Loss: 1.5093976259231567\n",
      "Iteration: 2825, Loss: 1.5093916654586792\n",
      "Iteration: 2826, Loss: 1.5093857049942017\n",
      "Iteration: 2827, Loss: 1.5093797445297241\n",
      "Iteration: 2828, Loss: 1.5093737840652466\n",
      "Iteration: 2829, Loss: 1.5093677043914795\n",
      "Iteration: 2830, Loss: 1.509361743927002\n",
      "Iteration: 2831, Loss: 1.5093557834625244\n",
      "Iteration: 2832, Loss: 1.5093498229980469\n",
      "Iteration: 2833, Loss: 1.5093438625335693\n",
      "Iteration: 2834, Loss: 1.5093379020690918\n",
      "Iteration: 2835, Loss: 1.5093319416046143\n",
      "Iteration: 2836, Loss: 1.5093259811401367\n",
      "Iteration: 2837, Loss: 1.5093200206756592\n",
      "Iteration: 2838, Loss: 1.5093140602111816\n",
      "Iteration: 2839, Loss: 1.509308099746704\n",
      "Iteration: 2840, Loss: 1.5093021392822266\n",
      "Iteration: 2841, Loss: 1.509296178817749\n",
      "Iteration: 2842, Loss: 1.5092902183532715\n",
      "Iteration: 2843, Loss: 1.509284257888794\n",
      "Iteration: 2844, Loss: 1.5092782974243164\n",
      "Iteration: 2845, Loss: 1.5092723369598389\n",
      "Iteration: 2846, Loss: 1.5092663764953613\n",
      "Iteration: 2847, Loss: 1.5092605352401733\n",
      "Iteration: 2848, Loss: 1.5092545747756958\n",
      "Iteration: 2849, Loss: 1.5092486143112183\n",
      "Iteration: 2850, Loss: 1.5092426538467407\n",
      "Iteration: 2851, Loss: 1.5092366933822632\n",
      "Iteration: 2852, Loss: 1.5092307329177856\n",
      "Iteration: 2853, Loss: 1.5092248916625977\n",
      "Iteration: 2854, Loss: 1.5092189311981201\n",
      "Iteration: 2855, Loss: 1.5092129707336426\n",
      "Iteration: 2856, Loss: 1.5092071294784546\n",
      "Iteration: 2857, Loss: 1.509201169013977\n",
      "Iteration: 2858, Loss: 1.5091952085494995\n",
      "Iteration: 2859, Loss: 1.5091893672943115\n",
      "Iteration: 2860, Loss: 1.509183406829834\n",
      "Iteration: 2861, Loss: 1.5091774463653564\n",
      "Iteration: 2862, Loss: 1.5091716051101685\n",
      "Iteration: 2863, Loss: 1.509165644645691\n",
      "Iteration: 2864, Loss: 1.509159803390503\n",
      "Iteration: 2865, Loss: 1.5091538429260254\n",
      "Iteration: 2866, Loss: 1.5091480016708374\n",
      "Iteration: 2867, Loss: 1.5091420412063599\n",
      "Iteration: 2868, Loss: 1.5091361999511719\n",
      "Iteration: 2869, Loss: 1.5091302394866943\n",
      "Iteration: 2870, Loss: 1.5091243982315063\n",
      "Iteration: 2871, Loss: 1.5091184377670288\n",
      "Iteration: 2872, Loss: 1.5091125965118408\n",
      "Iteration: 2873, Loss: 1.5091067552566528\n",
      "Iteration: 2874, Loss: 1.5091007947921753\n",
      "Iteration: 2875, Loss: 1.5090949535369873\n",
      "Iteration: 2876, Loss: 1.5090891122817993\n",
      "Iteration: 2877, Loss: 1.5090831518173218\n",
      "Iteration: 2878, Loss: 1.5090773105621338\n",
      "Iteration: 2879, Loss: 1.5090714693069458\n",
      "Iteration: 2880, Loss: 1.5090656280517578\n",
      "Iteration: 2881, Loss: 1.5090596675872803\n",
      "Iteration: 2882, Loss: 1.5090538263320923\n",
      "Iteration: 2883, Loss: 1.5090479850769043\n",
      "Iteration: 2884, Loss: 1.5090421438217163\n",
      "Iteration: 2885, Loss: 1.5090363025665283\n",
      "Iteration: 2886, Loss: 1.5090303421020508\n",
      "Iteration: 2887, Loss: 1.5090245008468628\n",
      "Iteration: 2888, Loss: 1.5090186595916748\n",
      "Iteration: 2889, Loss: 1.5090128183364868\n",
      "Iteration: 2890, Loss: 1.5090069770812988\n",
      "Iteration: 2891, Loss: 1.5090011358261108\n",
      "Iteration: 2892, Loss: 1.5089952945709229\n",
      "Iteration: 2893, Loss: 1.5089894533157349\n",
      "Iteration: 2894, Loss: 1.5089836120605469\n",
      "Iteration: 2895, Loss: 1.5089777708053589\n",
      "Iteration: 2896, Loss: 1.508971929550171\n",
      "Iteration: 2897, Loss: 1.508966088294983\n",
      "Iteration: 2898, Loss: 1.508960247039795\n",
      "Iteration: 2899, Loss: 1.508954405784607\n",
      "Iteration: 2900, Loss: 1.5089486837387085\n",
      "Iteration: 2901, Loss: 1.5089428424835205\n",
      "Iteration: 2902, Loss: 1.5089370012283325\n",
      "Iteration: 2903, Loss: 1.5089311599731445\n",
      "Iteration: 2904, Loss: 1.5089253187179565\n",
      "Iteration: 2905, Loss: 1.5089194774627686\n",
      "Iteration: 2906, Loss: 1.5089137554168701\n",
      "Iteration: 2907, Loss: 1.5089079141616821\n",
      "Iteration: 2908, Loss: 1.5089020729064941\n",
      "Iteration: 2909, Loss: 1.5088962316513062\n",
      "Iteration: 2910, Loss: 1.5088905096054077\n",
      "Iteration: 2911, Loss: 1.5088846683502197\n",
      "Iteration: 2912, Loss: 1.5088788270950317\n",
      "Iteration: 2913, Loss: 1.5088731050491333\n",
      "Iteration: 2914, Loss: 1.5088672637939453\n",
      "Iteration: 2915, Loss: 1.5088614225387573\n",
      "Iteration: 2916, Loss: 1.5088557004928589\n",
      "Iteration: 2917, Loss: 1.508849859237671\n",
      "Iteration: 2918, Loss: 1.5088441371917725\n",
      "Iteration: 2919, Loss: 1.5088382959365845\n",
      "Iteration: 2920, Loss: 1.5088324546813965\n",
      "Iteration: 2921, Loss: 1.508826732635498\n",
      "Iteration: 2922, Loss: 1.50882089138031\n",
      "Iteration: 2923, Loss: 1.5088151693344116\n",
      "Iteration: 2924, Loss: 1.5088093280792236\n",
      "Iteration: 2925, Loss: 1.5088036060333252\n",
      "Iteration: 2926, Loss: 1.5087978839874268\n",
      "Iteration: 2927, Loss: 1.5087920427322388\n",
      "Iteration: 2928, Loss: 1.5087863206863403\n",
      "Iteration: 2929, Loss: 1.5087804794311523\n",
      "Iteration: 2930, Loss: 1.508774757385254\n",
      "Iteration: 2931, Loss: 1.5087690353393555\n",
      "Iteration: 2932, Loss: 1.5087631940841675\n",
      "Iteration: 2933, Loss: 1.508757472038269\n",
      "Iteration: 2934, Loss: 1.5087517499923706\n",
      "Iteration: 2935, Loss: 1.5087459087371826\n",
      "Iteration: 2936, Loss: 1.5087401866912842\n",
      "Iteration: 2937, Loss: 1.5087344646453857\n",
      "Iteration: 2938, Loss: 1.5087287425994873\n",
      "Iteration: 2939, Loss: 1.5087229013442993\n",
      "Iteration: 2940, Loss: 1.5087171792984009\n",
      "Iteration: 2941, Loss: 1.5087114572525024\n",
      "Iteration: 2942, Loss: 1.508705735206604\n",
      "Iteration: 2943, Loss: 1.508699893951416\n",
      "Iteration: 2944, Loss: 1.5086941719055176\n",
      "Iteration: 2945, Loss: 1.5086884498596191\n",
      "Iteration: 2946, Loss: 1.5086827278137207\n",
      "Iteration: 2947, Loss: 1.5086770057678223\n",
      "Iteration: 2948, Loss: 1.5086712837219238\n",
      "Iteration: 2949, Loss: 1.5086655616760254\n",
      "Iteration: 2950, Loss: 1.508659839630127\n",
      "Iteration: 2951, Loss: 1.5086541175842285\n",
      "Iteration: 2952, Loss: 1.50864839553833\n",
      "Iteration: 2953, Loss: 1.5086426734924316\n",
      "Iteration: 2954, Loss: 1.5086369514465332\n",
      "Iteration: 2955, Loss: 1.5086312294006348\n",
      "Iteration: 2956, Loss: 1.5086255073547363\n",
      "Iteration: 2957, Loss: 1.508619785308838\n",
      "Iteration: 2958, Loss: 1.5086140632629395\n",
      "Iteration: 2959, Loss: 1.508608341217041\n",
      "Iteration: 2960, Loss: 1.5086026191711426\n",
      "Iteration: 2961, Loss: 1.5085968971252441\n",
      "Iteration: 2962, Loss: 1.5085911750793457\n",
      "Iteration: 2963, Loss: 1.5085854530334473\n",
      "Iteration: 2964, Loss: 1.5085797309875488\n",
      "Iteration: 2965, Loss: 1.50857412815094\n",
      "Iteration: 2966, Loss: 1.5085684061050415\n",
      "Iteration: 2967, Loss: 1.508562684059143\n",
      "Iteration: 2968, Loss: 1.5085569620132446\n",
      "Iteration: 2969, Loss: 1.5085512399673462\n",
      "Iteration: 2970, Loss: 1.5085456371307373\n",
      "Iteration: 2971, Loss: 1.5085399150848389\n",
      "Iteration: 2972, Loss: 1.5085341930389404\n",
      "Iteration: 2973, Loss: 1.5085285902023315\n",
      "Iteration: 2974, Loss: 1.508522868156433\n",
      "Iteration: 2975, Loss: 1.5085171461105347\n",
      "Iteration: 2976, Loss: 1.5085114240646362\n",
      "Iteration: 2977, Loss: 1.5085058212280273\n",
      "Iteration: 2978, Loss: 1.508500099182129\n",
      "Iteration: 2979, Loss: 1.50849449634552\n",
      "Iteration: 2980, Loss: 1.5084887742996216\n",
      "Iteration: 2981, Loss: 1.5084830522537231\n",
      "Iteration: 2982, Loss: 1.5084774494171143\n",
      "Iteration: 2983, Loss: 1.5084717273712158\n",
      "Iteration: 2984, Loss: 1.508466124534607\n",
      "Iteration: 2985, Loss: 1.5084604024887085\n",
      "Iteration: 2986, Loss: 1.5084547996520996\n",
      "Iteration: 2987, Loss: 1.5084490776062012\n",
      "Iteration: 2988, Loss: 1.5084434747695923\n",
      "Iteration: 2989, Loss: 1.5084377527236938\n",
      "Iteration: 2990, Loss: 1.508432149887085\n",
      "Iteration: 2991, Loss: 1.5084264278411865\n",
      "Iteration: 2992, Loss: 1.5084208250045776\n",
      "Iteration: 2993, Loss: 1.5084151029586792\n",
      "Iteration: 2994, Loss: 1.5084095001220703\n",
      "Iteration: 2995, Loss: 1.5084038972854614\n",
      "Iteration: 2996, Loss: 1.508398175239563\n",
      "Iteration: 2997, Loss: 1.508392572402954\n",
      "Iteration: 2998, Loss: 1.5083868503570557\n",
      "Iteration: 2999, Loss: 1.5083812475204468\n",
      "Iteration: 3000, Loss: 1.508375644683838\n",
      "Iteration: 3001, Loss: 1.5083699226379395\n",
      "Iteration: 3002, Loss: 1.5083643198013306\n",
      "Iteration: 3003, Loss: 1.5083587169647217\n",
      "Iteration: 3004, Loss: 1.5083531141281128\n",
      "Iteration: 3005, Loss: 1.5083473920822144\n",
      "Iteration: 3006, Loss: 1.5083417892456055\n",
      "Iteration: 3007, Loss: 1.5083361864089966\n",
      "Iteration: 3008, Loss: 1.5083305835723877\n",
      "Iteration: 3009, Loss: 1.5083249807357788\n",
      "Iteration: 3010, Loss: 1.5083192586898804\n",
      "Iteration: 3011, Loss: 1.5083136558532715\n",
      "Iteration: 3012, Loss: 1.5083080530166626\n",
      "Iteration: 3013, Loss: 1.5083024501800537\n",
      "Iteration: 3014, Loss: 1.5082968473434448\n",
      "Iteration: 3015, Loss: 1.508291244506836\n",
      "Iteration: 3016, Loss: 1.508285641670227\n",
      "Iteration: 3017, Loss: 1.5082799196243286\n",
      "Iteration: 3018, Loss: 1.5082743167877197\n",
      "Iteration: 3019, Loss: 1.5082687139511108\n",
      "Iteration: 3020, Loss: 1.508263111114502\n",
      "Iteration: 3021, Loss: 1.508257508277893\n",
      "Iteration: 3022, Loss: 1.5082519054412842\n",
      "Iteration: 3023, Loss: 1.5082463026046753\n",
      "Iteration: 3024, Loss: 1.5082406997680664\n",
      "Iteration: 3025, Loss: 1.5082350969314575\n",
      "Iteration: 3026, Loss: 1.5082294940948486\n",
      "Iteration: 3027, Loss: 1.5082238912582397\n",
      "Iteration: 3028, Loss: 1.5082182884216309\n",
      "Iteration: 3029, Loss: 1.5082128047943115\n",
      "Iteration: 3030, Loss: 1.5082072019577026\n",
      "Iteration: 3031, Loss: 1.5082015991210938\n",
      "Iteration: 3032, Loss: 1.5081959962844849\n",
      "Iteration: 3033, Loss: 1.508190393447876\n",
      "Iteration: 3034, Loss: 1.508184790611267\n",
      "Iteration: 3035, Loss: 1.5081791877746582\n",
      "Iteration: 3036, Loss: 1.5081737041473389\n",
      "Iteration: 3037, Loss: 1.50816810131073\n",
      "Iteration: 3038, Loss: 1.508162498474121\n",
      "Iteration: 3039, Loss: 1.5081568956375122\n",
      "Iteration: 3040, Loss: 1.5081512928009033\n",
      "Iteration: 3041, Loss: 1.508145809173584\n",
      "Iteration: 3042, Loss: 1.508140206336975\n",
      "Iteration: 3043, Loss: 1.5081346035003662\n",
      "Iteration: 3044, Loss: 1.5081290006637573\n",
      "Iteration: 3045, Loss: 1.508123517036438\n",
      "Iteration: 3046, Loss: 1.508117914199829\n",
      "Iteration: 3047, Loss: 1.5081123113632202\n",
      "Iteration: 3048, Loss: 1.5081068277359009\n",
      "Iteration: 3049, Loss: 1.508101224899292\n",
      "Iteration: 3050, Loss: 1.508095622062683\n",
      "Iteration: 3051, Loss: 1.5080901384353638\n",
      "Iteration: 3052, Loss: 1.5080845355987549\n",
      "Iteration: 3053, Loss: 1.5080790519714355\n",
      "Iteration: 3054, Loss: 1.5080734491348267\n",
      "Iteration: 3055, Loss: 1.5080678462982178\n",
      "Iteration: 3056, Loss: 1.5080623626708984\n",
      "Iteration: 3057, Loss: 1.5080567598342896\n",
      "Iteration: 3058, Loss: 1.5080512762069702\n",
      "Iteration: 3059, Loss: 1.5080456733703613\n",
      "Iteration: 3060, Loss: 1.508040189743042\n",
      "Iteration: 3061, Loss: 1.508034586906433\n",
      "Iteration: 3062, Loss: 1.5080291032791138\n",
      "Iteration: 3063, Loss: 1.5080235004425049\n",
      "Iteration: 3064, Loss: 1.5080180168151855\n",
      "Iteration: 3065, Loss: 1.5080124139785767\n",
      "Iteration: 3066, Loss: 1.5080069303512573\n",
      "Iteration: 3067, Loss: 1.508001446723938\n",
      "Iteration: 3068, Loss: 1.507995843887329\n",
      "Iteration: 3069, Loss: 1.5079903602600098\n",
      "Iteration: 3070, Loss: 1.5079847574234009\n",
      "Iteration: 3071, Loss: 1.5079792737960815\n",
      "Iteration: 3072, Loss: 1.5079737901687622\n",
      "Iteration: 3073, Loss: 1.5079681873321533\n",
      "Iteration: 3074, Loss: 1.507962703704834\n",
      "Iteration: 3075, Loss: 1.5079572200775146\n",
      "Iteration: 3076, Loss: 1.5079517364501953\n",
      "Iteration: 3077, Loss: 1.5079461336135864\n",
      "Iteration: 3078, Loss: 1.507940649986267\n",
      "Iteration: 3079, Loss: 1.5079351663589478\n",
      "Iteration: 3080, Loss: 1.5079295635223389\n",
      "Iteration: 3081, Loss: 1.5079240798950195\n",
      "Iteration: 3082, Loss: 1.5079185962677002\n",
      "Iteration: 3083, Loss: 1.5079131126403809\n",
      "Iteration: 3084, Loss: 1.5079076290130615\n",
      "Iteration: 3085, Loss: 1.5079020261764526\n",
      "Iteration: 3086, Loss: 1.5078965425491333\n",
      "Iteration: 3087, Loss: 1.507891058921814\n",
      "Iteration: 3088, Loss: 1.5078855752944946\n",
      "Iteration: 3089, Loss: 1.5078800916671753\n",
      "Iteration: 3090, Loss: 1.507874608039856\n",
      "Iteration: 3091, Loss: 1.5078691244125366\n",
      "Iteration: 3092, Loss: 1.5078636407852173\n",
      "Iteration: 3093, Loss: 1.507858157157898\n",
      "Iteration: 3094, Loss: 1.507852554321289\n",
      "Iteration: 3095, Loss: 1.5078470706939697\n",
      "Iteration: 3096, Loss: 1.5078415870666504\n",
      "Iteration: 3097, Loss: 1.507836103439331\n",
      "Iteration: 3098, Loss: 1.5078306198120117\n",
      "Iteration: 3099, Loss: 1.5078251361846924\n",
      "Iteration: 3100, Loss: 1.507819652557373\n",
      "Iteration: 3101, Loss: 1.5078141689300537\n",
      "Iteration: 3102, Loss: 1.5078086853027344\n",
      "Iteration: 3103, Loss: 1.507803201675415\n",
      "Iteration: 3104, Loss: 1.5077978372573853\n",
      "Iteration: 3105, Loss: 1.507792353630066\n",
      "Iteration: 3106, Loss: 1.5077868700027466\n",
      "Iteration: 3107, Loss: 1.5077813863754272\n",
      "Iteration: 3108, Loss: 1.507775902748108\n",
      "Iteration: 3109, Loss: 1.5077704191207886\n",
      "Iteration: 3110, Loss: 1.5077649354934692\n",
      "Iteration: 3111, Loss: 1.50775945186615\n",
      "Iteration: 3112, Loss: 1.5077539682388306\n",
      "Iteration: 3113, Loss: 1.5077486038208008\n",
      "Iteration: 3114, Loss: 1.5077431201934814\n",
      "Iteration: 3115, Loss: 1.507737636566162\n",
      "Iteration: 3116, Loss: 1.5077321529388428\n",
      "Iteration: 3117, Loss: 1.507726788520813\n",
      "Iteration: 3118, Loss: 1.5077213048934937\n",
      "Iteration: 3119, Loss: 1.5077158212661743\n",
      "Iteration: 3120, Loss: 1.507710337638855\n",
      "Iteration: 3121, Loss: 1.5077049732208252\n",
      "Iteration: 3122, Loss: 1.5076994895935059\n",
      "Iteration: 3123, Loss: 1.5076940059661865\n",
      "Iteration: 3124, Loss: 1.5076885223388672\n",
      "Iteration: 3125, Loss: 1.5076831579208374\n",
      "Iteration: 3126, Loss: 1.507677674293518\n",
      "Iteration: 3127, Loss: 1.5076721906661987\n",
      "Iteration: 3128, Loss: 1.507666826248169\n",
      "Iteration: 3129, Loss: 1.5076613426208496\n",
      "Iteration: 3130, Loss: 1.5076559782028198\n",
      "Iteration: 3131, Loss: 1.5076504945755005\n",
      "Iteration: 3132, Loss: 1.5076450109481812\n",
      "Iteration: 3133, Loss: 1.5076396465301514\n",
      "Iteration: 3134, Loss: 1.507634162902832\n",
      "Iteration: 3135, Loss: 1.5076287984848022\n",
      "Iteration: 3136, Loss: 1.507623314857483\n",
      "Iteration: 3137, Loss: 1.5076179504394531\n",
      "Iteration: 3138, Loss: 1.5076124668121338\n",
      "Iteration: 3139, Loss: 1.507607102394104\n",
      "Iteration: 3140, Loss: 1.5076016187667847\n",
      "Iteration: 3141, Loss: 1.5075962543487549\n",
      "Iteration: 3142, Loss: 1.5075907707214355\n",
      "Iteration: 3143, Loss: 1.5075854063034058\n",
      "Iteration: 3144, Loss: 1.5075799226760864\n",
      "Iteration: 3145, Loss: 1.5075745582580566\n",
      "Iteration: 3146, Loss: 1.5075690746307373\n",
      "Iteration: 3147, Loss: 1.5075637102127075\n",
      "Iteration: 3148, Loss: 1.5075583457946777\n",
      "Iteration: 3149, Loss: 1.5075528621673584\n",
      "Iteration: 3150, Loss: 1.5075474977493286\n",
      "Iteration: 3151, Loss: 1.5075421333312988\n",
      "Iteration: 3152, Loss: 1.5075366497039795\n",
      "Iteration: 3153, Loss: 1.5075312852859497\n",
      "Iteration: 3154, Loss: 1.50752592086792\n",
      "Iteration: 3155, Loss: 1.5075204372406006\n",
      "Iteration: 3156, Loss: 1.5075150728225708\n",
      "Iteration: 3157, Loss: 1.507509708404541\n",
      "Iteration: 3158, Loss: 1.5075042247772217\n",
      "Iteration: 3159, Loss: 1.507498860359192\n",
      "Iteration: 3160, Loss: 1.507493495941162\n",
      "Iteration: 3161, Loss: 1.5074881315231323\n",
      "Iteration: 3162, Loss: 1.507482647895813\n",
      "Iteration: 3163, Loss: 1.5074772834777832\n",
      "Iteration: 3164, Loss: 1.5074719190597534\n",
      "Iteration: 3165, Loss: 1.5074665546417236\n",
      "Iteration: 3166, Loss: 1.5074611902236938\n",
      "Iteration: 3167, Loss: 1.507455825805664\n",
      "Iteration: 3168, Loss: 1.5074503421783447\n",
      "Iteration: 3169, Loss: 1.507444977760315\n",
      "Iteration: 3170, Loss: 1.5074396133422852\n",
      "Iteration: 3171, Loss: 1.5074342489242554\n",
      "Iteration: 3172, Loss: 1.5074288845062256\n",
      "Iteration: 3173, Loss: 1.5074235200881958\n",
      "Iteration: 3174, Loss: 1.507418155670166\n",
      "Iteration: 3175, Loss: 1.5074127912521362\n",
      "Iteration: 3176, Loss: 1.5074074268341064\n",
      "Iteration: 3177, Loss: 1.5074020624160767\n",
      "Iteration: 3178, Loss: 1.5073966979980469\n",
      "Iteration: 3179, Loss: 1.507391333580017\n",
      "Iteration: 3180, Loss: 1.5073859691619873\n",
      "Iteration: 3181, Loss: 1.5073806047439575\n",
      "Iteration: 3182, Loss: 1.5073752403259277\n",
      "Iteration: 3183, Loss: 1.507369875907898\n",
      "Iteration: 3184, Loss: 1.5073645114898682\n",
      "Iteration: 3185, Loss: 1.5073591470718384\n",
      "Iteration: 3186, Loss: 1.5073537826538086\n",
      "Iteration: 3187, Loss: 1.5073484182357788\n",
      "Iteration: 3188, Loss: 1.507343053817749\n",
      "Iteration: 3189, Loss: 1.5073376893997192\n",
      "Iteration: 3190, Loss: 1.5073323249816895\n",
      "Iteration: 3191, Loss: 1.5073269605636597\n",
      "Iteration: 3192, Loss: 1.5073215961456299\n",
      "Iteration: 3193, Loss: 1.5073163509368896\n",
      "Iteration: 3194, Loss: 1.5073109865188599\n",
      "Iteration: 3195, Loss: 1.50730562210083\n",
      "Iteration: 3196, Loss: 1.5073002576828003\n",
      "Iteration: 3197, Loss: 1.5072948932647705\n",
      "Iteration: 3198, Loss: 1.5072895288467407\n",
      "Iteration: 3199, Loss: 1.5072842836380005\n",
      "Iteration: 3200, Loss: 1.5072789192199707\n",
      "Iteration: 3201, Loss: 1.507273554801941\n",
      "Iteration: 3202, Loss: 1.5072681903839111\n",
      "Iteration: 3203, Loss: 1.507262945175171\n",
      "Iteration: 3204, Loss: 1.5072575807571411\n",
      "Iteration: 3205, Loss: 1.5072522163391113\n",
      "Iteration: 3206, Loss: 1.507246971130371\n",
      "Iteration: 3207, Loss: 1.5072416067123413\n",
      "Iteration: 3208, Loss: 1.5072362422943115\n",
      "Iteration: 3209, Loss: 1.5072308778762817\n",
      "Iteration: 3210, Loss: 1.5072256326675415\n",
      "Iteration: 3211, Loss: 1.5072202682495117\n",
      "Iteration: 3212, Loss: 1.5072150230407715\n",
      "Iteration: 3213, Loss: 1.5072096586227417\n",
      "Iteration: 3214, Loss: 1.507204294204712\n",
      "Iteration: 3215, Loss: 1.5071990489959717\n",
      "Iteration: 3216, Loss: 1.507193684577942\n",
      "Iteration: 3217, Loss: 1.5071884393692017\n",
      "Iteration: 3218, Loss: 1.5071830749511719\n",
      "Iteration: 3219, Loss: 1.507177710533142\n",
      "Iteration: 3220, Loss: 1.5071724653244019\n",
      "Iteration: 3221, Loss: 1.507167100906372\n",
      "Iteration: 3222, Loss: 1.5071618556976318\n",
      "Iteration: 3223, Loss: 1.507156491279602\n",
      "Iteration: 3224, Loss: 1.5071512460708618\n",
      "Iteration: 3225, Loss: 1.507145881652832\n",
      "Iteration: 3226, Loss: 1.5071406364440918\n",
      "Iteration: 3227, Loss: 1.507135272026062\n",
      "Iteration: 3228, Loss: 1.5071300268173218\n",
      "Iteration: 3229, Loss: 1.5071247816085815\n",
      "Iteration: 3230, Loss: 1.5071194171905518\n",
      "Iteration: 3231, Loss: 1.5071141719818115\n",
      "Iteration: 3232, Loss: 1.5071088075637817\n",
      "Iteration: 3233, Loss: 1.5071035623550415\n",
      "Iteration: 3234, Loss: 1.5070983171463013\n",
      "Iteration: 3235, Loss: 1.5070929527282715\n",
      "Iteration: 3236, Loss: 1.5070877075195312\n",
      "Iteration: 3237, Loss: 1.507082462310791\n",
      "Iteration: 3238, Loss: 1.5070770978927612\n",
      "Iteration: 3239, Loss: 1.507071852684021\n",
      "Iteration: 3240, Loss: 1.5070666074752808\n",
      "Iteration: 3241, Loss: 1.507061243057251\n",
      "Iteration: 3242, Loss: 1.5070559978485107\n",
      "Iteration: 3243, Loss: 1.5070507526397705\n",
      "Iteration: 3244, Loss: 1.5070453882217407\n",
      "Iteration: 3245, Loss: 1.5070401430130005\n",
      "Iteration: 3246, Loss: 1.5070348978042603\n",
      "Iteration: 3247, Loss: 1.50702965259552\n",
      "Iteration: 3248, Loss: 1.5070242881774902\n",
      "Iteration: 3249, Loss: 1.50701904296875\n",
      "Iteration: 3250, Loss: 1.5070137977600098\n",
      "Iteration: 3251, Loss: 1.5070085525512695\n",
      "Iteration: 3252, Loss: 1.5070033073425293\n",
      "Iteration: 3253, Loss: 1.506998062133789\n",
      "Iteration: 3254, Loss: 1.5069926977157593\n",
      "Iteration: 3255, Loss: 1.506987452507019\n",
      "Iteration: 3256, Loss: 1.5069822072982788\n",
      "Iteration: 3257, Loss: 1.5069769620895386\n",
      "Iteration: 3258, Loss: 1.5069717168807983\n",
      "Iteration: 3259, Loss: 1.506966471672058\n",
      "Iteration: 3260, Loss: 1.5069612264633179\n",
      "Iteration: 3261, Loss: 1.5069559812545776\n",
      "Iteration: 3262, Loss: 1.5069507360458374\n",
      "Iteration: 3263, Loss: 1.5069454908370972\n",
      "Iteration: 3264, Loss: 1.506940245628357\n",
      "Iteration: 3265, Loss: 1.5069348812103271\n",
      "Iteration: 3266, Loss: 1.506929636001587\n",
      "Iteration: 3267, Loss: 1.5069243907928467\n",
      "Iteration: 3268, Loss: 1.5069191455841064\n",
      "Iteration: 3269, Loss: 1.5069139003753662\n",
      "Iteration: 3270, Loss: 1.5069087743759155\n",
      "Iteration: 3271, Loss: 1.5069035291671753\n",
      "Iteration: 3272, Loss: 1.506898283958435\n",
      "Iteration: 3273, Loss: 1.5068930387496948\n",
      "Iteration: 3274, Loss: 1.5068877935409546\n",
      "Iteration: 3275, Loss: 1.5068825483322144\n",
      "Iteration: 3276, Loss: 1.5068773031234741\n",
      "Iteration: 3277, Loss: 1.5068720579147339\n",
      "Iteration: 3278, Loss: 1.5068668127059937\n",
      "Iteration: 3279, Loss: 1.5068615674972534\n",
      "Iteration: 3280, Loss: 1.5068563222885132\n",
      "Iteration: 3281, Loss: 1.5068511962890625\n",
      "Iteration: 3282, Loss: 1.5068459510803223\n",
      "Iteration: 3283, Loss: 1.506840705871582\n",
      "Iteration: 3284, Loss: 1.5068354606628418\n",
      "Iteration: 3285, Loss: 1.5068302154541016\n",
      "Iteration: 3286, Loss: 1.5068249702453613\n",
      "Iteration: 3287, Loss: 1.5068198442459106\n",
      "Iteration: 3288, Loss: 1.5068145990371704\n",
      "Iteration: 3289, Loss: 1.5068093538284302\n",
      "Iteration: 3290, Loss: 1.50680410861969\n",
      "Iteration: 3291, Loss: 1.5067989826202393\n",
      "Iteration: 3292, Loss: 1.506793737411499\n",
      "Iteration: 3293, Loss: 1.5067884922027588\n",
      "Iteration: 3294, Loss: 1.506783366203308\n",
      "Iteration: 3295, Loss: 1.5067781209945679\n",
      "Iteration: 3296, Loss: 1.5067728757858276\n",
      "Iteration: 3297, Loss: 1.5067676305770874\n",
      "Iteration: 3298, Loss: 1.5067625045776367\n",
      "Iteration: 3299, Loss: 1.5067572593688965\n",
      "Iteration: 3300, Loss: 1.5067521333694458\n",
      "Iteration: 3301, Loss: 1.5067468881607056\n",
      "Iteration: 3302, Loss: 1.5067416429519653\n",
      "Iteration: 3303, Loss: 1.5067365169525146\n",
      "Iteration: 3304, Loss: 1.5067312717437744\n",
      "Iteration: 3305, Loss: 1.5067260265350342\n",
      "Iteration: 3306, Loss: 1.5067209005355835\n",
      "Iteration: 3307, Loss: 1.5067156553268433\n",
      "Iteration: 3308, Loss: 1.5067105293273926\n",
      "Iteration: 3309, Loss: 1.5067052841186523\n",
      "Iteration: 3310, Loss: 1.5067001581192017\n",
      "Iteration: 3311, Loss: 1.5066949129104614\n",
      "Iteration: 3312, Loss: 1.5066897869110107\n",
      "Iteration: 3313, Loss: 1.5066845417022705\n",
      "Iteration: 3314, Loss: 1.5066794157028198\n",
      "Iteration: 3315, Loss: 1.5066741704940796\n",
      "Iteration: 3316, Loss: 1.506669044494629\n",
      "Iteration: 3317, Loss: 1.5066637992858887\n",
      "Iteration: 3318, Loss: 1.506658673286438\n",
      "Iteration: 3319, Loss: 1.5066534280776978\n",
      "Iteration: 3320, Loss: 1.506648302078247\n",
      "Iteration: 3321, Loss: 1.5066431760787964\n",
      "Iteration: 3322, Loss: 1.5066379308700562\n",
      "Iteration: 3323, Loss: 1.5066328048706055\n",
      "Iteration: 3324, Loss: 1.5066275596618652\n",
      "Iteration: 3325, Loss: 1.5066224336624146\n",
      "Iteration: 3326, Loss: 1.5066173076629639\n",
      "Iteration: 3327, Loss: 1.5066120624542236\n",
      "Iteration: 3328, Loss: 1.506606936454773\n",
      "Iteration: 3329, Loss: 1.5066018104553223\n",
      "Iteration: 3330, Loss: 1.506596565246582\n",
      "Iteration: 3331, Loss: 1.5065914392471313\n",
      "Iteration: 3332, Loss: 1.5065863132476807\n",
      "Iteration: 3333, Loss: 1.50658118724823\n",
      "Iteration: 3334, Loss: 1.5065759420394897\n",
      "Iteration: 3335, Loss: 1.506570816040039\n",
      "Iteration: 3336, Loss: 1.5065656900405884\n",
      "Iteration: 3337, Loss: 1.5065604448318481\n",
      "Iteration: 3338, Loss: 1.5065553188323975\n",
      "Iteration: 3339, Loss: 1.5065501928329468\n",
      "Iteration: 3340, Loss: 1.506545066833496\n",
      "Iteration: 3341, Loss: 1.5065399408340454\n",
      "Iteration: 3342, Loss: 1.5065346956253052\n",
      "Iteration: 3343, Loss: 1.5065295696258545\n",
      "Iteration: 3344, Loss: 1.5065244436264038\n",
      "Iteration: 3345, Loss: 1.5065193176269531\n",
      "Iteration: 3346, Loss: 1.5065141916275024\n",
      "Iteration: 3347, Loss: 1.5065090656280518\n",
      "Iteration: 3348, Loss: 1.506503939628601\n",
      "Iteration: 3349, Loss: 1.5064986944198608\n",
      "Iteration: 3350, Loss: 1.5064935684204102\n",
      "Iteration: 3351, Loss: 1.5064884424209595\n",
      "Iteration: 3352, Loss: 1.5064833164215088\n",
      "Iteration: 3353, Loss: 1.506478190422058\n",
      "Iteration: 3354, Loss: 1.5064730644226074\n",
      "Iteration: 3355, Loss: 1.5064679384231567\n",
      "Iteration: 3356, Loss: 1.506462812423706\n",
      "Iteration: 3357, Loss: 1.5064576864242554\n",
      "Iteration: 3358, Loss: 1.5064525604248047\n",
      "Iteration: 3359, Loss: 1.506447434425354\n",
      "Iteration: 3360, Loss: 1.5064423084259033\n",
      "Iteration: 3361, Loss: 1.5064371824264526\n",
      "Iteration: 3362, Loss: 1.506432056427002\n",
      "Iteration: 3363, Loss: 1.5064269304275513\n",
      "Iteration: 3364, Loss: 1.5064218044281006\n",
      "Iteration: 3365, Loss: 1.50641667842865\n",
      "Iteration: 3366, Loss: 1.5064115524291992\n",
      "Iteration: 3367, Loss: 1.5064064264297485\n",
      "Iteration: 3368, Loss: 1.5064013004302979\n",
      "Iteration: 3369, Loss: 1.5063961744308472\n",
      "Iteration: 3370, Loss: 1.506391167640686\n",
      "Iteration: 3371, Loss: 1.5063860416412354\n",
      "Iteration: 3372, Loss: 1.5063809156417847\n",
      "Iteration: 3373, Loss: 1.506375789642334\n",
      "Iteration: 3374, Loss: 1.5063706636428833\n",
      "Iteration: 3375, Loss: 1.5063655376434326\n",
      "Iteration: 3376, Loss: 1.506360411643982\n",
      "Iteration: 3377, Loss: 1.5063554048538208\n",
      "Iteration: 3378, Loss: 1.5063502788543701\n",
      "Iteration: 3379, Loss: 1.5063451528549194\n",
      "Iteration: 3380, Loss: 1.5063400268554688\n",
      "Iteration: 3381, Loss: 1.506334900856018\n",
      "Iteration: 3382, Loss: 1.506329894065857\n",
      "Iteration: 3383, Loss: 1.5063247680664062\n",
      "Iteration: 3384, Loss: 1.5063196420669556\n",
      "Iteration: 3385, Loss: 1.5063145160675049\n",
      "Iteration: 3386, Loss: 1.5063095092773438\n",
      "Iteration: 3387, Loss: 1.506304383277893\n",
      "Iteration: 3388, Loss: 1.5062992572784424\n",
      "Iteration: 3389, Loss: 1.5062942504882812\n",
      "Iteration: 3390, Loss: 1.5062891244888306\n",
      "Iteration: 3391, Loss: 1.5062839984893799\n",
      "Iteration: 3392, Loss: 1.5062789916992188\n",
      "Iteration: 3393, Loss: 1.506273865699768\n",
      "Iteration: 3394, Loss: 1.5062687397003174\n",
      "Iteration: 3395, Loss: 1.5062637329101562\n",
      "Iteration: 3396, Loss: 1.5062586069107056\n",
      "Iteration: 3397, Loss: 1.5062534809112549\n",
      "Iteration: 3398, Loss: 1.5062484741210938\n",
      "Iteration: 3399, Loss: 1.506243348121643\n",
      "Iteration: 3400, Loss: 1.506238341331482\n",
      "Iteration: 3401, Loss: 1.5062332153320312\n",
      "Iteration: 3402, Loss: 1.5062282085418701\n",
      "Iteration: 3403, Loss: 1.5062230825424194\n",
      "Iteration: 3404, Loss: 1.5062179565429688\n",
      "Iteration: 3405, Loss: 1.5062129497528076\n",
      "Iteration: 3406, Loss: 1.506207823753357\n",
      "Iteration: 3407, Loss: 1.5062028169631958\n",
      "Iteration: 3408, Loss: 1.5061976909637451\n",
      "Iteration: 3409, Loss: 1.506192684173584\n",
      "Iteration: 3410, Loss: 1.5061875581741333\n",
      "Iteration: 3411, Loss: 1.5061825513839722\n",
      "Iteration: 3412, Loss: 1.5061774253845215\n",
      "Iteration: 3413, Loss: 1.5061724185943604\n",
      "Iteration: 3414, Loss: 1.5061674118041992\n",
      "Iteration: 3415, Loss: 1.5061622858047485\n",
      "Iteration: 3416, Loss: 1.5061572790145874\n",
      "Iteration: 3417, Loss: 1.5061521530151367\n",
      "Iteration: 3418, Loss: 1.5061471462249756\n",
      "Iteration: 3419, Loss: 1.5061421394348145\n",
      "Iteration: 3420, Loss: 1.5061370134353638\n",
      "Iteration: 3421, Loss: 1.5061320066452026\n",
      "Iteration: 3422, Loss: 1.506126880645752\n",
      "Iteration: 3423, Loss: 1.5061218738555908\n",
      "Iteration: 3424, Loss: 1.5061168670654297\n",
      "Iteration: 3425, Loss: 1.506111741065979\n",
      "Iteration: 3426, Loss: 1.5061067342758179\n",
      "Iteration: 3427, Loss: 1.5061017274856567\n",
      "Iteration: 3428, Loss: 1.5060967206954956\n",
      "Iteration: 3429, Loss: 1.506091594696045\n",
      "Iteration: 3430, Loss: 1.5060865879058838\n",
      "Iteration: 3431, Loss: 1.5060815811157227\n",
      "Iteration: 3432, Loss: 1.5060765743255615\n",
      "Iteration: 3433, Loss: 1.5060714483261108\n",
      "Iteration: 3434, Loss: 1.5060664415359497\n",
      "Iteration: 3435, Loss: 1.5060614347457886\n",
      "Iteration: 3436, Loss: 1.5060564279556274\n",
      "Iteration: 3437, Loss: 1.5060513019561768\n",
      "Iteration: 3438, Loss: 1.5060462951660156\n",
      "Iteration: 3439, Loss: 1.5060412883758545\n",
      "Iteration: 3440, Loss: 1.5060362815856934\n",
      "Iteration: 3441, Loss: 1.5060312747955322\n",
      "Iteration: 3442, Loss: 1.506026268005371\n",
      "Iteration: 3443, Loss: 1.5060211420059204\n",
      "Iteration: 3444, Loss: 1.5060161352157593\n",
      "Iteration: 3445, Loss: 1.5060111284255981\n",
      "Iteration: 3446, Loss: 1.506006121635437\n",
      "Iteration: 3447, Loss: 1.5060011148452759\n",
      "Iteration: 3448, Loss: 1.5059961080551147\n",
      "Iteration: 3449, Loss: 1.5059911012649536\n",
      "Iteration: 3450, Loss: 1.5059860944747925\n",
      "Iteration: 3451, Loss: 1.5059810876846313\n",
      "Iteration: 3452, Loss: 1.5059760808944702\n",
      "Iteration: 3453, Loss: 1.5059709548950195\n",
      "Iteration: 3454, Loss: 1.5059659481048584\n",
      "Iteration: 3455, Loss: 1.5059609413146973\n",
      "Iteration: 3456, Loss: 1.5059559345245361\n",
      "Iteration: 3457, Loss: 1.505950927734375\n",
      "Iteration: 3458, Loss: 1.5059459209442139\n",
      "Iteration: 3459, Loss: 1.5059409141540527\n",
      "Iteration: 3460, Loss: 1.5059359073638916\n",
      "Iteration: 3461, Loss: 1.50593101978302\n",
      "Iteration: 3462, Loss: 1.5059260129928589\n",
      "Iteration: 3463, Loss: 1.5059210062026978\n",
      "Iteration: 3464, Loss: 1.5059159994125366\n",
      "Iteration: 3465, Loss: 1.5059109926223755\n",
      "Iteration: 3466, Loss: 1.5059059858322144\n",
      "Iteration: 3467, Loss: 1.5059009790420532\n",
      "Iteration: 3468, Loss: 1.505895972251892\n",
      "Iteration: 3469, Loss: 1.505890965461731\n",
      "Iteration: 3470, Loss: 1.5058859586715698\n",
      "Iteration: 3471, Loss: 1.5058809518814087\n",
      "Iteration: 3472, Loss: 1.505876064300537\n",
      "Iteration: 3473, Loss: 1.505871057510376\n",
      "Iteration: 3474, Loss: 1.5058660507202148\n",
      "Iteration: 3475, Loss: 1.5058610439300537\n",
      "Iteration: 3476, Loss: 1.5058560371398926\n",
      "Iteration: 3477, Loss: 1.5058510303497314\n",
      "Iteration: 3478, Loss: 1.5058461427688599\n",
      "Iteration: 3479, Loss: 1.5058411359786987\n",
      "Iteration: 3480, Loss: 1.5058361291885376\n",
      "Iteration: 3481, Loss: 1.5058311223983765\n",
      "Iteration: 3482, Loss: 1.5058262348175049\n",
      "Iteration: 3483, Loss: 1.5058212280273438\n",
      "Iteration: 3484, Loss: 1.5058162212371826\n",
      "Iteration: 3485, Loss: 1.5058112144470215\n",
      "Iteration: 3486, Loss: 1.50580632686615\n",
      "Iteration: 3487, Loss: 1.5058013200759888\n",
      "Iteration: 3488, Loss: 1.5057963132858276\n",
      "Iteration: 3489, Loss: 1.505791425704956\n",
      "Iteration: 3490, Loss: 1.505786418914795\n",
      "Iteration: 3491, Loss: 1.5057814121246338\n",
      "Iteration: 3492, Loss: 1.5057765245437622\n",
      "Iteration: 3493, Loss: 1.505771517753601\n",
      "Iteration: 3494, Loss: 1.50576651096344\n",
      "Iteration: 3495, Loss: 1.5057616233825684\n",
      "Iteration: 3496, Loss: 1.5057566165924072\n",
      "Iteration: 3497, Loss: 1.505751609802246\n",
      "Iteration: 3498, Loss: 1.5057467222213745\n",
      "Iteration: 3499, Loss: 1.5057417154312134\n",
      "Iteration: 3500, Loss: 1.5057368278503418\n",
      "Iteration: 3501, Loss: 1.5057318210601807\n",
      "Iteration: 3502, Loss: 1.5057268142700195\n",
      "Iteration: 3503, Loss: 1.505721926689148\n",
      "Iteration: 3504, Loss: 1.5057169198989868\n",
      "Iteration: 3505, Loss: 1.5057120323181152\n",
      "Iteration: 3506, Loss: 1.505707025527954\n",
      "Iteration: 3507, Loss: 1.5057021379470825\n",
      "Iteration: 3508, Loss: 1.5056971311569214\n",
      "Iteration: 3509, Loss: 1.5056922435760498\n",
      "Iteration: 3510, Loss: 1.5056872367858887\n",
      "Iteration: 3511, Loss: 1.505682349205017\n",
      "Iteration: 3512, Loss: 1.505677342414856\n",
      "Iteration: 3513, Loss: 1.5056724548339844\n",
      "Iteration: 3514, Loss: 1.5056674480438232\n",
      "Iteration: 3515, Loss: 1.5056625604629517\n",
      "Iteration: 3516, Loss: 1.50565767288208\n",
      "Iteration: 3517, Loss: 1.505652666091919\n",
      "Iteration: 3518, Loss: 1.5056477785110474\n",
      "Iteration: 3519, Loss: 1.5056427717208862\n",
      "Iteration: 3520, Loss: 1.5056378841400146\n",
      "Iteration: 3521, Loss: 1.505632996559143\n",
      "Iteration: 3522, Loss: 1.505627989768982\n",
      "Iteration: 3523, Loss: 1.5056231021881104\n",
      "Iteration: 3524, Loss: 1.5056180953979492\n",
      "Iteration: 3525, Loss: 1.5056132078170776\n",
      "Iteration: 3526, Loss: 1.505608320236206\n",
      "Iteration: 3527, Loss: 1.505603313446045\n",
      "Iteration: 3528, Loss: 1.5055984258651733\n",
      "Iteration: 3529, Loss: 1.5055935382843018\n",
      "Iteration: 3530, Loss: 1.5055886507034302\n",
      "Iteration: 3531, Loss: 1.505583643913269\n",
      "Iteration: 3532, Loss: 1.5055787563323975\n",
      "Iteration: 3533, Loss: 1.5055738687515259\n",
      "Iteration: 3534, Loss: 1.5055689811706543\n",
      "Iteration: 3535, Loss: 1.5055639743804932\n",
      "Iteration: 3536, Loss: 1.5055590867996216\n",
      "Iteration: 3537, Loss: 1.50555419921875\n",
      "Iteration: 3538, Loss: 1.5055493116378784\n",
      "Iteration: 3539, Loss: 1.5055443048477173\n",
      "Iteration: 3540, Loss: 1.5055394172668457\n",
      "Iteration: 3541, Loss: 1.5055345296859741\n",
      "Iteration: 3542, Loss: 1.5055296421051025\n",
      "Iteration: 3543, Loss: 1.505524754524231\n",
      "Iteration: 3544, Loss: 1.5055197477340698\n",
      "Iteration: 3545, Loss: 1.5055148601531982\n",
      "Iteration: 3546, Loss: 1.5055099725723267\n",
      "Iteration: 3547, Loss: 1.505505084991455\n",
      "Iteration: 3548, Loss: 1.5055001974105835\n",
      "Iteration: 3549, Loss: 1.505495309829712\n",
      "Iteration: 3550, Loss: 1.5054904222488403\n",
      "Iteration: 3551, Loss: 1.5054855346679688\n",
      "Iteration: 3552, Loss: 1.5054805278778076\n",
      "Iteration: 3553, Loss: 1.505475640296936\n",
      "Iteration: 3554, Loss: 1.5054707527160645\n",
      "Iteration: 3555, Loss: 1.5054658651351929\n",
      "Iteration: 3556, Loss: 1.5054609775543213\n",
      "Iteration: 3557, Loss: 1.5054560899734497\n",
      "Iteration: 3558, Loss: 1.5054512023925781\n",
      "Iteration: 3559, Loss: 1.5054463148117065\n",
      "Iteration: 3560, Loss: 1.505441427230835\n",
      "Iteration: 3561, Loss: 1.5054365396499634\n",
      "Iteration: 3562, Loss: 1.5054316520690918\n",
      "Iteration: 3563, Loss: 1.5054267644882202\n",
      "Iteration: 3564, Loss: 1.5054218769073486\n",
      "Iteration: 3565, Loss: 1.505416989326477\n",
      "Iteration: 3566, Loss: 1.5054121017456055\n",
      "Iteration: 3567, Loss: 1.5054072141647339\n",
      "Iteration: 3568, Loss: 1.5054023265838623\n",
      "Iteration: 3569, Loss: 1.5053974390029907\n",
      "Iteration: 3570, Loss: 1.5053926706314087\n",
      "Iteration: 3571, Loss: 1.505387783050537\n",
      "Iteration: 3572, Loss: 1.5053828954696655\n",
      "Iteration: 3573, Loss: 1.505378007888794\n",
      "Iteration: 3574, Loss: 1.5053731203079224\n",
      "Iteration: 3575, Loss: 1.5053682327270508\n",
      "Iteration: 3576, Loss: 1.5053633451461792\n",
      "Iteration: 3577, Loss: 1.5053584575653076\n",
      "Iteration: 3578, Loss: 1.5053536891937256\n",
      "Iteration: 3579, Loss: 1.505348801612854\n",
      "Iteration: 3580, Loss: 1.5053439140319824\n",
      "Iteration: 3581, Loss: 1.5053390264511108\n",
      "Iteration: 3582, Loss: 1.5053341388702393\n",
      "Iteration: 3583, Loss: 1.5053292512893677\n",
      "Iteration: 3584, Loss: 1.5053244829177856\n",
      "Iteration: 3585, Loss: 1.505319595336914\n",
      "Iteration: 3586, Loss: 1.5053147077560425\n",
      "Iteration: 3587, Loss: 1.505309820175171\n",
      "Iteration: 3588, Loss: 1.5053050518035889\n",
      "Iteration: 3589, Loss: 1.5053001642227173\n",
      "Iteration: 3590, Loss: 1.5052952766418457\n",
      "Iteration: 3591, Loss: 1.5052903890609741\n",
      "Iteration: 3592, Loss: 1.505285620689392\n",
      "Iteration: 3593, Loss: 1.5052807331085205\n",
      "Iteration: 3594, Loss: 1.505275845527649\n",
      "Iteration: 3595, Loss: 1.505271077156067\n",
      "Iteration: 3596, Loss: 1.5052661895751953\n",
      "Iteration: 3597, Loss: 1.5052613019943237\n",
      "Iteration: 3598, Loss: 1.5052565336227417\n",
      "Iteration: 3599, Loss: 1.5052516460418701\n",
      "Iteration: 3600, Loss: 1.5052467584609985\n",
      "Iteration: 3601, Loss: 1.5052419900894165\n",
      "Iteration: 3602, Loss: 1.505237102508545\n",
      "Iteration: 3603, Loss: 1.5052322149276733\n",
      "Iteration: 3604, Loss: 1.5052274465560913\n",
      "Iteration: 3605, Loss: 1.5052225589752197\n",
      "Iteration: 3606, Loss: 1.5052177906036377\n",
      "Iteration: 3607, Loss: 1.5052129030227661\n",
      "Iteration: 3608, Loss: 1.5052080154418945\n",
      "Iteration: 3609, Loss: 1.5052032470703125\n",
      "Iteration: 3610, Loss: 1.505198359489441\n",
      "Iteration: 3611, Loss: 1.5051935911178589\n",
      "Iteration: 3612, Loss: 1.5051887035369873\n",
      "Iteration: 3613, Loss: 1.5051839351654053\n",
      "Iteration: 3614, Loss: 1.5051790475845337\n",
      "Iteration: 3615, Loss: 1.5051742792129517\n",
      "Iteration: 3616, Loss: 1.50516939163208\n",
      "Iteration: 3617, Loss: 1.505164623260498\n",
      "Iteration: 3618, Loss: 1.5051597356796265\n",
      "Iteration: 3619, Loss: 1.5051549673080444\n",
      "Iteration: 3620, Loss: 1.5051500797271729\n",
      "Iteration: 3621, Loss: 1.5051453113555908\n",
      "Iteration: 3622, Loss: 1.5051404237747192\n",
      "Iteration: 3623, Loss: 1.5051356554031372\n",
      "Iteration: 3624, Loss: 1.5051308870315552\n",
      "Iteration: 3625, Loss: 1.5051259994506836\n",
      "Iteration: 3626, Loss: 1.5051212310791016\n",
      "Iteration: 3627, Loss: 1.50511634349823\n",
      "Iteration: 3628, Loss: 1.505111575126648\n",
      "Iteration: 3629, Loss: 1.505106806755066\n",
      "Iteration: 3630, Loss: 1.5051019191741943\n",
      "Iteration: 3631, Loss: 1.5050971508026123\n",
      "Iteration: 3632, Loss: 1.5050922632217407\n",
      "Iteration: 3633, Loss: 1.5050874948501587\n",
      "Iteration: 3634, Loss: 1.5050827264785767\n",
      "Iteration: 3635, Loss: 1.505077838897705\n",
      "Iteration: 3636, Loss: 1.505073070526123\n",
      "Iteration: 3637, Loss: 1.505068302154541\n",
      "Iteration: 3638, Loss: 1.505063533782959\n",
      "Iteration: 3639, Loss: 1.5050586462020874\n",
      "Iteration: 3640, Loss: 1.5050538778305054\n",
      "Iteration: 3641, Loss: 1.5050491094589233\n",
      "Iteration: 3642, Loss: 1.5050442218780518\n",
      "Iteration: 3643, Loss: 1.5050394535064697\n",
      "Iteration: 3644, Loss: 1.5050346851348877\n",
      "Iteration: 3645, Loss: 1.5050299167633057\n",
      "Iteration: 3646, Loss: 1.505025029182434\n",
      "Iteration: 3647, Loss: 1.505020260810852\n",
      "Iteration: 3648, Loss: 1.50501549243927\n",
      "Iteration: 3649, Loss: 1.505010724067688\n",
      "Iteration: 3650, Loss: 1.505005955696106\n",
      "Iteration: 3651, Loss: 1.5050010681152344\n",
      "Iteration: 3652, Loss: 1.5049962997436523\n",
      "Iteration: 3653, Loss: 1.5049915313720703\n",
      "Iteration: 3654, Loss: 1.5049867630004883\n",
      "Iteration: 3655, Loss: 1.5049819946289062\n",
      "Iteration: 3656, Loss: 1.5049772262573242\n",
      "Iteration: 3657, Loss: 1.5049724578857422\n",
      "Iteration: 3658, Loss: 1.5049675703048706\n",
      "Iteration: 3659, Loss: 1.5049628019332886\n",
      "Iteration: 3660, Loss: 1.5049580335617065\n",
      "Iteration: 3661, Loss: 1.5049532651901245\n",
      "Iteration: 3662, Loss: 1.5049484968185425\n",
      "Iteration: 3663, Loss: 1.5049437284469604\n",
      "Iteration: 3664, Loss: 1.5049389600753784\n",
      "Iteration: 3665, Loss: 1.5049341917037964\n",
      "Iteration: 3666, Loss: 1.5049294233322144\n",
      "Iteration: 3667, Loss: 1.5049246549606323\n",
      "Iteration: 3668, Loss: 1.5049198865890503\n",
      "Iteration: 3669, Loss: 1.5049151182174683\n",
      "Iteration: 3670, Loss: 1.5049103498458862\n",
      "Iteration: 3671, Loss: 1.5049055814743042\n",
      "Iteration: 3672, Loss: 1.5049008131027222\n",
      "Iteration: 3673, Loss: 1.5048960447311401\n",
      "Iteration: 3674, Loss: 1.504891276359558\n",
      "Iteration: 3675, Loss: 1.504886507987976\n",
      "Iteration: 3676, Loss: 1.504881739616394\n",
      "Iteration: 3677, Loss: 1.504876971244812\n",
      "Iteration: 3678, Loss: 1.50487220287323\n",
      "Iteration: 3679, Loss: 1.504867434501648\n",
      "Iteration: 3680, Loss: 1.504862666130066\n",
      "Iteration: 3681, Loss: 1.5048578977584839\n",
      "Iteration: 3682, Loss: 1.5048531293869019\n",
      "Iteration: 3683, Loss: 1.5048483610153198\n",
      "Iteration: 3684, Loss: 1.5048435926437378\n",
      "Iteration: 3685, Loss: 1.5048388242721558\n",
      "Iteration: 3686, Loss: 1.5048341751098633\n",
      "Iteration: 3687, Loss: 1.5048294067382812\n",
      "Iteration: 3688, Loss: 1.5048246383666992\n",
      "Iteration: 3689, Loss: 1.5048198699951172\n",
      "Iteration: 3690, Loss: 1.5048151016235352\n",
      "Iteration: 3691, Loss: 1.5048103332519531\n",
      "Iteration: 3692, Loss: 1.504805564880371\n",
      "Iteration: 3693, Loss: 1.5048009157180786\n",
      "Iteration: 3694, Loss: 1.5047961473464966\n",
      "Iteration: 3695, Loss: 1.5047913789749146\n",
      "Iteration: 3696, Loss: 1.5047866106033325\n",
      "Iteration: 3697, Loss: 1.5047818422317505\n",
      "Iteration: 3698, Loss: 1.504777193069458\n",
      "Iteration: 3699, Loss: 1.504772424697876\n",
      "Iteration: 3700, Loss: 1.504767656326294\n",
      "Iteration: 3701, Loss: 1.504762887954712\n",
      "Iteration: 3702, Loss: 1.5047582387924194\n",
      "Iteration: 3703, Loss: 1.5047534704208374\n",
      "Iteration: 3704, Loss: 1.5047487020492554\n",
      "Iteration: 3705, Loss: 1.504744052886963\n",
      "Iteration: 3706, Loss: 1.5047392845153809\n",
      "Iteration: 3707, Loss: 1.5047345161437988\n",
      "Iteration: 3708, Loss: 1.5047297477722168\n",
      "Iteration: 3709, Loss: 1.5047250986099243\n",
      "Iteration: 3710, Loss: 1.5047203302383423\n",
      "Iteration: 3711, Loss: 1.5047155618667603\n",
      "Iteration: 3712, Loss: 1.5047109127044678\n",
      "Iteration: 3713, Loss: 1.5047061443328857\n",
      "Iteration: 3714, Loss: 1.5047014951705933\n",
      "Iteration: 3715, Loss: 1.5046967267990112\n",
      "Iteration: 3716, Loss: 1.5046919584274292\n",
      "Iteration: 3717, Loss: 1.5046873092651367\n",
      "Iteration: 3718, Loss: 1.5046825408935547\n",
      "Iteration: 3719, Loss: 1.5046777725219727\n",
      "Iteration: 3720, Loss: 1.5046731233596802\n",
      "Iteration: 3721, Loss: 1.5046683549880981\n",
      "Iteration: 3722, Loss: 1.5046637058258057\n",
      "Iteration: 3723, Loss: 1.5046589374542236\n",
      "Iteration: 3724, Loss: 1.5046542882919312\n",
      "Iteration: 3725, Loss: 1.5046495199203491\n",
      "Iteration: 3726, Loss: 1.5046448707580566\n",
      "Iteration: 3727, Loss: 1.5046401023864746\n",
      "Iteration: 3728, Loss: 1.5046353340148926\n",
      "Iteration: 3729, Loss: 1.5046306848526\n",
      "Iteration: 3730, Loss: 1.5046260356903076\n",
      "Iteration: 3731, Loss: 1.5046212673187256\n",
      "Iteration: 3732, Loss: 1.504616618156433\n",
      "Iteration: 3733, Loss: 1.504611849784851\n",
      "Iteration: 3734, Loss: 1.5046072006225586\n",
      "Iteration: 3735, Loss: 1.5046024322509766\n",
      "Iteration: 3736, Loss: 1.504597783088684\n",
      "Iteration: 3737, Loss: 1.504593014717102\n",
      "Iteration: 3738, Loss: 1.5045883655548096\n",
      "Iteration: 3739, Loss: 1.5045835971832275\n",
      "Iteration: 3740, Loss: 1.504578948020935\n",
      "Iteration: 3741, Loss: 1.5045742988586426\n",
      "Iteration: 3742, Loss: 1.5045695304870605\n",
      "Iteration: 3743, Loss: 1.504564881324768\n",
      "Iteration: 3744, Loss: 1.504560112953186\n",
      "Iteration: 3745, Loss: 1.5045554637908936\n",
      "Iteration: 3746, Loss: 1.504550814628601\n",
      "Iteration: 3747, Loss: 1.504546046257019\n",
      "Iteration: 3748, Loss: 1.5045413970947266\n",
      "Iteration: 3749, Loss: 1.504536747932434\n",
      "Iteration: 3750, Loss: 1.504531979560852\n",
      "Iteration: 3751, Loss: 1.5045273303985596\n",
      "Iteration: 3752, Loss: 1.504522681236267\n",
      "Iteration: 3753, Loss: 1.504517912864685\n",
      "Iteration: 3754, Loss: 1.5045132637023926\n",
      "Iteration: 3755, Loss: 1.5045086145401\n",
      "Iteration: 3756, Loss: 1.504503846168518\n",
      "Iteration: 3757, Loss: 1.5044991970062256\n",
      "Iteration: 3758, Loss: 1.504494547843933\n",
      "Iteration: 3759, Loss: 1.5044898986816406\n",
      "Iteration: 3760, Loss: 1.5044851303100586\n",
      "Iteration: 3761, Loss: 1.5044804811477661\n",
      "Iteration: 3762, Loss: 1.5044758319854736\n",
      "Iteration: 3763, Loss: 1.5044711828231812\n",
      "Iteration: 3764, Loss: 1.5044665336608887\n",
      "Iteration: 3765, Loss: 1.5044617652893066\n",
      "Iteration: 3766, Loss: 1.5044571161270142\n",
      "Iteration: 3767, Loss: 1.5044524669647217\n",
      "Iteration: 3768, Loss: 1.5044478178024292\n",
      "Iteration: 3769, Loss: 1.5044431686401367\n",
      "Iteration: 3770, Loss: 1.5044384002685547\n",
      "Iteration: 3771, Loss: 1.5044337511062622\n",
      "Iteration: 3772, Loss: 1.5044291019439697\n",
      "Iteration: 3773, Loss: 1.5044244527816772\n",
      "Iteration: 3774, Loss: 1.5044198036193848\n",
      "Iteration: 3775, Loss: 1.5044151544570923\n",
      "Iteration: 3776, Loss: 1.5044105052947998\n",
      "Iteration: 3777, Loss: 1.5044058561325073\n",
      "Iteration: 3778, Loss: 1.5044010877609253\n",
      "Iteration: 3779, Loss: 1.5043964385986328\n",
      "Iteration: 3780, Loss: 1.5043917894363403\n",
      "Iteration: 3781, Loss: 1.5043871402740479\n",
      "Iteration: 3782, Loss: 1.5043824911117554\n",
      "Iteration: 3783, Loss: 1.504377841949463\n",
      "Iteration: 3784, Loss: 1.5043731927871704\n",
      "Iteration: 3785, Loss: 1.504368543624878\n",
      "Iteration: 3786, Loss: 1.5043638944625854\n",
      "Iteration: 3787, Loss: 1.504359245300293\n",
      "Iteration: 3788, Loss: 1.5043545961380005\n",
      "Iteration: 3789, Loss: 1.504349946975708\n",
      "Iteration: 3790, Loss: 1.5043452978134155\n",
      "Iteration: 3791, Loss: 1.504340648651123\n",
      "Iteration: 3792, Loss: 1.5043359994888306\n",
      "Iteration: 3793, Loss: 1.504331350326538\n",
      "Iteration: 3794, Loss: 1.5043267011642456\n",
      "Iteration: 3795, Loss: 1.5043220520019531\n",
      "Iteration: 3796, Loss: 1.5043174028396606\n",
      "Iteration: 3797, Loss: 1.5043127536773682\n",
      "Iteration: 3798, Loss: 1.5043081045150757\n",
      "Iteration: 3799, Loss: 1.5043034553527832\n",
      "Iteration: 3800, Loss: 1.5042988061904907\n",
      "Iteration: 3801, Loss: 1.5042941570281982\n",
      "Iteration: 3802, Loss: 1.5042896270751953\n",
      "Iteration: 3803, Loss: 1.5042849779129028\n",
      "Iteration: 3804, Loss: 1.5042803287506104\n",
      "Iteration: 3805, Loss: 1.5042756795883179\n",
      "Iteration: 3806, Loss: 1.5042710304260254\n",
      "Iteration: 3807, Loss: 1.504266381263733\n",
      "Iteration: 3808, Loss: 1.5042617321014404\n",
      "Iteration: 3809, Loss: 1.5042572021484375\n",
      "Iteration: 3810, Loss: 1.504252552986145\n",
      "Iteration: 3811, Loss: 1.5042479038238525\n",
      "Iteration: 3812, Loss: 1.50424325466156\n",
      "Iteration: 3813, Loss: 1.5042386054992676\n",
      "Iteration: 3814, Loss: 1.504233956336975\n",
      "Iteration: 3815, Loss: 1.5042294263839722\n",
      "Iteration: 3816, Loss: 1.5042247772216797\n",
      "Iteration: 3817, Loss: 1.5042201280593872\n",
      "Iteration: 3818, Loss: 1.5042154788970947\n",
      "Iteration: 3819, Loss: 1.5042109489440918\n",
      "Iteration: 3820, Loss: 1.5042062997817993\n",
      "Iteration: 3821, Loss: 1.5042016506195068\n",
      "Iteration: 3822, Loss: 1.5041970014572144\n",
      "Iteration: 3823, Loss: 1.5041924715042114\n",
      "Iteration: 3824, Loss: 1.504187822341919\n",
      "Iteration: 3825, Loss: 1.5041831731796265\n",
      "Iteration: 3826, Loss: 1.504178524017334\n",
      "Iteration: 3827, Loss: 1.504173994064331\n",
      "Iteration: 3828, Loss: 1.5041693449020386\n",
      "Iteration: 3829, Loss: 1.504164695739746\n",
      "Iteration: 3830, Loss: 1.5041601657867432\n",
      "Iteration: 3831, Loss: 1.5041555166244507\n",
      "Iteration: 3832, Loss: 1.5041508674621582\n",
      "Iteration: 3833, Loss: 1.5041463375091553\n",
      "Iteration: 3834, Loss: 1.5041416883468628\n",
      "Iteration: 3835, Loss: 1.5041370391845703\n",
      "Iteration: 3836, Loss: 1.5041325092315674\n",
      "Iteration: 3837, Loss: 1.504127860069275\n",
      "Iteration: 3838, Loss: 1.504123330116272\n",
      "Iteration: 3839, Loss: 1.5041186809539795\n",
      "Iteration: 3840, Loss: 1.504114031791687\n",
      "Iteration: 3841, Loss: 1.504109501838684\n",
      "Iteration: 3842, Loss: 1.5041048526763916\n",
      "Iteration: 3843, Loss: 1.5041003227233887\n",
      "Iteration: 3844, Loss: 1.5040956735610962\n",
      "Iteration: 3845, Loss: 1.5040911436080933\n",
      "Iteration: 3846, Loss: 1.5040864944458008\n",
      "Iteration: 3847, Loss: 1.5040819644927979\n",
      "Iteration: 3848, Loss: 1.5040773153305054\n",
      "Iteration: 3849, Loss: 1.504072666168213\n",
      "Iteration: 3850, Loss: 1.50406813621521\n",
      "Iteration: 3851, Loss: 1.5040634870529175\n",
      "Iteration: 3852, Loss: 1.5040589570999146\n",
      "Iteration: 3853, Loss: 1.504054307937622\n",
      "Iteration: 3854, Loss: 1.5040497779846191\n",
      "Iteration: 3855, Loss: 1.5040452480316162\n",
      "Iteration: 3856, Loss: 1.5040405988693237\n",
      "Iteration: 3857, Loss: 1.5040360689163208\n",
      "Iteration: 3858, Loss: 1.5040314197540283\n",
      "Iteration: 3859, Loss: 1.5040268898010254\n",
      "Iteration: 3860, Loss: 1.504022240638733\n",
      "Iteration: 3861, Loss: 1.50401771068573\n",
      "Iteration: 3862, Loss: 1.5040130615234375\n",
      "Iteration: 3863, Loss: 1.5040085315704346\n",
      "Iteration: 3864, Loss: 1.5040040016174316\n",
      "Iteration: 3865, Loss: 1.5039993524551392\n",
      "Iteration: 3866, Loss: 1.5039948225021362\n",
      "Iteration: 3867, Loss: 1.5039902925491333\n",
      "Iteration: 3868, Loss: 1.5039856433868408\n",
      "Iteration: 3869, Loss: 1.503981113433838\n",
      "Iteration: 3870, Loss: 1.5039764642715454\n",
      "Iteration: 3871, Loss: 1.5039719343185425\n",
      "Iteration: 3872, Loss: 1.5039674043655396\n",
      "Iteration: 3873, Loss: 1.503962755203247\n",
      "Iteration: 3874, Loss: 1.5039582252502441\n",
      "Iteration: 3875, Loss: 1.5039536952972412\n",
      "Iteration: 3876, Loss: 1.5039490461349487\n",
      "Iteration: 3877, Loss: 1.5039445161819458\n",
      "Iteration: 3878, Loss: 1.5039399862289429\n",
      "Iteration: 3879, Loss: 1.50393545627594\n",
      "Iteration: 3880, Loss: 1.5039308071136475\n",
      "Iteration: 3881, Loss: 1.5039262771606445\n",
      "Iteration: 3882, Loss: 1.5039217472076416\n",
      "Iteration: 3883, Loss: 1.5039172172546387\n",
      "Iteration: 3884, Loss: 1.5039125680923462\n",
      "Iteration: 3885, Loss: 1.5039080381393433\n",
      "Iteration: 3886, Loss: 1.5039035081863403\n",
      "Iteration: 3887, Loss: 1.5038989782333374\n",
      "Iteration: 3888, Loss: 1.503894329071045\n",
      "Iteration: 3889, Loss: 1.503889799118042\n",
      "Iteration: 3890, Loss: 1.503885269165039\n",
      "Iteration: 3891, Loss: 1.5038807392120361\n",
      "Iteration: 3892, Loss: 1.5038762092590332\n",
      "Iteration: 3893, Loss: 1.5038716793060303\n",
      "Iteration: 3894, Loss: 1.5038670301437378\n",
      "Iteration: 3895, Loss: 1.5038625001907349\n",
      "Iteration: 3896, Loss: 1.503857970237732\n",
      "Iteration: 3897, Loss: 1.503853440284729\n",
      "Iteration: 3898, Loss: 1.503848910331726\n",
      "Iteration: 3899, Loss: 1.5038443803787231\n",
      "Iteration: 3900, Loss: 1.5038398504257202\n",
      "Iteration: 3901, Loss: 1.5038352012634277\n",
      "Iteration: 3902, Loss: 1.5038306713104248\n",
      "Iteration: 3903, Loss: 1.5038261413574219\n",
      "Iteration: 3904, Loss: 1.503821611404419\n",
      "Iteration: 3905, Loss: 1.503817081451416\n",
      "Iteration: 3906, Loss: 1.503812551498413\n",
      "Iteration: 3907, Loss: 1.5038080215454102\n",
      "Iteration: 3908, Loss: 1.5038034915924072\n",
      "Iteration: 3909, Loss: 1.5037989616394043\n",
      "Iteration: 3910, Loss: 1.5037944316864014\n",
      "Iteration: 3911, Loss: 1.5037899017333984\n",
      "Iteration: 3912, Loss: 1.5037853717803955\n",
      "Iteration: 3913, Loss: 1.5037808418273926\n",
      "Iteration: 3914, Loss: 1.5037763118743896\n",
      "Iteration: 3915, Loss: 1.5037717819213867\n",
      "Iteration: 3916, Loss: 1.5037672519683838\n",
      "Iteration: 3917, Loss: 1.5037627220153809\n",
      "Iteration: 3918, Loss: 1.503758192062378\n",
      "Iteration: 3919, Loss: 1.503753662109375\n",
      "Iteration: 3920, Loss: 1.503749132156372\n",
      "Iteration: 3921, Loss: 1.5037446022033691\n",
      "Iteration: 3922, Loss: 1.5037400722503662\n",
      "Iteration: 3923, Loss: 1.5037355422973633\n",
      "Iteration: 3924, Loss: 1.5037310123443604\n",
      "Iteration: 3925, Loss: 1.5037264823913574\n",
      "Iteration: 3926, Loss: 1.5037219524383545\n",
      "Iteration: 3927, Loss: 1.5037174224853516\n",
      "Iteration: 3928, Loss: 1.5037128925323486\n",
      "Iteration: 3929, Loss: 1.5037084817886353\n",
      "Iteration: 3930, Loss: 1.5037039518356323\n",
      "Iteration: 3931, Loss: 1.5036994218826294\n",
      "Iteration: 3932, Loss: 1.5036948919296265\n",
      "Iteration: 3933, Loss: 1.5036903619766235\n",
      "Iteration: 3934, Loss: 1.5036858320236206\n",
      "Iteration: 3935, Loss: 1.5036813020706177\n",
      "Iteration: 3936, Loss: 1.5036768913269043\n",
      "Iteration: 3937, Loss: 1.5036723613739014\n",
      "Iteration: 3938, Loss: 1.5036678314208984\n",
      "Iteration: 3939, Loss: 1.5036633014678955\n",
      "Iteration: 3940, Loss: 1.5036587715148926\n",
      "Iteration: 3941, Loss: 1.5036542415618896\n",
      "Iteration: 3942, Loss: 1.5036498308181763\n",
      "Iteration: 3943, Loss: 1.5036453008651733\n",
      "Iteration: 3944, Loss: 1.5036407709121704\n",
      "Iteration: 3945, Loss: 1.5036362409591675\n",
      "Iteration: 3946, Loss: 1.503631830215454\n",
      "Iteration: 3947, Loss: 1.5036273002624512\n",
      "Iteration: 3948, Loss: 1.5036227703094482\n",
      "Iteration: 3949, Loss: 1.5036182403564453\n",
      "Iteration: 3950, Loss: 1.503613829612732\n",
      "Iteration: 3951, Loss: 1.503609299659729\n",
      "Iteration: 3952, Loss: 1.503604769706726\n",
      "Iteration: 3953, Loss: 1.5036002397537231\n",
      "Iteration: 3954, Loss: 1.5035958290100098\n",
      "Iteration: 3955, Loss: 1.5035912990570068\n",
      "Iteration: 3956, Loss: 1.503586769104004\n",
      "Iteration: 3957, Loss: 1.5035823583602905\n",
      "Iteration: 3958, Loss: 1.5035778284072876\n",
      "Iteration: 3959, Loss: 1.5035732984542847\n",
      "Iteration: 3960, Loss: 1.5035688877105713\n",
      "Iteration: 3961, Loss: 1.5035643577575684\n",
      "Iteration: 3962, Loss: 1.5035598278045654\n",
      "Iteration: 3963, Loss: 1.503555417060852\n",
      "Iteration: 3964, Loss: 1.5035508871078491\n",
      "Iteration: 3965, Loss: 1.5035464763641357\n",
      "Iteration: 3966, Loss: 1.5035419464111328\n",
      "Iteration: 3967, Loss: 1.5035374164581299\n",
      "Iteration: 3968, Loss: 1.5035330057144165\n",
      "Iteration: 3969, Loss: 1.5035284757614136\n",
      "Iteration: 3970, Loss: 1.5035240650177002\n",
      "Iteration: 3971, Loss: 1.5035195350646973\n",
      "Iteration: 3972, Loss: 1.5035151243209839\n",
      "Iteration: 3973, Loss: 1.503510594367981\n",
      "Iteration: 3974, Loss: 1.503506064414978\n",
      "Iteration: 3975, Loss: 1.5035016536712646\n",
      "Iteration: 3976, Loss: 1.5034971237182617\n",
      "Iteration: 3977, Loss: 1.5034927129745483\n",
      "Iteration: 3978, Loss: 1.5034881830215454\n",
      "Iteration: 3979, Loss: 1.503483772277832\n",
      "Iteration: 3980, Loss: 1.503479242324829\n",
      "Iteration: 3981, Loss: 1.5034748315811157\n",
      "Iteration: 3982, Loss: 1.5034703016281128\n",
      "Iteration: 3983, Loss: 1.5034658908843994\n",
      "Iteration: 3984, Loss: 1.5034613609313965\n",
      "Iteration: 3985, Loss: 1.503456950187683\n",
      "Iteration: 3986, Loss: 1.5034524202346802\n",
      "Iteration: 3987, Loss: 1.5034480094909668\n",
      "Iteration: 3988, Loss: 1.5034435987472534\n",
      "Iteration: 3989, Loss: 1.5034390687942505\n",
      "Iteration: 3990, Loss: 1.503434658050537\n",
      "Iteration: 3991, Loss: 1.5034301280975342\n",
      "Iteration: 3992, Loss: 1.5034257173538208\n",
      "Iteration: 3993, Loss: 1.5034211874008179\n",
      "Iteration: 3994, Loss: 1.5034167766571045\n",
      "Iteration: 3995, Loss: 1.5034123659133911\n",
      "Iteration: 3996, Loss: 1.5034078359603882\n",
      "Iteration: 3997, Loss: 1.5034034252166748\n",
      "Iteration: 3998, Loss: 1.5033990144729614\n",
      "Iteration: 3999, Loss: 1.5033944845199585\n",
      "Iteration: 4000, Loss: 1.5033900737762451\n",
      "Iteration: 4001, Loss: 1.5033856630325317\n",
      "Iteration: 4002, Loss: 1.5033811330795288\n",
      "Iteration: 4003, Loss: 1.5033767223358154\n",
      "Iteration: 4004, Loss: 1.503372311592102\n",
      "Iteration: 4005, Loss: 1.5033677816390991\n",
      "Iteration: 4006, Loss: 1.5033633708953857\n",
      "Iteration: 4007, Loss: 1.5033589601516724\n",
      "Iteration: 4008, Loss: 1.5033544301986694\n",
      "Iteration: 4009, Loss: 1.503350019454956\n",
      "Iteration: 4010, Loss: 1.5033456087112427\n",
      "Iteration: 4011, Loss: 1.5033410787582397\n",
      "Iteration: 4012, Loss: 1.5033366680145264\n",
      "Iteration: 4013, Loss: 1.503332257270813\n",
      "Iteration: 4014, Loss: 1.5033278465270996\n",
      "Iteration: 4015, Loss: 1.5033233165740967\n",
      "Iteration: 4016, Loss: 1.5033189058303833\n",
      "Iteration: 4017, Loss: 1.50331449508667\n",
      "Iteration: 4018, Loss: 1.5033100843429565\n",
      "Iteration: 4019, Loss: 1.5033056735992432\n",
      "Iteration: 4020, Loss: 1.5033011436462402\n",
      "Iteration: 4021, Loss: 1.5032967329025269\n",
      "Iteration: 4022, Loss: 1.5032923221588135\n",
      "Iteration: 4023, Loss: 1.5032879114151\n",
      "Iteration: 4024, Loss: 1.5032835006713867\n",
      "Iteration: 4025, Loss: 1.5032790899276733\n",
      "Iteration: 4026, Loss: 1.5032745599746704\n",
      "Iteration: 4027, Loss: 1.503270149230957\n",
      "Iteration: 4028, Loss: 1.5032657384872437\n",
      "Iteration: 4029, Loss: 1.5032613277435303\n",
      "Iteration: 4030, Loss: 1.503256916999817\n",
      "Iteration: 4031, Loss: 1.5032525062561035\n",
      "Iteration: 4032, Loss: 1.5032480955123901\n",
      "Iteration: 4033, Loss: 1.5032435655593872\n",
      "Iteration: 4034, Loss: 1.5032391548156738\n",
      "Iteration: 4035, Loss: 1.5032347440719604\n",
      "Iteration: 4036, Loss: 1.503230333328247\n",
      "Iteration: 4037, Loss: 1.5032259225845337\n",
      "Iteration: 4038, Loss: 1.5032215118408203\n",
      "Iteration: 4039, Loss: 1.503217101097107\n",
      "Iteration: 4040, Loss: 1.5032126903533936\n",
      "Iteration: 4041, Loss: 1.5032082796096802\n",
      "Iteration: 4042, Loss: 1.5032038688659668\n",
      "Iteration: 4043, Loss: 1.5031994581222534\n",
      "Iteration: 4044, Loss: 1.50319504737854\n",
      "Iteration: 4045, Loss: 1.5031906366348267\n",
      "Iteration: 4046, Loss: 1.5031862258911133\n",
      "Iteration: 4047, Loss: 1.5031818151474\n",
      "Iteration: 4048, Loss: 1.5031774044036865\n",
      "Iteration: 4049, Loss: 1.5031729936599731\n",
      "Iteration: 4050, Loss: 1.5031685829162598\n",
      "Iteration: 4051, Loss: 1.5031641721725464\n",
      "Iteration: 4052, Loss: 1.503159761428833\n",
      "Iteration: 4053, Loss: 1.5031553506851196\n",
      "Iteration: 4054, Loss: 1.5031509399414062\n",
      "Iteration: 4055, Loss: 1.5031465291976929\n",
      "Iteration: 4056, Loss: 1.5031421184539795\n",
      "Iteration: 4057, Loss: 1.5031377077102661\n",
      "Iteration: 4058, Loss: 1.5031332969665527\n",
      "Iteration: 4059, Loss: 1.5031288862228394\n",
      "Iteration: 4060, Loss: 1.503124475479126\n",
      "Iteration: 4061, Loss: 1.5031201839447021\n",
      "Iteration: 4062, Loss: 1.5031157732009888\n",
      "Iteration: 4063, Loss: 1.5031113624572754\n",
      "Iteration: 4064, Loss: 1.503106951713562\n",
      "Iteration: 4065, Loss: 1.5031025409698486\n",
      "Iteration: 4066, Loss: 1.5030981302261353\n",
      "Iteration: 4067, Loss: 1.5030937194824219\n",
      "Iteration: 4068, Loss: 1.5030893087387085\n",
      "Iteration: 4069, Loss: 1.5030850172042847\n",
      "Iteration: 4070, Loss: 1.5030806064605713\n",
      "Iteration: 4071, Loss: 1.503076195716858\n",
      "Iteration: 4072, Loss: 1.5030717849731445\n",
      "Iteration: 4073, Loss: 1.5030673742294312\n",
      "Iteration: 4074, Loss: 1.5030630826950073\n",
      "Iteration: 4075, Loss: 1.503058671951294\n",
      "Iteration: 4076, Loss: 1.5030542612075806\n",
      "Iteration: 4077, Loss: 1.5030498504638672\n",
      "Iteration: 4078, Loss: 1.5030454397201538\n",
      "Iteration: 4079, Loss: 1.50304114818573\n",
      "Iteration: 4080, Loss: 1.5030367374420166\n",
      "Iteration: 4081, Loss: 1.5030323266983032\n",
      "Iteration: 4082, Loss: 1.5030279159545898\n",
      "Iteration: 4083, Loss: 1.503023624420166\n",
      "Iteration: 4084, Loss: 1.5030192136764526\n",
      "Iteration: 4085, Loss: 1.5030148029327393\n",
      "Iteration: 4086, Loss: 1.5030103921890259\n",
      "Iteration: 4087, Loss: 1.503006100654602\n",
      "Iteration: 4088, Loss: 1.5030016899108887\n",
      "Iteration: 4089, Loss: 1.5029972791671753\n",
      "Iteration: 4090, Loss: 1.5029929876327515\n",
      "Iteration: 4091, Loss: 1.502988576889038\n",
      "Iteration: 4092, Loss: 1.5029841661453247\n",
      "Iteration: 4093, Loss: 1.5029798746109009\n",
      "Iteration: 4094, Loss: 1.5029754638671875\n",
      "Iteration: 4095, Loss: 1.5029710531234741\n",
      "Iteration: 4096, Loss: 1.5029667615890503\n",
      "Iteration: 4097, Loss: 1.502962350845337\n",
      "Iteration: 4098, Loss: 1.5029579401016235\n",
      "Iteration: 4099, Loss: 1.5029536485671997\n",
      "Iteration: 4100, Loss: 1.5029492378234863\n",
      "Iteration: 4101, Loss: 1.5029449462890625\n",
      "Iteration: 4102, Loss: 1.5029405355453491\n",
      "Iteration: 4103, Loss: 1.5029361248016357\n",
      "Iteration: 4104, Loss: 1.502931833267212\n",
      "Iteration: 4105, Loss: 1.5029274225234985\n",
      "Iteration: 4106, Loss: 1.5029231309890747\n",
      "Iteration: 4107, Loss: 1.5029187202453613\n",
      "Iteration: 4108, Loss: 1.5029144287109375\n",
      "Iteration: 4109, Loss: 1.5029100179672241\n",
      "Iteration: 4110, Loss: 1.5029057264328003\n",
      "Iteration: 4111, Loss: 1.502901315689087\n",
      "Iteration: 4112, Loss: 1.5028969049453735\n",
      "Iteration: 4113, Loss: 1.5028926134109497\n",
      "Iteration: 4114, Loss: 1.5028882026672363\n",
      "Iteration: 4115, Loss: 1.5028839111328125\n",
      "Iteration: 4116, Loss: 1.5028795003890991\n",
      "Iteration: 4117, Loss: 1.5028752088546753\n",
      "Iteration: 4118, Loss: 1.502870798110962\n",
      "Iteration: 4119, Loss: 1.502866506576538\n",
      "Iteration: 4120, Loss: 1.5028622150421143\n",
      "Iteration: 4121, Loss: 1.5028578042984009\n",
      "Iteration: 4122, Loss: 1.502853512763977\n",
      "Iteration: 4123, Loss: 1.5028491020202637\n",
      "Iteration: 4124, Loss: 1.5028448104858398\n",
      "Iteration: 4125, Loss: 1.5028403997421265\n",
      "Iteration: 4126, Loss: 1.5028361082077026\n",
      "Iteration: 4127, Loss: 1.5028316974639893\n",
      "Iteration: 4128, Loss: 1.5028274059295654\n",
      "Iteration: 4129, Loss: 1.5028231143951416\n",
      "Iteration: 4130, Loss: 1.5028187036514282\n",
      "Iteration: 4131, Loss: 1.5028144121170044\n",
      "Iteration: 4132, Loss: 1.502810001373291\n",
      "Iteration: 4133, Loss: 1.5028057098388672\n",
      "Iteration: 4134, Loss: 1.5028014183044434\n",
      "Iteration: 4135, Loss: 1.50279700756073\n",
      "Iteration: 4136, Loss: 1.5027927160263062\n",
      "Iteration: 4137, Loss: 1.5027884244918823\n",
      "Iteration: 4138, Loss: 1.502784013748169\n",
      "Iteration: 4139, Loss: 1.5027797222137451\n",
      "Iteration: 4140, Loss: 1.5027754306793213\n",
      "Iteration: 4141, Loss: 1.502771019935608\n",
      "Iteration: 4142, Loss: 1.502766728401184\n",
      "Iteration: 4143, Loss: 1.5027624368667603\n",
      "Iteration: 4144, Loss: 1.5027580261230469\n",
      "Iteration: 4145, Loss: 1.502753734588623\n",
      "Iteration: 4146, Loss: 1.5027494430541992\n",
      "Iteration: 4147, Loss: 1.5027451515197754\n",
      "Iteration: 4148, Loss: 1.502740740776062\n",
      "Iteration: 4149, Loss: 1.5027364492416382\n",
      "Iteration: 4150, Loss: 1.5027321577072144\n",
      "Iteration: 4151, Loss: 1.5027278661727905\n",
      "Iteration: 4152, Loss: 1.5027234554290771\n",
      "Iteration: 4153, Loss: 1.5027191638946533\n",
      "Iteration: 4154, Loss: 1.5027148723602295\n",
      "Iteration: 4155, Loss: 1.5027105808258057\n",
      "Iteration: 4156, Loss: 1.5027061700820923\n",
      "Iteration: 4157, Loss: 1.5027018785476685\n",
      "Iteration: 4158, Loss: 1.5026975870132446\n",
      "Iteration: 4159, Loss: 1.5026932954788208\n",
      "Iteration: 4160, Loss: 1.502689003944397\n",
      "Iteration: 4161, Loss: 1.5026845932006836\n",
      "Iteration: 4162, Loss: 1.5026803016662598\n",
      "Iteration: 4163, Loss: 1.502676010131836\n",
      "Iteration: 4164, Loss: 1.502671718597412\n",
      "Iteration: 4165, Loss: 1.5026674270629883\n",
      "Iteration: 4166, Loss: 1.5026631355285645\n",
      "Iteration: 4167, Loss: 1.5026588439941406\n",
      "Iteration: 4168, Loss: 1.5026544332504272\n",
      "Iteration: 4169, Loss: 1.5026501417160034\n",
      "Iteration: 4170, Loss: 1.5026458501815796\n",
      "Iteration: 4171, Loss: 1.5026415586471558\n",
      "Iteration: 4172, Loss: 1.502637267112732\n",
      "Iteration: 4173, Loss: 1.502632975578308\n",
      "Iteration: 4174, Loss: 1.5026286840438843\n",
      "Iteration: 4175, Loss: 1.5026243925094604\n",
      "Iteration: 4176, Loss: 1.5026201009750366\n",
      "Iteration: 4177, Loss: 1.5026158094406128\n",
      "Iteration: 4178, Loss: 1.502611517906189\n",
      "Iteration: 4179, Loss: 1.5026072263717651\n",
      "Iteration: 4180, Loss: 1.5026029348373413\n",
      "Iteration: 4181, Loss: 1.5025986433029175\n",
      "Iteration: 4182, Loss: 1.502594232559204\n",
      "Iteration: 4183, Loss: 1.5025899410247803\n",
      "Iteration: 4184, Loss: 1.5025856494903564\n",
      "Iteration: 4185, Loss: 1.5025813579559326\n",
      "Iteration: 4186, Loss: 1.5025770664215088\n",
      "Iteration: 4187, Loss: 1.502572774887085\n",
      "Iteration: 4188, Loss: 1.5025684833526611\n",
      "Iteration: 4189, Loss: 1.5025643110275269\n",
      "Iteration: 4190, Loss: 1.502560019493103\n",
      "Iteration: 4191, Loss: 1.5025557279586792\n",
      "Iteration: 4192, Loss: 1.5025514364242554\n",
      "Iteration: 4193, Loss: 1.5025471448898315\n",
      "Iteration: 4194, Loss: 1.5025428533554077\n",
      "Iteration: 4195, Loss: 1.5025385618209839\n",
      "Iteration: 4196, Loss: 1.50253427028656\n",
      "Iteration: 4197, Loss: 1.5025299787521362\n",
      "Iteration: 4198, Loss: 1.5025256872177124\n",
      "Iteration: 4199, Loss: 1.5025213956832886\n",
      "Iteration: 4200, Loss: 1.5025171041488647\n",
      "Iteration: 4201, Loss: 1.502512812614441\n",
      "Iteration: 4202, Loss: 1.502508521080017\n",
      "Iteration: 4203, Loss: 1.5025043487548828\n",
      "Iteration: 4204, Loss: 1.502500057220459\n",
      "Iteration: 4205, Loss: 1.5024957656860352\n",
      "Iteration: 4206, Loss: 1.5024914741516113\n",
      "Iteration: 4207, Loss: 1.5024871826171875\n",
      "Iteration: 4208, Loss: 1.5024828910827637\n",
      "Iteration: 4209, Loss: 1.5024787187576294\n",
      "Iteration: 4210, Loss: 1.5024744272232056\n",
      "Iteration: 4211, Loss: 1.5024701356887817\n",
      "Iteration: 4212, Loss: 1.502465844154358\n",
      "Iteration: 4213, Loss: 1.502461552619934\n",
      "Iteration: 4214, Loss: 1.5024572610855103\n",
      "Iteration: 4215, Loss: 1.502453088760376\n",
      "Iteration: 4216, Loss: 1.5024487972259521\n",
      "Iteration: 4217, Loss: 1.5024445056915283\n",
      "Iteration: 4218, Loss: 1.5024402141571045\n",
      "Iteration: 4219, Loss: 1.5024360418319702\n",
      "Iteration: 4220, Loss: 1.5024317502975464\n",
      "Iteration: 4221, Loss: 1.5024274587631226\n",
      "Iteration: 4222, Loss: 1.5024231672286987\n",
      "Iteration: 4223, Loss: 1.5024189949035645\n",
      "Iteration: 4224, Loss: 1.5024147033691406\n",
      "Iteration: 4225, Loss: 1.5024104118347168\n",
      "Iteration: 4226, Loss: 1.502406120300293\n",
      "Iteration: 4227, Loss: 1.5024019479751587\n",
      "Iteration: 4228, Loss: 1.5023976564407349\n",
      "Iteration: 4229, Loss: 1.502393364906311\n",
      "Iteration: 4230, Loss: 1.5023891925811768\n",
      "Iteration: 4231, Loss: 1.502384901046753\n",
      "Iteration: 4232, Loss: 1.502380609512329\n",
      "Iteration: 4233, Loss: 1.5023764371871948\n",
      "Iteration: 4234, Loss: 1.502372145652771\n",
      "Iteration: 4235, Loss: 1.5023678541183472\n",
      "Iteration: 4236, Loss: 1.502363681793213\n",
      "Iteration: 4237, Loss: 1.502359390258789\n",
      "Iteration: 4238, Loss: 1.5023550987243652\n",
      "Iteration: 4239, Loss: 1.502350926399231\n",
      "Iteration: 4240, Loss: 1.5023466348648071\n",
      "Iteration: 4241, Loss: 1.5023424625396729\n",
      "Iteration: 4242, Loss: 1.502338171005249\n",
      "Iteration: 4243, Loss: 1.5023338794708252\n",
      "Iteration: 4244, Loss: 1.502329707145691\n",
      "Iteration: 4245, Loss: 1.502325415611267\n",
      "Iteration: 4246, Loss: 1.5023212432861328\n",
      "Iteration: 4247, Loss: 1.502316951751709\n",
      "Iteration: 4248, Loss: 1.5023126602172852\n",
      "Iteration: 4249, Loss: 1.5023084878921509\n",
      "Iteration: 4250, Loss: 1.502304196357727\n",
      "Iteration: 4251, Loss: 1.5023000240325928\n",
      "Iteration: 4252, Loss: 1.502295732498169\n",
      "Iteration: 4253, Loss: 1.5022915601730347\n",
      "Iteration: 4254, Loss: 1.5022872686386108\n",
      "Iteration: 4255, Loss: 1.5022830963134766\n",
      "Iteration: 4256, Loss: 1.5022788047790527\n",
      "Iteration: 4257, Loss: 1.5022746324539185\n",
      "Iteration: 4258, Loss: 1.5022703409194946\n",
      "Iteration: 4259, Loss: 1.5022661685943604\n",
      "Iteration: 4260, Loss: 1.5022618770599365\n",
      "Iteration: 4261, Loss: 1.5022577047348022\n",
      "Iteration: 4262, Loss: 1.5022534132003784\n",
      "Iteration: 4263, Loss: 1.5022492408752441\n",
      "Iteration: 4264, Loss: 1.5022449493408203\n",
      "Iteration: 4265, Loss: 1.502240777015686\n",
      "Iteration: 4266, Loss: 1.5022366046905518\n",
      "Iteration: 4267, Loss: 1.502232313156128\n",
      "Iteration: 4268, Loss: 1.5022281408309937\n",
      "Iteration: 4269, Loss: 1.5022238492965698\n",
      "Iteration: 4270, Loss: 1.5022196769714355\n",
      "Iteration: 4271, Loss: 1.5022153854370117\n",
      "Iteration: 4272, Loss: 1.5022112131118774\n",
      "Iteration: 4273, Loss: 1.5022070407867432\n",
      "Iteration: 4274, Loss: 1.5022027492523193\n",
      "Iteration: 4275, Loss: 1.502198576927185\n",
      "Iteration: 4276, Loss: 1.5021944046020508\n",
      "Iteration: 4277, Loss: 1.502190113067627\n",
      "Iteration: 4278, Loss: 1.5021859407424927\n",
      "Iteration: 4279, Loss: 1.5021816492080688\n",
      "Iteration: 4280, Loss: 1.5021774768829346\n",
      "Iteration: 4281, Loss: 1.5021733045578003\n",
      "Iteration: 4282, Loss: 1.5021690130233765\n",
      "Iteration: 4283, Loss: 1.5021648406982422\n",
      "Iteration: 4284, Loss: 1.502160668373108\n",
      "Iteration: 4285, Loss: 1.5021564960479736\n",
      "Iteration: 4286, Loss: 1.5021522045135498\n",
      "Iteration: 4287, Loss: 1.5021480321884155\n",
      "Iteration: 4288, Loss: 1.5021438598632812\n",
      "Iteration: 4289, Loss: 1.5021395683288574\n",
      "Iteration: 4290, Loss: 1.5021353960037231\n",
      "Iteration: 4291, Loss: 1.5021312236785889\n",
      "Iteration: 4292, Loss: 1.5021270513534546\n",
      "Iteration: 4293, Loss: 1.5021227598190308\n",
      "Iteration: 4294, Loss: 1.5021185874938965\n",
      "Iteration: 4295, Loss: 1.5021144151687622\n",
      "Iteration: 4296, Loss: 1.502110242843628\n",
      "Iteration: 4297, Loss: 1.502105951309204\n",
      "Iteration: 4298, Loss: 1.5021017789840698\n",
      "Iteration: 4299, Loss: 1.5020976066589355\n",
      "Iteration: 4300, Loss: 1.5020934343338013\n",
      "Iteration: 4301, Loss: 1.502089023590088\n",
      "Iteration: 4302, Loss: 1.5020848512649536\n",
      "Iteration: 4303, Loss: 1.5020806789398193\n",
      "Iteration: 4304, Loss: 1.502076506614685\n",
      "Iteration: 4305, Loss: 1.5020723342895508\n",
      "Iteration: 4306, Loss: 1.5020681619644165\n",
      "Iteration: 4307, Loss: 1.5020638704299927\n",
      "Iteration: 4308, Loss: 1.5020596981048584\n",
      "Iteration: 4309, Loss: 1.5020555257797241\n",
      "Iteration: 4310, Loss: 1.5020513534545898\n",
      "Iteration: 4311, Loss: 1.5020471811294556\n",
      "Iteration: 4312, Loss: 1.5020430088043213\n",
      "Iteration: 4313, Loss: 1.5020387172698975\n",
      "Iteration: 4314, Loss: 1.5020345449447632\n",
      "Iteration: 4315, Loss: 1.502030372619629\n",
      "Iteration: 4316, Loss: 1.5020262002944946\n",
      "Iteration: 4317, Loss: 1.5020220279693604\n",
      "Iteration: 4318, Loss: 1.502017855644226\n",
      "Iteration: 4319, Loss: 1.5020136833190918\n",
      "Iteration: 4320, Loss: 1.5020095109939575\n",
      "Iteration: 4321, Loss: 1.5020053386688232\n",
      "Iteration: 4322, Loss: 1.502001166343689\n",
      "Iteration: 4323, Loss: 1.5019969940185547\n",
      "Iteration: 4324, Loss: 1.5019927024841309\n",
      "Iteration: 4325, Loss: 1.5019885301589966\n",
      "Iteration: 4326, Loss: 1.5019843578338623\n",
      "Iteration: 4327, Loss: 1.501980185508728\n",
      "Iteration: 4328, Loss: 1.5019758939743042\n",
      "Iteration: 4329, Loss: 1.5019718408584595\n",
      "Iteration: 4330, Loss: 1.5019676685333252\n",
      "Iteration: 4331, Loss: 1.501963496208191\n",
      "Iteration: 4332, Loss: 1.501959204673767\n",
      "Iteration: 4333, Loss: 1.5019550323486328\n",
      "Iteration: 4334, Loss: 1.5019508600234985\n",
      "Iteration: 4335, Loss: 1.5019466876983643\n",
      "Iteration: 4336, Loss: 1.50194251537323\n",
      "Iteration: 4337, Loss: 1.5019383430480957\n",
      "Iteration: 4338, Loss: 1.5019341707229614\n",
      "Iteration: 4339, Loss: 1.5019299983978271\n",
      "Iteration: 4340, Loss: 1.5019258260726929\n",
      "Iteration: 4341, Loss: 1.5019216537475586\n",
      "Iteration: 4342, Loss: 1.5019174814224243\n",
      "Iteration: 4343, Loss: 1.50191330909729\n",
      "Iteration: 4344, Loss: 1.5019091367721558\n",
      "Iteration: 4345, Loss: 1.5019049644470215\n",
      "Iteration: 4346, Loss: 1.5019009113311768\n",
      "Iteration: 4347, Loss: 1.5018967390060425\n",
      "Iteration: 4348, Loss: 1.5018925666809082\n",
      "Iteration: 4349, Loss: 1.501888394355774\n",
      "Iteration: 4350, Loss: 1.5018842220306396\n",
      "Iteration: 4351, Loss: 1.5018798112869263\n",
      "Iteration: 4352, Loss: 1.501875638961792\n",
      "Iteration: 4353, Loss: 1.5018714666366577\n",
      "Iteration: 4354, Loss: 1.5018671751022339\n",
      "Iteration: 4355, Loss: 1.5018631219863892\n",
      "Iteration: 4356, Loss: 1.5018588304519653\n",
      "Iteration: 4357, Loss: 1.501854658126831\n",
      "Iteration: 4358, Loss: 1.5018506050109863\n",
      "Iteration: 4359, Loss: 1.501846432685852\n",
      "Iteration: 4360, Loss: 1.5018422603607178\n",
      "Iteration: 4361, Loss: 1.501837968826294\n",
      "Iteration: 4362, Loss: 1.5018337965011597\n",
      "Iteration: 4363, Loss: 1.5018296241760254\n",
      "Iteration: 4364, Loss: 1.5018254518508911\n",
      "Iteration: 4365, Loss: 1.5018212795257568\n",
      "Iteration: 4366, Loss: 1.501817226409912\n",
      "Iteration: 4367, Loss: 1.5018130540847778\n",
      "Iteration: 4368, Loss: 1.5018086433410645\n",
      "Iteration: 4369, Loss: 1.5018044710159302\n",
      "Iteration: 4370, Loss: 1.501800298690796\n",
      "Iteration: 4371, Loss: 1.501796007156372\n",
      "Iteration: 4372, Loss: 1.5017918348312378\n",
      "Iteration: 4373, Loss: 1.501787781715393\n",
      "Iteration: 4374, Loss: 1.5017834901809692\n",
      "Iteration: 4375, Loss: 1.501779317855835\n",
      "Iteration: 4376, Loss: 1.5017751455307007\n",
      "Iteration: 4377, Loss: 1.5017709732055664\n",
      "Iteration: 4378, Loss: 1.501766562461853\n",
      "Iteration: 4379, Loss: 1.5017622709274292\n",
      "Iteration: 4380, Loss: 1.5017582178115845\n",
      "Iteration: 4381, Loss: 1.5017539262771606\n",
      "Iteration: 4382, Loss: 1.5017497539520264\n",
      "Iteration: 4383, Loss: 1.501745581626892\n",
      "Iteration: 4384, Loss: 1.5017411708831787\n",
      "Iteration: 4385, Loss: 1.5017368793487549\n",
      "Iteration: 4386, Loss: 1.5017327070236206\n",
      "Iteration: 4387, Loss: 1.5017284154891968\n",
      "Iteration: 4388, Loss: 1.501724123954773\n",
      "Iteration: 4389, Loss: 1.5017198324203491\n",
      "Iteration: 4390, Loss: 1.5017155408859253\n",
      "Iteration: 4391, Loss: 1.501711130142212\n",
      "Iteration: 4392, Loss: 1.501706838607788\n",
      "Iteration: 4393, Loss: 1.5017049312591553\n",
      "Iteration: 4394, Loss: 1.501700758934021\n",
      "Iteration: 4395, Loss: 1.5016965866088867\n",
      "Iteration: 4396, Loss: 1.5016924142837524\n",
      "Iteration: 4397, Loss: 1.501688003540039\n",
      "Iteration: 4398, Loss: 1.5016839504241943\n",
      "Iteration: 4399, Loss: 1.50167977809906\n",
      "Iteration: 4400, Loss: 1.5016754865646362\n",
      "Iteration: 4401, Loss: 1.501671314239502\n",
      "Iteration: 4402, Loss: 1.5016672611236572\n",
      "Iteration: 4403, Loss: 1.5016629695892334\n",
      "Iteration: 4404, Loss: 1.5016587972640991\n",
      "Iteration: 4405, Loss: 1.5016546249389648\n",
      "Iteration: 4406, Loss: 1.5016505718231201\n",
      "Iteration: 4407, Loss: 1.5016461610794067\n",
      "Iteration: 4408, Loss: 1.501641869544983\n",
      "Iteration: 4409, Loss: 1.5016378164291382\n",
      "Iteration: 4410, Loss: 1.5016335248947144\n",
      "Iteration: 4411, Loss: 1.50162935256958\n",
      "Iteration: 4412, Loss: 1.5016252994537354\n",
      "Iteration: 4413, Loss: 1.501620888710022\n",
      "Iteration: 4414, Loss: 1.5016165971755981\n",
      "Iteration: 4415, Loss: 1.5016125440597534\n",
      "Iteration: 4416, Loss: 1.5016082525253296\n",
      "Iteration: 4417, Loss: 1.5016040802001953\n",
      "Iteration: 4418, Loss: 1.5015997886657715\n",
      "Iteration: 4419, Loss: 1.5015954971313477\n",
      "Iteration: 4420, Loss: 1.5015912055969238\n",
      "Iteration: 4421, Loss: 1.5015869140625\n",
      "Iteration: 4422, Loss: 1.5015826225280762\n",
      "Iteration: 4423, Loss: 1.5015783309936523\n",
      "Iteration: 4424, Loss: 1.501574158668518\n",
      "Iteration: 4425, Loss: 1.5015697479248047\n",
      "Iteration: 4426, Loss: 1.5015654563903809\n",
      "Iteration: 4427, Loss: 1.501560926437378\n",
      "Iteration: 4428, Loss: 1.501556634902954\n",
      "Iteration: 4429, Loss: 1.5015522241592407\n",
      "Iteration: 4430, Loss: 1.5015478134155273\n",
      "Iteration: 4431, Loss: 1.5015432834625244\n",
      "Iteration: 4432, Loss: 1.5015387535095215\n",
      "Iteration: 4433, Loss: 1.501534104347229\n",
      "Iteration: 4434, Loss: 1.5015294551849365\n",
      "Iteration: 4435, Loss: 1.5015249252319336\n",
      "Iteration: 4436, Loss: 1.5015202760696411\n",
      "Iteration: 4437, Loss: 1.501515507698059\n",
      "Iteration: 4438, Loss: 1.501510500907898\n",
      "Iteration: 4439, Loss: 1.5015054941177368\n",
      "Iteration: 4440, Loss: 1.5015002489089966\n",
      "Iteration: 4441, Loss: 1.5014950037002563\n",
      "Iteration: 4442, Loss: 1.5014894008636475\n",
      "Iteration: 4443, Loss: 1.501483678817749\n",
      "Iteration: 4444, Loss: 1.5014774799346924\n",
      "Iteration: 4445, Loss: 1.501470923423767\n",
      "Iteration: 4446, Loss: 1.501463770866394\n",
      "Iteration: 4447, Loss: 1.5014557838439941\n",
      "Iteration: 4448, Loss: 1.5014463663101196\n",
      "Iteration: 4449, Loss: 1.5014358758926392\n",
      "Iteration: 4450, Loss: 1.5014222860336304\n",
      "Iteration: 4451, Loss: 1.5014044046401978\n",
      "Iteration: 4452, Loss: 1.501379370689392\n",
      "Iteration: 4453, Loss: 1.5013400316238403\n",
      "Iteration: 4454, Loss: 1.501272439956665\n",
      "Iteration: 4455, Loss: 1.5011247396469116\n",
      "Iteration: 4456, Loss: 1.500690221786499\n",
      "Iteration: 4457, Loss: 1.4982105493545532\n",
      "Iteration: 4458, Loss: 1.3961693048477173\n",
      "Iteration: 4459, Loss: 1.5053356885910034\n",
      "Iteration: 4460, Loss: 1.501697301864624\n",
      "Iteration: 4461, Loss: 1.5019584894180298\n",
      "Iteration: 4462, Loss: 1.5022647380828857\n",
      "Iteration: 4463, Loss: 1.5025882720947266\n",
      "Iteration: 4464, Loss: 1.5029120445251465\n",
      "Iteration: 4465, Loss: 1.5032261610031128\n",
      "Iteration: 4466, Loss: 1.5035250186920166\n",
      "Iteration: 4467, Loss: 1.5038052797317505\n",
      "Iteration: 4468, Loss: 1.504065752029419\n",
      "Iteration: 4469, Loss: 1.5043061971664429\n",
      "Iteration: 4470, Loss: 1.504526972770691\n",
      "Iteration: 4471, Loss: 1.5047287940979004\n",
      "Iteration: 4472, Loss: 1.5049126148223877\n",
      "Iteration: 4473, Loss: 1.5050796270370483\n",
      "Iteration: 4474, Loss: 1.5052310228347778\n",
      "Iteration: 4475, Loss: 1.5053679943084717\n",
      "Iteration: 4476, Loss: 1.5054914951324463\n",
      "Iteration: 4477, Loss: 1.5056029558181763\n",
      "Iteration: 4478, Loss: 1.505703091621399\n",
      "Iteration: 4479, Loss: 1.5057930946350098\n",
      "Iteration: 4480, Loss: 1.5058737993240356\n",
      "Iteration: 4481, Loss: 1.5059460401535034\n",
      "Iteration: 4482, Loss: 1.50601065158844\n",
      "Iteration: 4483, Loss: 1.5060683488845825\n",
      "Iteration: 4484, Loss: 1.506119728088379\n",
      "Iteration: 4485, Loss: 1.5061653852462769\n",
      "Iteration: 4486, Loss: 1.5062057971954346\n",
      "Iteration: 4487, Loss: 1.5062416791915894\n",
      "Iteration: 4488, Loss: 1.5062732696533203\n",
      "Iteration: 4489, Loss: 1.5063010454177856\n",
      "Iteration: 4490, Loss: 1.5063254833221436\n",
      "Iteration: 4491, Loss: 1.5063467025756836\n",
      "Iteration: 4492, Loss: 1.506365180015564\n",
      "Iteration: 4493, Loss: 1.5063811540603638\n",
      "Iteration: 4494, Loss: 1.506394863128662\n",
      "Iteration: 4495, Loss: 1.5064064264297485\n",
      "Iteration: 4496, Loss: 1.5064162015914917\n",
      "Iteration: 4497, Loss: 1.5064243078231812\n",
      "Iteration: 4498, Loss: 1.506430983543396\n",
      "Iteration: 4499, Loss: 1.5064362287521362\n",
      "Iteration: 4500, Loss: 1.506440281867981\n",
      "Iteration: 4501, Loss: 1.5064432621002197\n",
      "Iteration: 4502, Loss: 1.5064451694488525\n",
      "Iteration: 4503, Loss: 1.5064462423324585\n",
      "Iteration: 4504, Loss: 1.5064464807510376\n",
      "Iteration: 4505, Loss: 1.506446123123169\n",
      "Iteration: 4506, Loss: 1.506445050239563\n",
      "Iteration: 4507, Loss: 1.5064433813095093\n",
      "Iteration: 4508, Loss: 1.5064411163330078\n",
      "Iteration: 4509, Loss: 1.5064384937286377\n",
      "Iteration: 4510, Loss: 1.5064353942871094\n",
      "Iteration: 4511, Loss: 1.5064319372177124\n",
      "Iteration: 4512, Loss: 1.5064281225204468\n",
      "Iteration: 4513, Loss: 1.5064239501953125\n",
      "Iteration: 4514, Loss: 1.5064195394515991\n",
      "Iteration: 4515, Loss: 1.5064148902893066\n",
      "Iteration: 4516, Loss: 1.5064101219177246\n",
      "Iteration: 4517, Loss: 1.506404995918274\n",
      "Iteration: 4518, Loss: 1.5063997507095337\n",
      "Iteration: 4519, Loss: 1.506394386291504\n",
      "Iteration: 4520, Loss: 1.506388783454895\n",
      "Iteration: 4521, Loss: 1.5063830614089966\n",
      "Iteration: 4522, Loss: 1.5063773393630981\n",
      "Iteration: 4523, Loss: 1.5063713788986206\n",
      "Iteration: 4524, Loss: 1.506365418434143\n",
      "Iteration: 4525, Loss: 1.506359338760376\n",
      "Iteration: 4526, Loss: 1.5063531398773193\n",
      "Iteration: 4527, Loss: 1.5063469409942627\n",
      "Iteration: 4528, Loss: 1.5063406229019165\n",
      "Iteration: 4529, Loss: 1.5063343048095703\n",
      "Iteration: 4530, Loss: 1.5063278675079346\n",
      "Iteration: 4531, Loss: 1.5063214302062988\n",
      "Iteration: 4532, Loss: 1.506314992904663\n",
      "Iteration: 4533, Loss: 1.5063085556030273\n",
      "Iteration: 4534, Loss: 1.506301999092102\n",
      "Iteration: 4535, Loss: 1.5062954425811768\n",
      "Iteration: 4536, Loss: 1.5062888860702515\n",
      "Iteration: 4537, Loss: 1.5062822103500366\n",
      "Iteration: 4538, Loss: 1.5062756538391113\n",
      "Iteration: 4539, Loss: 1.5062689781188965\n",
      "Iteration: 4540, Loss: 1.5062624216079712\n",
      "Iteration: 4541, Loss: 1.5062557458877563\n",
      "Iteration: 4542, Loss: 1.5062490701675415\n",
      "Iteration: 4543, Loss: 1.5062423944473267\n",
      "Iteration: 4544, Loss: 1.5062357187271118\n",
      "Iteration: 4545, Loss: 1.5062289237976074\n",
      "Iteration: 4546, Loss: 1.5062222480773926\n",
      "Iteration: 4547, Loss: 1.5062155723571777\n",
      "Iteration: 4548, Loss: 1.506208896636963\n",
      "Iteration: 4549, Loss: 1.506202220916748\n",
      "Iteration: 4550, Loss: 1.5061954259872437\n",
      "Iteration: 4551, Loss: 1.5061887502670288\n",
      "Iteration: 4552, Loss: 1.506182074546814\n",
      "Iteration: 4553, Loss: 1.5061752796173096\n",
      "Iteration: 4554, Loss: 1.5061686038970947\n",
      "Iteration: 4555, Loss: 1.5061619281768799\n",
      "Iteration: 4556, Loss: 1.5061551332473755\n",
      "Iteration: 4557, Loss: 1.5061484575271606\n",
      "Iteration: 4558, Loss: 1.5061417818069458\n",
      "Iteration: 4559, Loss: 1.5061349868774414\n",
      "Iteration: 4560, Loss: 1.5061283111572266\n",
      "Iteration: 4561, Loss: 1.5061216354370117\n",
      "Iteration: 4562, Loss: 1.5061149597167969\n",
      "Iteration: 4563, Loss: 1.5061081647872925\n",
      "Iteration: 4564, Loss: 1.5061014890670776\n",
      "Iteration: 4565, Loss: 1.5060948133468628\n",
      "Iteration: 4566, Loss: 1.506088137626648\n",
      "Iteration: 4567, Loss: 1.506081461906433\n",
      "Iteration: 4568, Loss: 1.5060747861862183\n",
      "Iteration: 4569, Loss: 1.5060681104660034\n",
      "Iteration: 4570, Loss: 1.5060614347457886\n",
      "Iteration: 4571, Loss: 1.5060547590255737\n",
      "Iteration: 4572, Loss: 1.5060480833053589\n",
      "Iteration: 4573, Loss: 1.506041407585144\n",
      "Iteration: 4574, Loss: 1.5060347318649292\n",
      "Iteration: 4575, Loss: 1.5060280561447144\n",
      "Iteration: 4576, Loss: 1.5060213804244995\n",
      "Iteration: 4577, Loss: 1.5060147047042847\n",
      "Iteration: 4578, Loss: 1.5060080289840698\n",
      "Iteration: 4579, Loss: 1.506001353263855\n",
      "Iteration: 4580, Loss: 1.5059946775436401\n",
      "Iteration: 4581, Loss: 1.5059881210327148\n",
      "Iteration: 4582, Loss: 1.5059814453125\n",
      "Iteration: 4583, Loss: 1.5059747695922852\n",
      "Iteration: 4584, Loss: 1.5059682130813599\n",
      "Iteration: 4585, Loss: 1.505961537361145\n",
      "Iteration: 4586, Loss: 1.5059548616409302\n",
      "Iteration: 4587, Loss: 1.5059483051300049\n",
      "Iteration: 4588, Loss: 1.50594162940979\n",
      "Iteration: 4589, Loss: 1.5059350728988647\n",
      "Iteration: 4590, Loss: 1.50592839717865\n",
      "Iteration: 4591, Loss: 1.5059218406677246\n",
      "Iteration: 4592, Loss: 1.5059151649475098\n",
      "Iteration: 4593, Loss: 1.5059086084365845\n",
      "Iteration: 4594, Loss: 1.5059020519256592\n",
      "Iteration: 4595, Loss: 1.5058953762054443\n",
      "Iteration: 4596, Loss: 1.505888819694519\n",
      "Iteration: 4597, Loss: 1.5058822631835938\n",
      "Iteration: 4598, Loss: 1.505875587463379\n",
      "Iteration: 4599, Loss: 1.5058690309524536\n",
      "Iteration: 4600, Loss: 1.5058624744415283\n",
      "Iteration: 4601, Loss: 1.505855917930603\n",
      "Iteration: 4602, Loss: 1.5058492422103882\n",
      "Iteration: 4603, Loss: 1.505842685699463\n",
      "Iteration: 4604, Loss: 1.5058361291885376\n",
      "Iteration: 4605, Loss: 1.5058295726776123\n",
      "Iteration: 4606, Loss: 1.505823016166687\n",
      "Iteration: 4607, Loss: 1.5058164596557617\n",
      "Iteration: 4608, Loss: 1.5058099031448364\n",
      "Iteration: 4609, Loss: 1.5058033466339111\n",
      "Iteration: 4610, Loss: 1.5057967901229858\n",
      "Iteration: 4611, Loss: 1.5057902336120605\n",
      "Iteration: 4612, Loss: 1.5057836771011353\n",
      "Iteration: 4613, Loss: 1.5057772397994995\n",
      "Iteration: 4614, Loss: 1.5057706832885742\n",
      "Iteration: 4615, Loss: 1.505764126777649\n",
      "Iteration: 4616, Loss: 1.5057575702667236\n",
      "Iteration: 4617, Loss: 1.5057510137557983\n",
      "Iteration: 4618, Loss: 1.5057445764541626\n",
      "Iteration: 4619, Loss: 1.5057380199432373\n",
      "Iteration: 4620, Loss: 1.505731463432312\n",
      "Iteration: 4621, Loss: 1.5057250261306763\n",
      "Iteration: 4622, Loss: 1.505718469619751\n",
      "Iteration: 4623, Loss: 1.5057119131088257\n",
      "Iteration: 4624, Loss: 1.50570547580719\n",
      "Iteration: 4625, Loss: 1.5056989192962646\n",
      "Iteration: 4626, Loss: 1.505692481994629\n",
      "Iteration: 4627, Loss: 1.5056859254837036\n",
      "Iteration: 4628, Loss: 1.5056794881820679\n",
      "Iteration: 4629, Loss: 1.5056729316711426\n",
      "Iteration: 4630, Loss: 1.5056664943695068\n",
      "Iteration: 4631, Loss: 1.505660057067871\n",
      "Iteration: 4632, Loss: 1.5056535005569458\n",
      "Iteration: 4633, Loss: 1.50564706325531\n",
      "Iteration: 4634, Loss: 1.5056406259536743\n",
      "Iteration: 4635, Loss: 1.505634069442749\n",
      "Iteration: 4636, Loss: 1.5056276321411133\n",
      "Iteration: 4637, Loss: 1.5056211948394775\n",
      "Iteration: 4638, Loss: 1.5056147575378418\n",
      "Iteration: 4639, Loss: 1.505608320236206\n",
      "Iteration: 4640, Loss: 1.5056017637252808\n",
      "Iteration: 4641, Loss: 1.505595326423645\n",
      "Iteration: 4642, Loss: 1.5055888891220093\n",
      "Iteration: 4643, Loss: 1.5055824518203735\n",
      "Iteration: 4644, Loss: 1.5055760145187378\n",
      "Iteration: 4645, Loss: 1.505569577217102\n",
      "Iteration: 4646, Loss: 1.5055631399154663\n",
      "Iteration: 4647, Loss: 1.5055567026138306\n",
      "Iteration: 4648, Loss: 1.5055502653121948\n",
      "Iteration: 4649, Loss: 1.505543828010559\n",
      "Iteration: 4650, Loss: 1.5055373907089233\n",
      "Iteration: 4651, Loss: 1.5055309534072876\n",
      "Iteration: 4652, Loss: 1.5055246353149414\n",
      "Iteration: 4653, Loss: 1.5055181980133057\n",
      "Iteration: 4654, Loss: 1.50551176071167\n",
      "Iteration: 4655, Loss: 1.5055053234100342\n",
      "Iteration: 4656, Loss: 1.5054988861083984\n",
      "Iteration: 4657, Loss: 1.5054925680160522\n",
      "Iteration: 4658, Loss: 1.5054861307144165\n",
      "Iteration: 4659, Loss: 1.5054796934127808\n",
      "Iteration: 4660, Loss: 1.5054733753204346\n",
      "Iteration: 4661, Loss: 1.5054669380187988\n",
      "Iteration: 4662, Loss: 1.505460500717163\n",
      "Iteration: 4663, Loss: 1.505454182624817\n",
      "Iteration: 4664, Loss: 1.5054477453231812\n",
      "Iteration: 4665, Loss: 1.505441427230835\n",
      "Iteration: 4666, Loss: 1.5054349899291992\n",
      "Iteration: 4667, Loss: 1.505428671836853\n",
      "Iteration: 4668, Loss: 1.5054222345352173\n",
      "Iteration: 4669, Loss: 1.505415916442871\n",
      "Iteration: 4670, Loss: 1.5054094791412354\n",
      "Iteration: 4671, Loss: 1.5054031610488892\n",
      "Iteration: 4672, Loss: 1.505396842956543\n",
      "Iteration: 4673, Loss: 1.5053904056549072\n",
      "Iteration: 4674, Loss: 1.505384087562561\n",
      "Iteration: 4675, Loss: 1.5053777694702148\n",
      "Iteration: 4676, Loss: 1.5053714513778687\n",
      "Iteration: 4677, Loss: 1.505365014076233\n",
      "Iteration: 4678, Loss: 1.5053586959838867\n",
      "Iteration: 4679, Loss: 1.5053523778915405\n",
      "Iteration: 4680, Loss: 1.5053460597991943\n",
      "Iteration: 4681, Loss: 1.5053397417068481\n",
      "Iteration: 4682, Loss: 1.5053333044052124\n",
      "Iteration: 4683, Loss: 1.5053269863128662\n",
      "Iteration: 4684, Loss: 1.50532066822052\n",
      "Iteration: 4685, Loss: 1.5053143501281738\n",
      "Iteration: 4686, Loss: 1.5053080320358276\n",
      "Iteration: 4687, Loss: 1.5053017139434814\n",
      "Iteration: 4688, Loss: 1.5052953958511353\n",
      "Iteration: 4689, Loss: 1.505289077758789\n",
      "Iteration: 4690, Loss: 1.5052827596664429\n",
      "Iteration: 4691, Loss: 1.5052764415740967\n",
      "Iteration: 4692, Loss: 1.5052701234817505\n",
      "Iteration: 4693, Loss: 1.5052639245986938\n",
      "Iteration: 4694, Loss: 1.5052576065063477\n",
      "Iteration: 4695, Loss: 1.5052512884140015\n",
      "Iteration: 4696, Loss: 1.5052449703216553\n",
      "Iteration: 4697, Loss: 1.505238652229309\n",
      "Iteration: 4698, Loss: 1.5052324533462524\n",
      "Iteration: 4699, Loss: 1.5052261352539062\n",
      "Iteration: 4700, Loss: 1.50521981716156\n",
      "Iteration: 4701, Loss: 1.5052134990692139\n",
      "Iteration: 4702, Loss: 1.5052073001861572\n",
      "Iteration: 4703, Loss: 1.505200982093811\n",
      "Iteration: 4704, Loss: 1.5051946640014648\n",
      "Iteration: 4705, Loss: 1.5051884651184082\n",
      "Iteration: 4706, Loss: 1.505182147026062\n",
      "Iteration: 4707, Loss: 1.5051759481430054\n",
      "Iteration: 4708, Loss: 1.5051696300506592\n",
      "Iteration: 4709, Loss: 1.5051634311676025\n",
      "Iteration: 4710, Loss: 1.5051571130752563\n",
      "Iteration: 4711, Loss: 1.5051509141921997\n",
      "Iteration: 4712, Loss: 1.5051445960998535\n",
      "Iteration: 4713, Loss: 1.5051383972167969\n",
      "Iteration: 4714, Loss: 1.5051321983337402\n",
      "Iteration: 4715, Loss: 1.505125880241394\n",
      "Iteration: 4716, Loss: 1.5051196813583374\n",
      "Iteration: 4717, Loss: 1.5051133632659912\n",
      "Iteration: 4718, Loss: 1.5051071643829346\n",
      "Iteration: 4719, Loss: 1.505100965499878\n",
      "Iteration: 4720, Loss: 1.5050947666168213\n",
      "Iteration: 4721, Loss: 1.505088448524475\n",
      "Iteration: 4722, Loss: 1.5050822496414185\n",
      "Iteration: 4723, Loss: 1.5050760507583618\n",
      "Iteration: 4724, Loss: 1.5050698518753052\n",
      "Iteration: 4725, Loss: 1.5050636529922485\n",
      "Iteration: 4726, Loss: 1.5050573348999023\n",
      "Iteration: 4727, Loss: 1.5050511360168457\n",
      "Iteration: 4728, Loss: 1.505044937133789\n",
      "Iteration: 4729, Loss: 1.5050387382507324\n",
      "Iteration: 4730, Loss: 1.5050325393676758\n",
      "Iteration: 4731, Loss: 1.5050263404846191\n",
      "Iteration: 4732, Loss: 1.5050201416015625\n",
      "Iteration: 4733, Loss: 1.5050139427185059\n",
      "Iteration: 4734, Loss: 1.5050077438354492\n",
      "Iteration: 4735, Loss: 1.5050015449523926\n",
      "Iteration: 4736, Loss: 1.504995346069336\n",
      "Iteration: 4737, Loss: 1.5049892663955688\n",
      "Iteration: 4738, Loss: 1.5049830675125122\n",
      "Iteration: 4739, Loss: 1.5049768686294556\n",
      "Iteration: 4740, Loss: 1.504970669746399\n",
      "Iteration: 4741, Loss: 1.5049644708633423\n",
      "Iteration: 4742, Loss: 1.5049582719802856\n",
      "Iteration: 4743, Loss: 1.5049521923065186\n",
      "Iteration: 4744, Loss: 1.504945993423462\n",
      "Iteration: 4745, Loss: 1.5049397945404053\n",
      "Iteration: 4746, Loss: 1.5049335956573486\n",
      "Iteration: 4747, Loss: 1.5049275159835815\n",
      "Iteration: 4748, Loss: 1.504921317100525\n",
      "Iteration: 4749, Loss: 1.5049152374267578\n",
      "Iteration: 4750, Loss: 1.5049090385437012\n",
      "Iteration: 4751, Loss: 1.5049028396606445\n",
      "Iteration: 4752, Loss: 1.5048967599868774\n",
      "Iteration: 4753, Loss: 1.5048905611038208\n",
      "Iteration: 4754, Loss: 1.5048844814300537\n",
      "Iteration: 4755, Loss: 1.504878282546997\n",
      "Iteration: 4756, Loss: 1.50487220287323\n",
      "Iteration: 4757, Loss: 1.5048660039901733\n",
      "Iteration: 4758, Loss: 1.5048599243164062\n",
      "Iteration: 4759, Loss: 1.5048537254333496\n",
      "Iteration: 4760, Loss: 1.5048476457595825\n",
      "Iteration: 4761, Loss: 1.5048415660858154\n",
      "Iteration: 4762, Loss: 1.5048353672027588\n",
      "Iteration: 4763, Loss: 1.5048292875289917\n",
      "Iteration: 4764, Loss: 1.5048232078552246\n",
      "Iteration: 4765, Loss: 1.504817008972168\n",
      "Iteration: 4766, Loss: 1.5048109292984009\n",
      "Iteration: 4767, Loss: 1.5048048496246338\n",
      "Iteration: 4768, Loss: 1.5047987699508667\n",
      "Iteration: 4769, Loss: 1.50479257106781\n",
      "Iteration: 4770, Loss: 1.504786491394043\n",
      "Iteration: 4771, Loss: 1.5047804117202759\n",
      "Iteration: 4772, Loss: 1.5047743320465088\n",
      "Iteration: 4773, Loss: 1.5047682523727417\n",
      "Iteration: 4774, Loss: 1.5047621726989746\n",
      "Iteration: 4775, Loss: 1.5047560930252075\n",
      "Iteration: 4776, Loss: 1.5047500133514404\n",
      "Iteration: 4777, Loss: 1.5047438144683838\n",
      "Iteration: 4778, Loss: 1.5047377347946167\n",
      "Iteration: 4779, Loss: 1.5047316551208496\n",
      "Iteration: 4780, Loss: 1.5047255754470825\n",
      "Iteration: 4781, Loss: 1.5047194957733154\n",
      "Iteration: 4782, Loss: 1.504713535308838\n",
      "Iteration: 4783, Loss: 1.5047074556350708\n",
      "Iteration: 4784, Loss: 1.5047013759613037\n",
      "Iteration: 4785, Loss: 1.5046952962875366\n",
      "Iteration: 4786, Loss: 1.5046892166137695\n",
      "Iteration: 4787, Loss: 1.5046831369400024\n",
      "Iteration: 4788, Loss: 1.5046770572662354\n",
      "Iteration: 4789, Loss: 1.5046710968017578\n",
      "Iteration: 4790, Loss: 1.5046650171279907\n",
      "Iteration: 4791, Loss: 1.5046589374542236\n",
      "Iteration: 4792, Loss: 1.5046528577804565\n",
      "Iteration: 4793, Loss: 1.504646897315979\n",
      "Iteration: 4794, Loss: 1.504640817642212\n",
      "Iteration: 4795, Loss: 1.5046347379684448\n",
      "Iteration: 4796, Loss: 1.5046287775039673\n",
      "Iteration: 4797, Loss: 1.5046226978302002\n",
      "Iteration: 4798, Loss: 1.504616618156433\n",
      "Iteration: 4799, Loss: 1.5046106576919556\n",
      "Iteration: 4800, Loss: 1.5046045780181885\n",
      "Iteration: 4801, Loss: 1.504598617553711\n",
      "Iteration: 4802, Loss: 1.5045925378799438\n",
      "Iteration: 4803, Loss: 1.5045865774154663\n",
      "Iteration: 4804, Loss: 1.5045804977416992\n",
      "Iteration: 4805, Loss: 1.5045745372772217\n",
      "Iteration: 4806, Loss: 1.5045684576034546\n",
      "Iteration: 4807, Loss: 1.504562497138977\n",
      "Iteration: 4808, Loss: 1.50455641746521\n",
      "Iteration: 4809, Loss: 1.5045504570007324\n",
      "Iteration: 4810, Loss: 1.5045444965362549\n",
      "Iteration: 4811, Loss: 1.5045384168624878\n",
      "Iteration: 4812, Loss: 1.5045324563980103\n",
      "Iteration: 4813, Loss: 1.5045264959335327\n",
      "Iteration: 4814, Loss: 1.5045204162597656\n",
      "Iteration: 4815, Loss: 1.504514455795288\n",
      "Iteration: 4816, Loss: 1.5045084953308105\n",
      "Iteration: 4817, Loss: 1.504502534866333\n",
      "Iteration: 4818, Loss: 1.504496455192566\n",
      "Iteration: 4819, Loss: 1.5044904947280884\n",
      "Iteration: 4820, Loss: 1.5044845342636108\n",
      "Iteration: 4821, Loss: 1.5044785737991333\n",
      "Iteration: 4822, Loss: 1.5044726133346558\n",
      "Iteration: 4823, Loss: 1.5044666528701782\n",
      "Iteration: 4824, Loss: 1.5044606924057007\n",
      "Iteration: 4825, Loss: 1.5044547319412231\n",
      "Iteration: 4826, Loss: 1.5044487714767456\n",
      "Iteration: 4827, Loss: 1.504442811012268\n",
      "Iteration: 4828, Loss: 1.5044368505477905\n",
      "Iteration: 4829, Loss: 1.504430890083313\n",
      "Iteration: 4830, Loss: 1.5044249296188354\n",
      "Iteration: 4831, Loss: 1.504418969154358\n",
      "Iteration: 4832, Loss: 1.5044130086898804\n",
      "Iteration: 4833, Loss: 1.5044070482254028\n",
      "Iteration: 4834, Loss: 1.5044010877609253\n",
      "Iteration: 4835, Loss: 1.5043951272964478\n",
      "Iteration: 4836, Loss: 1.5043891668319702\n",
      "Iteration: 4837, Loss: 1.5043832063674927\n",
      "Iteration: 4838, Loss: 1.5043773651123047\n",
      "Iteration: 4839, Loss: 1.5043714046478271\n",
      "Iteration: 4840, Loss: 1.5043654441833496\n",
      "Iteration: 4841, Loss: 1.504359483718872\n",
      "Iteration: 4842, Loss: 1.504353642463684\n",
      "Iteration: 4843, Loss: 1.5043476819992065\n",
      "Iteration: 4844, Loss: 1.504341721534729\n",
      "Iteration: 4845, Loss: 1.504335880279541\n",
      "Iteration: 4846, Loss: 1.5043299198150635\n",
      "Iteration: 4847, Loss: 1.504323959350586\n",
      "Iteration: 4848, Loss: 1.504318118095398\n",
      "Iteration: 4849, Loss: 1.5043121576309204\n",
      "Iteration: 4850, Loss: 1.5043061971664429\n",
      "Iteration: 4851, Loss: 1.5043003559112549\n",
      "Iteration: 4852, Loss: 1.5042943954467773\n",
      "Iteration: 4853, Loss: 1.5042885541915894\n",
      "Iteration: 4854, Loss: 1.5042825937271118\n",
      "Iteration: 4855, Loss: 1.5042767524719238\n",
      "Iteration: 4856, Loss: 1.5042707920074463\n",
      "Iteration: 4857, Loss: 1.5042649507522583\n",
      "Iteration: 4858, Loss: 1.5042591094970703\n",
      "Iteration: 4859, Loss: 1.5042531490325928\n",
      "Iteration: 4860, Loss: 1.5042473077774048\n",
      "Iteration: 4861, Loss: 1.5042413473129272\n",
      "Iteration: 4862, Loss: 1.5042355060577393\n",
      "Iteration: 4863, Loss: 1.5042296648025513\n",
      "Iteration: 4864, Loss: 1.5042238235473633\n",
      "Iteration: 4865, Loss: 1.5042178630828857\n",
      "Iteration: 4866, Loss: 1.5042120218276978\n",
      "Iteration: 4867, Loss: 1.5042061805725098\n",
      "Iteration: 4868, Loss: 1.5042003393173218\n",
      "Iteration: 4869, Loss: 1.5041943788528442\n",
      "Iteration: 4870, Loss: 1.5041885375976562\n",
      "Iteration: 4871, Loss: 1.5041826963424683\n",
      "Iteration: 4872, Loss: 1.5041768550872803\n",
      "Iteration: 4873, Loss: 1.5041710138320923\n",
      "Iteration: 4874, Loss: 1.5041651725769043\n",
      "Iteration: 4875, Loss: 1.5041593313217163\n",
      "Iteration: 4876, Loss: 1.5041534900665283\n",
      "Iteration: 4877, Loss: 1.5041475296020508\n",
      "Iteration: 4878, Loss: 1.5041416883468628\n",
      "Iteration: 4879, Loss: 1.5041358470916748\n",
      "Iteration: 4880, Loss: 1.5041300058364868\n",
      "Iteration: 4881, Loss: 1.5041241645812988\n",
      "Iteration: 4882, Loss: 1.5041184425354004\n",
      "Iteration: 4883, Loss: 1.5041126012802124\n",
      "Iteration: 4884, Loss: 1.5041067600250244\n",
      "Iteration: 4885, Loss: 1.5041009187698364\n",
      "Iteration: 4886, Loss: 1.5040950775146484\n",
      "Iteration: 4887, Loss: 1.5040892362594604\n",
      "Iteration: 4888, Loss: 1.5040833950042725\n",
      "Iteration: 4889, Loss: 1.5040775537490845\n",
      "Iteration: 4890, Loss: 1.504071831703186\n",
      "Iteration: 4891, Loss: 1.504065990447998\n",
      "Iteration: 4892, Loss: 1.50406014919281\n",
      "Iteration: 4893, Loss: 1.504054307937622\n",
      "Iteration: 4894, Loss: 1.5040485858917236\n",
      "Iteration: 4895, Loss: 1.5040427446365356\n",
      "Iteration: 4896, Loss: 1.5040369033813477\n",
      "Iteration: 4897, Loss: 1.5040311813354492\n",
      "Iteration: 4898, Loss: 1.5040253400802612\n",
      "Iteration: 4899, Loss: 1.5040194988250732\n",
      "Iteration: 4900, Loss: 1.5040137767791748\n",
      "Iteration: 4901, Loss: 1.5040079355239868\n",
      "Iteration: 4902, Loss: 1.5040022134780884\n",
      "Iteration: 4903, Loss: 1.5039963722229004\n",
      "Iteration: 4904, Loss: 1.5039905309677124\n",
      "Iteration: 4905, Loss: 1.503984808921814\n",
      "Iteration: 4906, Loss: 1.503978967666626\n",
      "Iteration: 4907, Loss: 1.5039732456207275\n",
      "Iteration: 4908, Loss: 1.503967523574829\n",
      "Iteration: 4909, Loss: 1.5039616823196411\n",
      "Iteration: 4910, Loss: 1.5039559602737427\n",
      "Iteration: 4911, Loss: 1.5039501190185547\n",
      "Iteration: 4912, Loss: 1.5039443969726562\n",
      "Iteration: 4913, Loss: 1.5039386749267578\n",
      "Iteration: 4914, Loss: 1.5039328336715698\n",
      "Iteration: 4915, Loss: 1.5039271116256714\n",
      "Iteration: 4916, Loss: 1.503921389579773\n",
      "Iteration: 4917, Loss: 1.503915548324585\n",
      "Iteration: 4918, Loss: 1.5039098262786865\n",
      "Iteration: 4919, Loss: 1.503904104232788\n",
      "Iteration: 4920, Loss: 1.5038983821868896\n",
      "Iteration: 4921, Loss: 1.5038925409317017\n",
      "Iteration: 4922, Loss: 1.5038868188858032\n",
      "Iteration: 4923, Loss: 1.5038810968399048\n",
      "Iteration: 4924, Loss: 1.5038753747940063\n",
      "Iteration: 4925, Loss: 1.503869652748108\n",
      "Iteration: 4926, Loss: 1.5038639307022095\n",
      "Iteration: 4927, Loss: 1.503858208656311\n",
      "Iteration: 4928, Loss: 1.503852367401123\n",
      "Iteration: 4929, Loss: 1.5038466453552246\n",
      "Iteration: 4930, Loss: 1.5038409233093262\n",
      "Iteration: 4931, Loss: 1.5038352012634277\n",
      "Iteration: 4932, Loss: 1.5038294792175293\n",
      "Iteration: 4933, Loss: 1.5038237571716309\n",
      "Iteration: 4934, Loss: 1.5038180351257324\n",
      "Iteration: 4935, Loss: 1.503812313079834\n",
      "Iteration: 4936, Loss: 1.503806710243225\n",
      "Iteration: 4937, Loss: 1.5038009881973267\n",
      "Iteration: 4938, Loss: 1.5037952661514282\n",
      "Iteration: 4939, Loss: 1.5037895441055298\n",
      "Iteration: 4940, Loss: 1.5037838220596313\n",
      "Iteration: 4941, Loss: 1.503778100013733\n",
      "Iteration: 4942, Loss: 1.5037723779678345\n",
      "Iteration: 4943, Loss: 1.5037667751312256\n",
      "Iteration: 4944, Loss: 1.5037610530853271\n",
      "Iteration: 4945, Loss: 1.5037553310394287\n",
      "Iteration: 4946, Loss: 1.5037496089935303\n",
      "Iteration: 4947, Loss: 1.5037440061569214\n",
      "Iteration: 4948, Loss: 1.503738284111023\n",
      "Iteration: 4949, Loss: 1.5037325620651245\n",
      "Iteration: 4950, Loss: 1.5037269592285156\n",
      "Iteration: 4951, Loss: 1.5037212371826172\n",
      "Iteration: 4952, Loss: 1.5037155151367188\n",
      "Iteration: 4953, Loss: 1.5037099123001099\n",
      "Iteration: 4954, Loss: 1.5037041902542114\n",
      "Iteration: 4955, Loss: 1.503698468208313\n",
      "Iteration: 4956, Loss: 1.503692865371704\n",
      "Iteration: 4957, Loss: 1.5036871433258057\n",
      "Iteration: 4958, Loss: 1.5036815404891968\n",
      "Iteration: 4959, Loss: 1.5036758184432983\n",
      "Iteration: 4960, Loss: 1.5036702156066895\n",
      "Iteration: 4961, Loss: 1.503664493560791\n",
      "Iteration: 4962, Loss: 1.5036588907241821\n",
      "Iteration: 4963, Loss: 1.5036532878875732\n",
      "Iteration: 4964, Loss: 1.5036475658416748\n",
      "Iteration: 4965, Loss: 1.503641963005066\n",
      "Iteration: 4966, Loss: 1.5036362409591675\n",
      "Iteration: 4967, Loss: 1.5036306381225586\n",
      "Iteration: 4968, Loss: 1.5036250352859497\n",
      "Iteration: 4969, Loss: 1.5036193132400513\n",
      "Iteration: 4970, Loss: 1.5036137104034424\n",
      "Iteration: 4971, Loss: 1.5036081075668335\n",
      "Iteration: 4972, Loss: 1.5036025047302246\n",
      "Iteration: 4973, Loss: 1.5035967826843262\n",
      "Iteration: 4974, Loss: 1.5035911798477173\n",
      "Iteration: 4975, Loss: 1.5035855770111084\n",
      "Iteration: 4976, Loss: 1.5035799741744995\n",
      "Iteration: 4977, Loss: 1.503574252128601\n",
      "Iteration: 4978, Loss: 1.5035686492919922\n",
      "Iteration: 4979, Loss: 1.5035630464553833\n",
      "Iteration: 4980, Loss: 1.5035574436187744\n",
      "Iteration: 4981, Loss: 1.5035518407821655\n",
      "Iteration: 4982, Loss: 1.5035462379455566\n",
      "Iteration: 4983, Loss: 1.5035406351089478\n",
      "Iteration: 4984, Loss: 1.5035350322723389\n",
      "Iteration: 4985, Loss: 1.50352942943573\n",
      "Iteration: 4986, Loss: 1.503523826599121\n",
      "Iteration: 4987, Loss: 1.5035182237625122\n",
      "Iteration: 4988, Loss: 1.5035126209259033\n",
      "Iteration: 4989, Loss: 1.5035070180892944\n",
      "Iteration: 4990, Loss: 1.5035014152526855\n",
      "Iteration: 4991, Loss: 1.5034958124160767\n",
      "Iteration: 4992, Loss: 1.5034902095794678\n",
      "Iteration: 4993, Loss: 1.5034846067428589\n",
      "Iteration: 4994, Loss: 1.50347900390625\n",
      "Iteration: 4995, Loss: 1.5034735202789307\n",
      "Iteration: 4996, Loss: 1.5034679174423218\n",
      "Iteration: 4997, Loss: 1.503462314605713\n",
      "Iteration: 4998, Loss: 1.503456711769104\n",
      "Iteration: 4999, Loss: 1.5034511089324951\n",
      "Iteration: 5000, Loss: 1.5034456253051758\n",
      "Iteration: 5001, Loss: 1.503440022468567\n",
      "Iteration: 5002, Loss: 1.503434419631958\n",
      "Iteration: 5003, Loss: 1.5034288167953491\n",
      "Iteration: 5004, Loss: 1.5034233331680298\n",
      "Iteration: 5005, Loss: 1.503417730331421\n",
      "Iteration: 5006, Loss: 1.5034122467041016\n",
      "Iteration: 5007, Loss: 1.5034066438674927\n",
      "Iteration: 5008, Loss: 1.5034010410308838\n",
      "Iteration: 5009, Loss: 1.5033955574035645\n",
      "Iteration: 5010, Loss: 1.5033899545669556\n",
      "Iteration: 5011, Loss: 1.5033844709396362\n",
      "Iteration: 5012, Loss: 1.5033788681030273\n",
      "Iteration: 5013, Loss: 1.503373384475708\n",
      "Iteration: 5014, Loss: 1.5033677816390991\n",
      "Iteration: 5015, Loss: 1.5033622980117798\n",
      "Iteration: 5016, Loss: 1.503356695175171\n",
      "Iteration: 5017, Loss: 1.5033512115478516\n",
      "Iteration: 5018, Loss: 1.5033456087112427\n",
      "Iteration: 5019, Loss: 1.5033401250839233\n",
      "Iteration: 5020, Loss: 1.5033345222473145\n",
      "Iteration: 5021, Loss: 1.5033290386199951\n",
      "Iteration: 5022, Loss: 1.5033235549926758\n",
      "Iteration: 5023, Loss: 1.503317952156067\n",
      "Iteration: 5024, Loss: 1.5033124685287476\n",
      "Iteration: 5025, Loss: 1.5033069849014282\n",
      "Iteration: 5026, Loss: 1.5033013820648193\n",
      "Iteration: 5027, Loss: 1.5032958984375\n",
      "Iteration: 5028, Loss: 1.5032904148101807\n",
      "Iteration: 5029, Loss: 1.5032849311828613\n",
      "Iteration: 5030, Loss: 1.503279447555542\n",
      "Iteration: 5031, Loss: 1.503273844718933\n",
      "Iteration: 5032, Loss: 1.5032683610916138\n",
      "Iteration: 5033, Loss: 1.5032628774642944\n",
      "Iteration: 5034, Loss: 1.503257393836975\n",
      "Iteration: 5035, Loss: 1.5032519102096558\n",
      "Iteration: 5036, Loss: 1.5032464265823364\n",
      "Iteration: 5037, Loss: 1.503240942955017\n",
      "Iteration: 5038, Loss: 1.5032354593276978\n",
      "Iteration: 5039, Loss: 1.5032298564910889\n",
      "Iteration: 5040, Loss: 1.5032243728637695\n",
      "Iteration: 5041, Loss: 1.5032188892364502\n",
      "Iteration: 5042, Loss: 1.5032134056091309\n",
      "Iteration: 5043, Loss: 1.5032079219818115\n",
      "Iteration: 5044, Loss: 1.5032025575637817\n",
      "Iteration: 5045, Loss: 1.5031970739364624\n",
      "Iteration: 5046, Loss: 1.503191590309143\n",
      "Iteration: 5047, Loss: 1.5031861066818237\n",
      "Iteration: 5048, Loss: 1.5031806230545044\n",
      "Iteration: 5049, Loss: 1.503175139427185\n",
      "Iteration: 5050, Loss: 1.5031696557998657\n",
      "Iteration: 5051, Loss: 1.5031641721725464\n",
      "Iteration: 5052, Loss: 1.5031588077545166\n",
      "Iteration: 5053, Loss: 1.5031533241271973\n",
      "Iteration: 5054, Loss: 1.503147840499878\n",
      "Iteration: 5055, Loss: 1.5031423568725586\n",
      "Iteration: 5056, Loss: 1.5031368732452393\n",
      "Iteration: 5057, Loss: 1.5031315088272095\n",
      "Iteration: 5058, Loss: 1.5031260251998901\n",
      "Iteration: 5059, Loss: 1.5031205415725708\n",
      "Iteration: 5060, Loss: 1.503115177154541\n",
      "Iteration: 5061, Loss: 1.5031096935272217\n",
      "Iteration: 5062, Loss: 1.5031042098999023\n",
      "Iteration: 5063, Loss: 1.5030988454818726\n",
      "Iteration: 5064, Loss: 1.5030933618545532\n",
      "Iteration: 5065, Loss: 1.5030879974365234\n",
      "Iteration: 5066, Loss: 1.503082513809204\n",
      "Iteration: 5067, Loss: 1.5030770301818848\n",
      "Iteration: 5068, Loss: 1.503071665763855\n",
      "Iteration: 5069, Loss: 1.5030661821365356\n",
      "Iteration: 5070, Loss: 1.5030608177185059\n",
      "Iteration: 5071, Loss: 1.5030553340911865\n",
      "Iteration: 5072, Loss: 1.5030499696731567\n",
      "Iteration: 5073, Loss: 1.503044605255127\n",
      "Iteration: 5074, Loss: 1.5030391216278076\n",
      "Iteration: 5075, Loss: 1.5030337572097778\n",
      "Iteration: 5076, Loss: 1.5030282735824585\n",
      "Iteration: 5077, Loss: 1.5030229091644287\n",
      "Iteration: 5078, Loss: 1.503017544746399\n",
      "Iteration: 5079, Loss: 1.5030120611190796\n",
      "Iteration: 5080, Loss: 1.5030066967010498\n",
      "Iteration: 5081, Loss: 1.50300133228302\n",
      "Iteration: 5082, Loss: 1.5029958486557007\n",
      "Iteration: 5083, Loss: 1.502990484237671\n",
      "Iteration: 5084, Loss: 1.5029851198196411\n",
      "Iteration: 5085, Loss: 1.5029797554016113\n",
      "Iteration: 5086, Loss: 1.502974271774292\n",
      "Iteration: 5087, Loss: 1.5029689073562622\n",
      "Iteration: 5088, Loss: 1.5029635429382324\n",
      "Iteration: 5089, Loss: 1.5029581785202026\n",
      "Iteration: 5090, Loss: 1.5029528141021729\n",
      "Iteration: 5091, Loss: 1.502947449684143\n",
      "Iteration: 5092, Loss: 1.5029420852661133\n",
      "Iteration: 5093, Loss: 1.502936601638794\n",
      "Iteration: 5094, Loss: 1.5029312372207642\n",
      "Iteration: 5095, Loss: 1.5029258728027344\n",
      "Iteration: 5096, Loss: 1.5029205083847046\n",
      "Iteration: 5097, Loss: 1.5029151439666748\n",
      "Iteration: 5098, Loss: 1.502909779548645\n",
      "Iteration: 5099, Loss: 1.5029044151306152\n",
      "Iteration: 5100, Loss: 1.5028990507125854\n",
      "Iteration: 5101, Loss: 1.5028936862945557\n",
      "Iteration: 5102, Loss: 1.5028883218765259\n",
      "Iteration: 5103, Loss: 1.5028830766677856\n",
      "Iteration: 5104, Loss: 1.5028777122497559\n",
      "Iteration: 5105, Loss: 1.502872347831726\n",
      "Iteration: 5106, Loss: 1.5028669834136963\n",
      "Iteration: 5107, Loss: 1.5028616189956665\n",
      "Iteration: 5108, Loss: 1.5028562545776367\n",
      "Iteration: 5109, Loss: 1.502850890159607\n",
      "Iteration: 5110, Loss: 1.5028456449508667\n",
      "Iteration: 5111, Loss: 1.502840280532837\n",
      "Iteration: 5112, Loss: 1.5028349161148071\n",
      "Iteration: 5113, Loss: 1.5028295516967773\n",
      "Iteration: 5114, Loss: 1.502824306488037\n",
      "Iteration: 5115, Loss: 1.5028189420700073\n",
      "Iteration: 5116, Loss: 1.5028135776519775\n",
      "Iteration: 5117, Loss: 1.5028083324432373\n",
      "Iteration: 5118, Loss: 1.5028029680252075\n",
      "Iteration: 5119, Loss: 1.5027976036071777\n",
      "Iteration: 5120, Loss: 1.5027923583984375\n",
      "Iteration: 5121, Loss: 1.5027869939804077\n",
      "Iteration: 5122, Loss: 1.5027817487716675\n",
      "Iteration: 5123, Loss: 1.5027763843536377\n",
      "Iteration: 5124, Loss: 1.502771019935608\n",
      "Iteration: 5125, Loss: 1.5027657747268677\n",
      "Iteration: 5126, Loss: 1.502760410308838\n",
      "Iteration: 5127, Loss: 1.5027551651000977\n",
      "Iteration: 5128, Loss: 1.5027498006820679\n",
      "Iteration: 5129, Loss: 1.5027445554733276\n",
      "Iteration: 5130, Loss: 1.5027393102645874\n",
      "Iteration: 5131, Loss: 1.5027339458465576\n",
      "Iteration: 5132, Loss: 1.5027287006378174\n",
      "Iteration: 5133, Loss: 1.5027233362197876\n",
      "Iteration: 5134, Loss: 1.5027180910110474\n",
      "Iteration: 5135, Loss: 1.5027128458023071\n",
      "Iteration: 5136, Loss: 1.5027074813842773\n",
      "Iteration: 5137, Loss: 1.502702236175537\n",
      "Iteration: 5138, Loss: 1.5026969909667969\n",
      "Iteration: 5139, Loss: 1.502691626548767\n",
      "Iteration: 5140, Loss: 1.5026863813400269\n",
      "Iteration: 5141, Loss: 1.5026811361312866\n",
      "Iteration: 5142, Loss: 1.5026758909225464\n",
      "Iteration: 5143, Loss: 1.5026706457138062\n",
      "Iteration: 5144, Loss: 1.5026652812957764\n",
      "Iteration: 5145, Loss: 1.5026600360870361\n",
      "Iteration: 5146, Loss: 1.502654790878296\n",
      "Iteration: 5147, Loss: 1.5026495456695557\n",
      "Iteration: 5148, Loss: 1.5026443004608154\n",
      "Iteration: 5149, Loss: 1.5026390552520752\n",
      "Iteration: 5150, Loss: 1.502633810043335\n",
      "Iteration: 5151, Loss: 1.5026284456253052\n",
      "Iteration: 5152, Loss: 1.502623200416565\n",
      "Iteration: 5153, Loss: 1.5026179552078247\n",
      "Iteration: 5154, Loss: 1.5026127099990845\n",
      "Iteration: 5155, Loss: 1.5026074647903442\n",
      "Iteration: 5156, Loss: 1.502602219581604\n",
      "Iteration: 5157, Loss: 1.5025969743728638\n",
      "Iteration: 5158, Loss: 1.5025917291641235\n",
      "Iteration: 5159, Loss: 1.5025866031646729\n",
      "Iteration: 5160, Loss: 1.5025813579559326\n",
      "Iteration: 5161, Loss: 1.5025761127471924\n",
      "Iteration: 5162, Loss: 1.5025708675384521\n",
      "Iteration: 5163, Loss: 1.502565622329712\n",
      "Iteration: 5164, Loss: 1.5025603771209717\n",
      "Iteration: 5165, Loss: 1.5025551319122314\n",
      "Iteration: 5166, Loss: 1.5025500059127808\n",
      "Iteration: 5167, Loss: 1.5025447607040405\n",
      "Iteration: 5168, Loss: 1.5025395154953003\n",
      "Iteration: 5169, Loss: 1.50253427028656\n",
      "Iteration: 5170, Loss: 1.5025290250778198\n",
      "Iteration: 5171, Loss: 1.5025238990783691\n",
      "Iteration: 5172, Loss: 1.502518653869629\n",
      "Iteration: 5173, Loss: 1.5025134086608887\n",
      "Iteration: 5174, Loss: 1.502508282661438\n",
      "Iteration: 5175, Loss: 1.5025030374526978\n",
      "Iteration: 5176, Loss: 1.5024977922439575\n",
      "Iteration: 5177, Loss: 1.5024926662445068\n",
      "Iteration: 5178, Loss: 1.5024874210357666\n",
      "Iteration: 5179, Loss: 1.502482295036316\n",
      "Iteration: 5180, Loss: 1.5024770498275757\n",
      "Iteration: 5181, Loss: 1.502471923828125\n",
      "Iteration: 5182, Loss: 1.5024666786193848\n",
      "Iteration: 5183, Loss: 1.5024614334106445\n",
      "Iteration: 5184, Loss: 1.5024563074111938\n",
      "Iteration: 5185, Loss: 1.5024510622024536\n",
      "Iteration: 5186, Loss: 1.502445936203003\n",
      "Iteration: 5187, Loss: 1.5024408102035522\n",
      "Iteration: 5188, Loss: 1.502435564994812\n",
      "Iteration: 5189, Loss: 1.5024304389953613\n",
      "Iteration: 5190, Loss: 1.502425193786621\n",
      "Iteration: 5191, Loss: 1.5024200677871704\n",
      "Iteration: 5192, Loss: 1.5024149417877197\n",
      "Iteration: 5193, Loss: 1.5024096965789795\n",
      "Iteration: 5194, Loss: 1.5024045705795288\n",
      "Iteration: 5195, Loss: 1.5023994445800781\n",
      "Iteration: 5196, Loss: 1.502394199371338\n",
      "Iteration: 5197, Loss: 1.5023890733718872\n",
      "Iteration: 5198, Loss: 1.5023839473724365\n",
      "Iteration: 5199, Loss: 1.5023788213729858\n",
      "Iteration: 5200, Loss: 1.5023735761642456\n",
      "Iteration: 5201, Loss: 1.502368450164795\n",
      "Iteration: 5202, Loss: 1.5023633241653442\n",
      "Iteration: 5203, Loss: 1.5023581981658936\n",
      "Iteration: 5204, Loss: 1.5023530721664429\n",
      "Iteration: 5205, Loss: 1.5023479461669922\n",
      "Iteration: 5206, Loss: 1.502342700958252\n",
      "Iteration: 5207, Loss: 1.5023375749588013\n",
      "Iteration: 5208, Loss: 1.5023324489593506\n",
      "Iteration: 5209, Loss: 1.5023273229599\n",
      "Iteration: 5210, Loss: 1.5023221969604492\n",
      "Iteration: 5211, Loss: 1.5023170709609985\n",
      "Iteration: 5212, Loss: 1.5023119449615479\n",
      "Iteration: 5213, Loss: 1.5023068189620972\n",
      "Iteration: 5214, Loss: 1.5023016929626465\n",
      "Iteration: 5215, Loss: 1.5022965669631958\n",
      "Iteration: 5216, Loss: 1.5022914409637451\n",
      "Iteration: 5217, Loss: 1.5022863149642944\n",
      "Iteration: 5218, Loss: 1.5022811889648438\n",
      "Iteration: 5219, Loss: 1.5022761821746826\n",
      "Iteration: 5220, Loss: 1.502271056175232\n",
      "Iteration: 5221, Loss: 1.5022659301757812\n",
      "Iteration: 5222, Loss: 1.5022608041763306\n",
      "Iteration: 5223, Loss: 1.5022556781768799\n",
      "Iteration: 5224, Loss: 1.5022505521774292\n",
      "Iteration: 5225, Loss: 1.502245545387268\n",
      "Iteration: 5226, Loss: 1.5022404193878174\n",
      "Iteration: 5227, Loss: 1.5022352933883667\n",
      "Iteration: 5228, Loss: 1.502230167388916\n",
      "Iteration: 5229, Loss: 1.5022251605987549\n",
      "Iteration: 5230, Loss: 1.5022200345993042\n",
      "Iteration: 5231, Loss: 1.5022149085998535\n",
      "Iteration: 5232, Loss: 1.5022099018096924\n",
      "Iteration: 5233, Loss: 1.5022047758102417\n",
      "Iteration: 5234, Loss: 1.502199649810791\n",
      "Iteration: 5235, Loss: 1.5021946430206299\n",
      "Iteration: 5236, Loss: 1.5021895170211792\n",
      "Iteration: 5237, Loss: 1.502184510231018\n",
      "Iteration: 5238, Loss: 1.5021793842315674\n",
      "Iteration: 5239, Loss: 1.5021742582321167\n",
      "Iteration: 5240, Loss: 1.5021692514419556\n",
      "Iteration: 5241, Loss: 1.5021641254425049\n",
      "Iteration: 5242, Loss: 1.5021591186523438\n",
      "Iteration: 5243, Loss: 1.502153992652893\n",
      "Iteration: 5244, Loss: 1.502148985862732\n",
      "Iteration: 5245, Loss: 1.5021439790725708\n",
      "Iteration: 5246, Loss: 1.5021388530731201\n",
      "Iteration: 5247, Loss: 1.502133846282959\n",
      "Iteration: 5248, Loss: 1.5021287202835083\n",
      "Iteration: 5249, Loss: 1.5021237134933472\n",
      "Iteration: 5250, Loss: 1.502118706703186\n",
      "Iteration: 5251, Loss: 1.5021135807037354\n",
      "Iteration: 5252, Loss: 1.5021085739135742\n",
      "Iteration: 5253, Loss: 1.502103567123413\n",
      "Iteration: 5254, Loss: 1.5020984411239624\n",
      "Iteration: 5255, Loss: 1.5020934343338013\n",
      "Iteration: 5256, Loss: 1.5020884275436401\n",
      "Iteration: 5257, Loss: 1.502083420753479\n",
      "Iteration: 5258, Loss: 1.5020782947540283\n",
      "Iteration: 5259, Loss: 1.5020732879638672\n",
      "Iteration: 5260, Loss: 1.502068281173706\n",
      "Iteration: 5261, Loss: 1.502063274383545\n",
      "Iteration: 5262, Loss: 1.5020582675933838\n",
      "Iteration: 5263, Loss: 1.5020532608032227\n",
      "Iteration: 5264, Loss: 1.502048134803772\n",
      "Iteration: 5265, Loss: 1.5020431280136108\n",
      "Iteration: 5266, Loss: 1.5020381212234497\n",
      "Iteration: 5267, Loss: 1.5020331144332886\n",
      "Iteration: 5268, Loss: 1.5020281076431274\n",
      "Iteration: 5269, Loss: 1.5020231008529663\n",
      "Iteration: 5270, Loss: 1.5020180940628052\n",
      "Iteration: 5271, Loss: 1.502013087272644\n",
      "Iteration: 5272, Loss: 1.502008080482483\n",
      "Iteration: 5273, Loss: 1.5020030736923218\n",
      "Iteration: 5274, Loss: 1.5019980669021606\n",
      "Iteration: 5275, Loss: 1.5019930601119995\n",
      "Iteration: 5276, Loss: 1.501988172531128\n",
      "Iteration: 5277, Loss: 1.5019831657409668\n",
      "Iteration: 5278, Loss: 1.5019781589508057\n",
      "Iteration: 5279, Loss: 1.5019731521606445\n",
      "Iteration: 5280, Loss: 1.5019681453704834\n",
      "Iteration: 5281, Loss: 1.5019631385803223\n",
      "Iteration: 5282, Loss: 1.5019581317901611\n",
      "Iteration: 5283, Loss: 1.5019532442092896\n",
      "Iteration: 5284, Loss: 1.5019482374191284\n",
      "Iteration: 5285, Loss: 1.5019432306289673\n",
      "Iteration: 5286, Loss: 1.5019382238388062\n",
      "Iteration: 5287, Loss: 1.5019333362579346\n",
      "Iteration: 5288, Loss: 1.5019283294677734\n",
      "Iteration: 5289, Loss: 1.5019233226776123\n",
      "Iteration: 5290, Loss: 1.5019184350967407\n",
      "Iteration: 5291, Loss: 1.5019134283065796\n",
      "Iteration: 5292, Loss: 1.5019084215164185\n",
      "Iteration: 5293, Loss: 1.5019035339355469\n",
      "Iteration: 5294, Loss: 1.5018985271453857\n",
      "Iteration: 5295, Loss: 1.5018936395645142\n",
      "Iteration: 5296, Loss: 1.501888632774353\n",
      "Iteration: 5297, Loss: 1.501883625984192\n",
      "Iteration: 5298, Loss: 1.5018787384033203\n",
      "Iteration: 5299, Loss: 1.5018737316131592\n",
      "Iteration: 5300, Loss: 1.5018688440322876\n",
      "Iteration: 5301, Loss: 1.5018638372421265\n",
      "Iteration: 5302, Loss: 1.5018589496612549\n",
      "Iteration: 5303, Loss: 1.5018540620803833\n",
      "Iteration: 5304, Loss: 1.5018490552902222\n",
      "Iteration: 5305, Loss: 1.5018441677093506\n",
      "Iteration: 5306, Loss: 1.5018391609191895\n",
      "Iteration: 5307, Loss: 1.5018342733383179\n",
      "Iteration: 5308, Loss: 1.5018293857574463\n",
      "Iteration: 5309, Loss: 1.5018243789672852\n",
      "Iteration: 5310, Loss: 1.5018194913864136\n",
      "Iteration: 5311, Loss: 1.501814603805542\n",
      "Iteration: 5312, Loss: 1.5018095970153809\n",
      "Iteration: 5313, Loss: 1.5018047094345093\n",
      "Iteration: 5314, Loss: 1.5017998218536377\n",
      "Iteration: 5315, Loss: 1.5017949342727661\n",
      "Iteration: 5316, Loss: 1.501789927482605\n",
      "Iteration: 5317, Loss: 1.5017850399017334\n",
      "Iteration: 5318, Loss: 1.5017801523208618\n",
      "Iteration: 5319, Loss: 1.5017752647399902\n",
      "Iteration: 5320, Loss: 1.5017703771591187\n",
      "Iteration: 5321, Loss: 1.501765489578247\n",
      "Iteration: 5322, Loss: 1.5017606019973755\n",
      "Iteration: 5323, Loss: 1.5017555952072144\n",
      "Iteration: 5324, Loss: 1.5017507076263428\n",
      "Iteration: 5325, Loss: 1.5017458200454712\n",
      "Iteration: 5326, Loss: 1.5017409324645996\n",
      "Iteration: 5327, Loss: 1.501736044883728\n",
      "Iteration: 5328, Loss: 1.5017311573028564\n",
      "Iteration: 5329, Loss: 1.5017262697219849\n",
      "Iteration: 5330, Loss: 1.5017213821411133\n",
      "Iteration: 5331, Loss: 1.5017164945602417\n",
      "Iteration: 5332, Loss: 1.5017116069793701\n",
      "Iteration: 5333, Loss: 1.5017067193984985\n",
      "Iteration: 5334, Loss: 1.5017019510269165\n",
      "Iteration: 5335, Loss: 1.501697063446045\n",
      "Iteration: 5336, Loss: 1.5016921758651733\n",
      "Iteration: 5337, Loss: 1.5016872882843018\n",
      "Iteration: 5338, Loss: 1.5016824007034302\n",
      "Iteration: 5339, Loss: 1.5016775131225586\n",
      "Iteration: 5340, Loss: 1.501672625541687\n",
      "Iteration: 5341, Loss: 1.501667857170105\n",
      "Iteration: 5342, Loss: 1.5016629695892334\n",
      "Iteration: 5343, Loss: 1.5016580820083618\n",
      "Iteration: 5344, Loss: 1.5016531944274902\n",
      "Iteration: 5345, Loss: 1.5016484260559082\n",
      "Iteration: 5346, Loss: 1.5016435384750366\n",
      "Iteration: 5347, Loss: 1.501638650894165\n",
      "Iteration: 5348, Loss: 1.501633882522583\n",
      "Iteration: 5349, Loss: 1.5016289949417114\n",
      "Iteration: 5350, Loss: 1.5016241073608398\n",
      "Iteration: 5351, Loss: 1.5016193389892578\n",
      "Iteration: 5352, Loss: 1.5016144514083862\n",
      "Iteration: 5353, Loss: 1.5016096830368042\n",
      "Iteration: 5354, Loss: 1.5016047954559326\n",
      "Iteration: 5355, Loss: 1.501599907875061\n",
      "Iteration: 5356, Loss: 1.501595139503479\n",
      "Iteration: 5357, Loss: 1.5015902519226074\n",
      "Iteration: 5358, Loss: 1.5015854835510254\n",
      "Iteration: 5359, Loss: 1.5015805959701538\n",
      "Iteration: 5360, Loss: 1.5015758275985718\n",
      "Iteration: 5361, Loss: 1.5015710592269897\n",
      "Iteration: 5362, Loss: 1.5015661716461182\n",
      "Iteration: 5363, Loss: 1.5015614032745361\n",
      "Iteration: 5364, Loss: 1.5015565156936646\n",
      "Iteration: 5365, Loss: 1.5015517473220825\n",
      "Iteration: 5366, Loss: 1.5015469789505005\n",
      "Iteration: 5367, Loss: 1.501542091369629\n",
      "Iteration: 5368, Loss: 1.5015373229980469\n",
      "Iteration: 5369, Loss: 1.5015325546264648\n",
      "Iteration: 5370, Loss: 1.5015276670455933\n",
      "Iteration: 5371, Loss: 1.5015228986740112\n",
      "Iteration: 5372, Loss: 1.5015181303024292\n",
      "Iteration: 5373, Loss: 1.5015133619308472\n",
      "Iteration: 5374, Loss: 1.5015084743499756\n",
      "Iteration: 5375, Loss: 1.5015037059783936\n",
      "Iteration: 5376, Loss: 1.5014989376068115\n",
      "Iteration: 5377, Loss: 1.5014941692352295\n",
      "Iteration: 5378, Loss: 1.5014894008636475\n",
      "Iteration: 5379, Loss: 1.5014846324920654\n",
      "Iteration: 5380, Loss: 1.5014798641204834\n",
      "Iteration: 5381, Loss: 1.5014749765396118\n",
      "Iteration: 5382, Loss: 1.5014702081680298\n",
      "Iteration: 5383, Loss: 1.5014654397964478\n",
      "Iteration: 5384, Loss: 1.5014606714248657\n",
      "Iteration: 5385, Loss: 1.5014559030532837\n",
      "Iteration: 5386, Loss: 1.5014511346817017\n",
      "Iteration: 5387, Loss: 1.5014463663101196\n",
      "Iteration: 5388, Loss: 1.5014415979385376\n",
      "Iteration: 5389, Loss: 1.5014368295669556\n",
      "Iteration: 5390, Loss: 1.5014320611953735\n",
      "Iteration: 5391, Loss: 1.5014272928237915\n",
      "Iteration: 5392, Loss: 1.5014225244522095\n",
      "Iteration: 5393, Loss: 1.501417875289917\n",
      "Iteration: 5394, Loss: 1.501413106918335\n",
      "Iteration: 5395, Loss: 1.501408338546753\n",
      "Iteration: 5396, Loss: 1.501403570175171\n",
      "Iteration: 5397, Loss: 1.5013988018035889\n",
      "Iteration: 5398, Loss: 1.5013940334320068\n",
      "Iteration: 5399, Loss: 1.5013893842697144\n",
      "Iteration: 5400, Loss: 1.5013846158981323\n",
      "Iteration: 5401, Loss: 1.5013798475265503\n",
      "Iteration: 5402, Loss: 1.5013750791549683\n",
      "Iteration: 5403, Loss: 1.5013704299926758\n",
      "Iteration: 5404, Loss: 1.5013656616210938\n",
      "Iteration: 5405, Loss: 1.5013608932495117\n",
      "Iteration: 5406, Loss: 1.5013562440872192\n",
      "Iteration: 5407, Loss: 1.5013514757156372\n",
      "Iteration: 5408, Loss: 1.5013467073440552\n",
      "Iteration: 5409, Loss: 1.5013420581817627\n",
      "Iteration: 5410, Loss: 1.5013372898101807\n",
      "Iteration: 5411, Loss: 1.5013326406478882\n",
      "Iteration: 5412, Loss: 1.5013278722763062\n",
      "Iteration: 5413, Loss: 1.5013231039047241\n",
      "Iteration: 5414, Loss: 1.5013184547424316\n",
      "Iteration: 5415, Loss: 1.5013136863708496\n",
      "Iteration: 5416, Loss: 1.5013090372085571\n",
      "Iteration: 5417, Loss: 1.501304268836975\n",
      "Iteration: 5418, Loss: 1.5012996196746826\n",
      "Iteration: 5419, Loss: 1.5012949705123901\n",
      "Iteration: 5420, Loss: 1.501290202140808\n",
      "Iteration: 5421, Loss: 1.5012855529785156\n",
      "Iteration: 5422, Loss: 1.5012807846069336\n",
      "Iteration: 5423, Loss: 1.5012761354446411\n",
      "Iteration: 5424, Loss: 1.5012714862823486\n",
      "Iteration: 5425, Loss: 1.5012667179107666\n",
      "Iteration: 5426, Loss: 1.5012620687484741\n",
      "Iteration: 5427, Loss: 1.5012574195861816\n",
      "Iteration: 5428, Loss: 1.5012526512145996\n",
      "Iteration: 5429, Loss: 1.5012480020523071\n",
      "Iteration: 5430, Loss: 1.5012433528900146\n",
      "Iteration: 5431, Loss: 1.5012387037277222\n",
      "Iteration: 5432, Loss: 1.5012339353561401\n",
      "Iteration: 5433, Loss: 1.5012292861938477\n",
      "Iteration: 5434, Loss: 1.5012246370315552\n",
      "Iteration: 5435, Loss: 1.5012199878692627\n",
      "Iteration: 5436, Loss: 1.5012153387069702\n",
      "Iteration: 5437, Loss: 1.5012106895446777\n",
      "Iteration: 5438, Loss: 1.5012059211730957\n",
      "Iteration: 5439, Loss: 1.5012012720108032\n",
      "Iteration: 5440, Loss: 1.5011966228485107\n",
      "Iteration: 5441, Loss: 1.5011919736862183\n",
      "Iteration: 5442, Loss: 1.5011873245239258\n",
      "Iteration: 5443, Loss: 1.5011826753616333\n",
      "Iteration: 5444, Loss: 1.5011780261993408\n",
      "Iteration: 5445, Loss: 1.5011733770370483\n",
      "Iteration: 5446, Loss: 1.5011687278747559\n",
      "Iteration: 5447, Loss: 1.5011640787124634\n",
      "Iteration: 5448, Loss: 1.501159429550171\n",
      "Iteration: 5449, Loss: 1.5011547803878784\n",
      "Iteration: 5450, Loss: 1.5011502504348755\n",
      "Iteration: 5451, Loss: 1.501145601272583\n",
      "Iteration: 5452, Loss: 1.5011409521102905\n",
      "Iteration: 5453, Loss: 1.501136302947998\n",
      "Iteration: 5454, Loss: 1.5011316537857056\n",
      "Iteration: 5455, Loss: 1.501127004623413\n",
      "Iteration: 5456, Loss: 1.5011223554611206\n",
      "Iteration: 5457, Loss: 1.5011178255081177\n",
      "Iteration: 5458, Loss: 1.5011131763458252\n",
      "Iteration: 5459, Loss: 1.5011085271835327\n",
      "Iteration: 5460, Loss: 1.5011038780212402\n",
      "Iteration: 5461, Loss: 1.5010993480682373\n",
      "Iteration: 5462, Loss: 1.5010946989059448\n",
      "Iteration: 5463, Loss: 1.5010900497436523\n",
      "Iteration: 5464, Loss: 1.5010855197906494\n",
      "Iteration: 5465, Loss: 1.501080870628357\n",
      "Iteration: 5466, Loss: 1.5010762214660645\n",
      "Iteration: 5467, Loss: 1.5010716915130615\n",
      "Iteration: 5468, Loss: 1.501067042350769\n",
      "Iteration: 5469, Loss: 1.5010625123977661\n",
      "Iteration: 5470, Loss: 1.5010578632354736\n",
      "Iteration: 5471, Loss: 1.5010533332824707\n",
      "Iteration: 5472, Loss: 1.5010486841201782\n",
      "Iteration: 5473, Loss: 1.5010441541671753\n",
      "Iteration: 5474, Loss: 1.5010395050048828\n",
      "Iteration: 5475, Loss: 1.5010349750518799\n",
      "Iteration: 5476, Loss: 1.5010303258895874\n",
      "Iteration: 5477, Loss: 1.5010257959365845\n",
      "Iteration: 5478, Loss: 1.501021146774292\n",
      "Iteration: 5479, Loss: 1.501016616821289\n",
      "Iteration: 5480, Loss: 1.5010119676589966\n",
      "Iteration: 5481, Loss: 1.5010074377059937\n",
      "Iteration: 5482, Loss: 1.5010029077529907\n",
      "Iteration: 5483, Loss: 1.5009982585906982\n",
      "Iteration: 5484, Loss: 1.5009937286376953\n",
      "Iteration: 5485, Loss: 1.5009891986846924\n",
      "Iteration: 5486, Loss: 1.5009846687316895\n",
      "Iteration: 5487, Loss: 1.500980019569397\n",
      "Iteration: 5488, Loss: 1.500975489616394\n",
      "Iteration: 5489, Loss: 1.5009709596633911\n",
      "Iteration: 5490, Loss: 1.5009664297103882\n",
      "Iteration: 5491, Loss: 1.5009617805480957\n",
      "Iteration: 5492, Loss: 1.5009572505950928\n",
      "Iteration: 5493, Loss: 1.5009527206420898\n",
      "Iteration: 5494, Loss: 1.500948190689087\n",
      "Iteration: 5495, Loss: 1.500943660736084\n",
      "Iteration: 5496, Loss: 1.500939130783081\n",
      "Iteration: 5497, Loss: 1.5009346008300781\n",
      "Iteration: 5498, Loss: 1.5009300708770752\n",
      "Iteration: 5499, Loss: 1.5009255409240723\n",
      "Iteration: 5500, Loss: 1.5009210109710693\n",
      "Iteration: 5501, Loss: 1.5009163618087769\n",
      "Iteration: 5502, Loss: 1.500911831855774\n",
      "Iteration: 5503, Loss: 1.5009074211120605\n",
      "Iteration: 5504, Loss: 1.5009028911590576\n",
      "Iteration: 5505, Loss: 1.5008983612060547\n",
      "Iteration: 5506, Loss: 1.5008938312530518\n",
      "Iteration: 5507, Loss: 1.5008893013000488\n",
      "Iteration: 5508, Loss: 1.500884771347046\n",
      "Iteration: 5509, Loss: 1.500880241394043\n",
      "Iteration: 5510, Loss: 1.50087571144104\n",
      "Iteration: 5511, Loss: 1.500871181488037\n",
      "Iteration: 5512, Loss: 1.5008666515350342\n",
      "Iteration: 5513, Loss: 1.5008622407913208\n",
      "Iteration: 5514, Loss: 1.5008577108383179\n",
      "Iteration: 5515, Loss: 1.500853180885315\n",
      "Iteration: 5516, Loss: 1.500848650932312\n",
      "Iteration: 5517, Loss: 1.5008442401885986\n",
      "Iteration: 5518, Loss: 1.5008397102355957\n",
      "Iteration: 5519, Loss: 1.5008351802825928\n",
      "Iteration: 5520, Loss: 1.5008306503295898\n",
      "Iteration: 5521, Loss: 1.5008262395858765\n",
      "Iteration: 5522, Loss: 1.5008217096328735\n",
      "Iteration: 5523, Loss: 1.5008172988891602\n",
      "Iteration: 5524, Loss: 1.5008127689361572\n",
      "Iteration: 5525, Loss: 1.5008082389831543\n",
      "Iteration: 5526, Loss: 1.500803828239441\n",
      "Iteration: 5527, Loss: 1.500799298286438\n",
      "Iteration: 5528, Loss: 1.5007948875427246\n",
      "Iteration: 5529, Loss: 1.5007903575897217\n",
      "Iteration: 5530, Loss: 1.5007859468460083\n",
      "Iteration: 5531, Loss: 1.5007814168930054\n",
      "Iteration: 5532, Loss: 1.500777006149292\n",
      "Iteration: 5533, Loss: 1.500772476196289\n",
      "Iteration: 5534, Loss: 1.5007680654525757\n",
      "Iteration: 5535, Loss: 1.5007635354995728\n",
      "Iteration: 5536, Loss: 1.5007591247558594\n",
      "Iteration: 5537, Loss: 1.500754714012146\n",
      "Iteration: 5538, Loss: 1.500750184059143\n",
      "Iteration: 5539, Loss: 1.5007457733154297\n",
      "Iteration: 5540, Loss: 1.5007412433624268\n",
      "Iteration: 5541, Loss: 1.5007368326187134\n",
      "Iteration: 5542, Loss: 1.500732421875\n",
      "Iteration: 5543, Loss: 1.5007280111312866\n",
      "Iteration: 5544, Loss: 1.5007234811782837\n",
      "Iteration: 5545, Loss: 1.5007190704345703\n",
      "Iteration: 5546, Loss: 1.500714659690857\n",
      "Iteration: 5547, Loss: 1.5007102489471436\n",
      "Iteration: 5548, Loss: 1.5007057189941406\n",
      "Iteration: 5549, Loss: 1.5007013082504272\n",
      "Iteration: 5550, Loss: 1.5006968975067139\n",
      "Iteration: 5551, Loss: 1.5006924867630005\n",
      "Iteration: 5552, Loss: 1.500688076019287\n",
      "Iteration: 5553, Loss: 1.5006836652755737\n",
      "Iteration: 5554, Loss: 1.5006792545318604\n",
      "Iteration: 5555, Loss: 1.500674843788147\n",
      "Iteration: 5556, Loss: 1.5006704330444336\n",
      "Iteration: 5557, Loss: 1.5006659030914307\n",
      "Iteration: 5558, Loss: 1.5006614923477173\n",
      "Iteration: 5559, Loss: 1.500657081604004\n",
      "Iteration: 5560, Loss: 1.5006526708602905\n",
      "Iteration: 5561, Loss: 1.5006483793258667\n",
      "Iteration: 5562, Loss: 1.5006439685821533\n",
      "Iteration: 5563, Loss: 1.50063955783844\n",
      "Iteration: 5564, Loss: 1.5006351470947266\n",
      "Iteration: 5565, Loss: 1.5006307363510132\n",
      "Iteration: 5566, Loss: 1.5006263256072998\n",
      "Iteration: 5567, Loss: 1.5006219148635864\n",
      "Iteration: 5568, Loss: 1.500617504119873\n",
      "Iteration: 5569, Loss: 1.5006130933761597\n",
      "Iteration: 5570, Loss: 1.5006088018417358\n",
      "Iteration: 5571, Loss: 1.5006043910980225\n",
      "Iteration: 5572, Loss: 1.500599980354309\n",
      "Iteration: 5573, Loss: 1.5005955696105957\n",
      "Iteration: 5574, Loss: 1.5005912780761719\n",
      "Iteration: 5575, Loss: 1.5005868673324585\n",
      "Iteration: 5576, Loss: 1.5005824565887451\n",
      "Iteration: 5577, Loss: 1.5005780458450317\n",
      "Iteration: 5578, Loss: 1.500573754310608\n",
      "Iteration: 5579, Loss: 1.5005693435668945\n",
      "Iteration: 5580, Loss: 1.5005650520324707\n",
      "Iteration: 5581, Loss: 1.5005606412887573\n",
      "Iteration: 5582, Loss: 1.500556230545044\n",
      "Iteration: 5583, Loss: 1.5005519390106201\n",
      "Iteration: 5584, Loss: 1.5005475282669067\n",
      "Iteration: 5585, Loss: 1.500543236732483\n",
      "Iteration: 5586, Loss: 1.5005388259887695\n",
      "Iteration: 5587, Loss: 1.5005345344543457\n",
      "Iteration: 5588, Loss: 1.5005301237106323\n",
      "Iteration: 5589, Loss: 1.5005258321762085\n",
      "Iteration: 5590, Loss: 1.5005214214324951\n",
      "Iteration: 5591, Loss: 1.5005171298980713\n",
      "Iteration: 5592, Loss: 1.500512719154358\n",
      "Iteration: 5593, Loss: 1.500508427619934\n",
      "Iteration: 5594, Loss: 1.5005041360855103\n",
      "Iteration: 5595, Loss: 1.5004997253417969\n",
      "Iteration: 5596, Loss: 1.500495433807373\n",
      "Iteration: 5597, Loss: 1.5004910230636597\n",
      "Iteration: 5598, Loss: 1.5004867315292358\n",
      "Iteration: 5599, Loss: 1.500482439994812\n",
      "Iteration: 5600, Loss: 1.5004781484603882\n",
      "Iteration: 5601, Loss: 1.5004737377166748\n",
      "Iteration: 5602, Loss: 1.500469446182251\n",
      "Iteration: 5603, Loss: 1.5004651546478271\n",
      "Iteration: 5604, Loss: 1.5004608631134033\n",
      "Iteration: 5605, Loss: 1.50045645236969\n",
      "Iteration: 5606, Loss: 1.5004521608352661\n",
      "Iteration: 5607, Loss: 1.5004478693008423\n",
      "Iteration: 5608, Loss: 1.5004435777664185\n",
      "Iteration: 5609, Loss: 1.5004392862319946\n",
      "Iteration: 5610, Loss: 1.5004349946975708\n",
      "Iteration: 5611, Loss: 1.500430703163147\n",
      "Iteration: 5612, Loss: 1.5004264116287231\n",
      "Iteration: 5613, Loss: 1.5004221200942993\n",
      "Iteration: 5614, Loss: 1.5004178285598755\n",
      "Iteration: 5615, Loss: 1.5004135370254517\n",
      "Iteration: 5616, Loss: 1.5004092454910278\n",
      "Iteration: 5617, Loss: 1.500404953956604\n",
      "Iteration: 5618, Loss: 1.5004006624221802\n",
      "Iteration: 5619, Loss: 1.5003963708877563\n",
      "Iteration: 5620, Loss: 1.5003920793533325\n",
      "Iteration: 5621, Loss: 1.5003877878189087\n",
      "Iteration: 5622, Loss: 1.5003834962844849\n",
      "Iteration: 5623, Loss: 1.500379204750061\n",
      "Iteration: 5624, Loss: 1.5003749132156372\n",
      "Iteration: 5625, Loss: 1.5003706216812134\n",
      "Iteration: 5626, Loss: 1.500366449356079\n",
      "Iteration: 5627, Loss: 1.5003621578216553\n",
      "Iteration: 5628, Loss: 1.5003578662872314\n",
      "Iteration: 5629, Loss: 1.5003535747528076\n",
      "Iteration: 5630, Loss: 1.5003494024276733\n",
      "Iteration: 5631, Loss: 1.5003451108932495\n",
      "Iteration: 5632, Loss: 1.5003408193588257\n",
      "Iteration: 5633, Loss: 1.5003365278244019\n",
      "Iteration: 5634, Loss: 1.5003323554992676\n",
      "Iteration: 5635, Loss: 1.5003280639648438\n",
      "Iteration: 5636, Loss: 1.50032377243042\n",
      "Iteration: 5637, Loss: 1.5003196001052856\n",
      "Iteration: 5638, Loss: 1.5003153085708618\n",
      "Iteration: 5639, Loss: 1.5003111362457275\n",
      "Iteration: 5640, Loss: 1.5003068447113037\n",
      "Iteration: 5641, Loss: 1.5003025531768799\n",
      "Iteration: 5642, Loss: 1.5002983808517456\n",
      "Iteration: 5643, Loss: 1.5002940893173218\n",
      "Iteration: 5644, Loss: 1.5002899169921875\n",
      "Iteration: 5645, Loss: 1.5002856254577637\n",
      "Iteration: 5646, Loss: 1.5002814531326294\n",
      "Iteration: 5647, Loss: 1.5002772808074951\n",
      "Iteration: 5648, Loss: 1.5002729892730713\n",
      "Iteration: 5649, Loss: 1.500268816947937\n",
      "Iteration: 5650, Loss: 1.5002645254135132\n",
      "Iteration: 5651, Loss: 1.500260353088379\n",
      "Iteration: 5652, Loss: 1.5002561807632446\n",
      "Iteration: 5653, Loss: 1.5002518892288208\n",
      "Iteration: 5654, Loss: 1.5002477169036865\n",
      "Iteration: 5655, Loss: 1.5002435445785522\n",
      "Iteration: 5656, Loss: 1.5002392530441284\n",
      "Iteration: 5657, Loss: 1.5002350807189941\n",
      "Iteration: 5658, Loss: 1.5002309083938599\n",
      "Iteration: 5659, Loss: 1.5002267360687256\n",
      "Iteration: 5660, Loss: 1.5002224445343018\n",
      "Iteration: 5661, Loss: 1.5002182722091675\n",
      "Iteration: 5662, Loss: 1.5002140998840332\n",
      "Iteration: 5663, Loss: 1.500209927558899\n",
      "Iteration: 5664, Loss: 1.5002057552337646\n",
      "Iteration: 5665, Loss: 1.5002015829086304\n",
      "Iteration: 5666, Loss: 1.500197410583496\n",
      "Iteration: 5667, Loss: 1.5001932382583618\n",
      "Iteration: 5668, Loss: 1.500188946723938\n",
      "Iteration: 5669, Loss: 1.5001847743988037\n",
      "Iteration: 5670, Loss: 1.5001806020736694\n",
      "Iteration: 5671, Loss: 1.5001764297485352\n",
      "Iteration: 5672, Loss: 1.5001722574234009\n",
      "Iteration: 5673, Loss: 1.5001680850982666\n",
      "Iteration: 5674, Loss: 1.5001639127731323\n",
      "Iteration: 5675, Loss: 1.500159740447998\n",
      "Iteration: 5676, Loss: 1.5001556873321533\n",
      "Iteration: 5677, Loss: 1.500151515007019\n",
      "Iteration: 5678, Loss: 1.5001473426818848\n",
      "Iteration: 5679, Loss: 1.5001431703567505\n",
      "Iteration: 5680, Loss: 1.5001389980316162\n",
      "Iteration: 5681, Loss: 1.500134825706482\n",
      "Iteration: 5682, Loss: 1.5001306533813477\n",
      "Iteration: 5683, Loss: 1.500126600265503\n",
      "Iteration: 5684, Loss: 1.5001224279403687\n",
      "Iteration: 5685, Loss: 1.5001182556152344\n",
      "Iteration: 5686, Loss: 1.5001140832901\n",
      "Iteration: 5687, Loss: 1.5001100301742554\n",
      "Iteration: 5688, Loss: 1.500105857849121\n",
      "Iteration: 5689, Loss: 1.5001016855239868\n",
      "Iteration: 5690, Loss: 1.500097632408142\n",
      "Iteration: 5691, Loss: 1.5000934600830078\n",
      "Iteration: 5692, Loss: 1.5000892877578735\n",
      "Iteration: 5693, Loss: 1.5000852346420288\n",
      "Iteration: 5694, Loss: 1.5000810623168945\n",
      "Iteration: 5695, Loss: 1.5000768899917603\n",
      "Iteration: 5696, Loss: 1.5000728368759155\n",
      "Iteration: 5697, Loss: 1.5000686645507812\n",
      "Iteration: 5698, Loss: 1.5000646114349365\n",
      "Iteration: 5699, Loss: 1.5000604391098022\n",
      "Iteration: 5700, Loss: 1.5000563859939575\n",
      "Iteration: 5701, Loss: 1.5000522136688232\n",
      "Iteration: 5702, Loss: 1.5000481605529785\n",
      "Iteration: 5703, Loss: 1.5000439882278442\n",
      "Iteration: 5704, Loss: 1.5000399351119995\n",
      "Iteration: 5705, Loss: 1.5000358819961548\n",
      "Iteration: 5706, Loss: 1.5000317096710205\n",
      "Iteration: 5707, Loss: 1.5000276565551758\n",
      "Iteration: 5708, Loss: 1.500023603439331\n",
      "Iteration: 5709, Loss: 1.5000194311141968\n",
      "Iteration: 5710, Loss: 1.500015377998352\n",
      "Iteration: 5711, Loss: 1.5000113248825073\n",
      "Iteration: 5712, Loss: 1.500007152557373\n",
      "Iteration: 5713, Loss: 1.5000030994415283\n",
      "Iteration: 5714, Loss: 1.4999990463256836\n",
      "Iteration: 5715, Loss: 1.4999949932098389\n",
      "Iteration: 5716, Loss: 1.4999908208847046\n",
      "Iteration: 5717, Loss: 1.4999867677688599\n",
      "Iteration: 5718, Loss: 1.4999827146530151\n",
      "Iteration: 5719, Loss: 1.4999786615371704\n",
      "Iteration: 5720, Loss: 1.4999746084213257\n",
      "Iteration: 5721, Loss: 1.499970555305481\n",
      "Iteration: 5722, Loss: 1.4999665021896362\n",
      "Iteration: 5723, Loss: 1.4999624490737915\n",
      "Iteration: 5724, Loss: 1.4999582767486572\n",
      "Iteration: 5725, Loss: 1.4999542236328125\n",
      "Iteration: 5726, Loss: 1.4999501705169678\n",
      "Iteration: 5727, Loss: 1.499946117401123\n",
      "Iteration: 5728, Loss: 1.4999420642852783\n",
      "Iteration: 5729, Loss: 1.4999380111694336\n",
      "Iteration: 5730, Loss: 1.499934196472168\n",
      "Iteration: 5731, Loss: 1.4999301433563232\n",
      "Iteration: 5732, Loss: 1.4999260902404785\n",
      "Iteration: 5733, Loss: 1.4999220371246338\n",
      "Iteration: 5734, Loss: 1.499917984008789\n",
      "Iteration: 5735, Loss: 1.4999139308929443\n",
      "Iteration: 5736, Loss: 1.4999098777770996\n",
      "Iteration: 5737, Loss: 1.4999058246612549\n",
      "Iteration: 5738, Loss: 1.4999018907546997\n",
      "Iteration: 5739, Loss: 1.499897837638855\n",
      "Iteration: 5740, Loss: 1.4998937845230103\n",
      "Iteration: 5741, Loss: 1.4998897314071655\n",
      "Iteration: 5742, Loss: 1.4998857975006104\n",
      "Iteration: 5743, Loss: 1.4998817443847656\n",
      "Iteration: 5744, Loss: 1.499877691268921\n",
      "Iteration: 5745, Loss: 1.4998737573623657\n",
      "Iteration: 5746, Loss: 1.499869704246521\n",
      "Iteration: 5747, Loss: 1.4998656511306763\n",
      "Iteration: 5748, Loss: 1.4998618364334106\n",
      "Iteration: 5749, Loss: 1.499857783317566\n",
      "Iteration: 5750, Loss: 1.4998538494110107\n",
      "Iteration: 5751, Loss: 1.499849796295166\n",
      "Iteration: 5752, Loss: 1.4998457431793213\n",
      "Iteration: 5753, Loss: 1.4998418092727661\n",
      "Iteration: 5754, Loss: 1.4998377561569214\n",
      "Iteration: 5755, Loss: 1.4998338222503662\n",
      "Iteration: 5756, Loss: 1.499829888343811\n",
      "Iteration: 5757, Loss: 1.4998258352279663\n",
      "Iteration: 5758, Loss: 1.4998219013214111\n",
      "Iteration: 5759, Loss: 1.4998178482055664\n",
      "Iteration: 5760, Loss: 1.4998139142990112\n",
      "Iteration: 5761, Loss: 1.499809980392456\n",
      "Iteration: 5762, Loss: 1.4998059272766113\n",
      "Iteration: 5763, Loss: 1.4998019933700562\n",
      "Iteration: 5764, Loss: 1.499798059463501\n",
      "Iteration: 5765, Loss: 1.4997940063476562\n",
      "Iteration: 5766, Loss: 1.499790072441101\n",
      "Iteration: 5767, Loss: 1.499786138534546\n",
      "Iteration: 5768, Loss: 1.4997822046279907\n",
      "Iteration: 5769, Loss: 1.499778151512146\n",
      "Iteration: 5770, Loss: 1.4997742176055908\n",
      "Iteration: 5771, Loss: 1.4997702836990356\n",
      "Iteration: 5772, Loss: 1.4997663497924805\n",
      "Iteration: 5773, Loss: 1.4997624158859253\n",
      "Iteration: 5774, Loss: 1.4997584819793701\n",
      "Iteration: 5775, Loss: 1.499754548072815\n",
      "Iteration: 5776, Loss: 1.4997506141662598\n",
      "Iteration: 5777, Loss: 1.499746561050415\n",
      "Iteration: 5778, Loss: 1.4997426271438599\n",
      "Iteration: 5779, Loss: 1.4997386932373047\n",
      "Iteration: 5780, Loss: 1.4997347593307495\n",
      "Iteration: 5781, Loss: 1.4997308254241943\n",
      "Iteration: 5782, Loss: 1.4997270107269287\n",
      "Iteration: 5783, Loss: 1.4997230768203735\n",
      "Iteration: 5784, Loss: 1.4997191429138184\n",
      "Iteration: 5785, Loss: 1.4997152090072632\n",
      "Iteration: 5786, Loss: 1.499711275100708\n",
      "Iteration: 5787, Loss: 1.4997073411941528\n",
      "Iteration: 5788, Loss: 1.4997034072875977\n",
      "Iteration: 5789, Loss: 1.4996994733810425\n",
      "Iteration: 5790, Loss: 1.4996956586837769\n",
      "Iteration: 5791, Loss: 1.4996917247772217\n",
      "Iteration: 5792, Loss: 1.4996877908706665\n",
      "Iteration: 5793, Loss: 1.4996838569641113\n",
      "Iteration: 5794, Loss: 1.4996800422668457\n",
      "Iteration: 5795, Loss: 1.4996761083602905\n",
      "Iteration: 5796, Loss: 1.4996721744537354\n",
      "Iteration: 5797, Loss: 1.4996683597564697\n",
      "Iteration: 5798, Loss: 1.4996644258499146\n",
      "Iteration: 5799, Loss: 1.4996604919433594\n",
      "Iteration: 5800, Loss: 1.4996566772460938\n",
      "Iteration: 5801, Loss: 1.4996527433395386\n",
      "Iteration: 5802, Loss: 1.499648928642273\n",
      "Iteration: 5803, Loss: 1.4996449947357178\n",
      "Iteration: 5804, Loss: 1.4996411800384521\n",
      "Iteration: 5805, Loss: 1.4996373653411865\n",
      "Iteration: 5806, Loss: 1.499633550643921\n",
      "Iteration: 5807, Loss: 1.4996296167373657\n",
      "Iteration: 5808, Loss: 1.4996258020401\n",
      "Iteration: 5809, Loss: 1.4996219873428345\n",
      "Iteration: 5810, Loss: 1.4996180534362793\n",
      "Iteration: 5811, Loss: 1.4996142387390137\n",
      "Iteration: 5812, Loss: 1.499610424041748\n",
      "Iteration: 5813, Loss: 1.4996064901351929\n",
      "Iteration: 5814, Loss: 1.4996026754379272\n",
      "Iteration: 5815, Loss: 1.4995988607406616\n",
      "Iteration: 5816, Loss: 1.499595046043396\n",
      "Iteration: 5817, Loss: 1.4995912313461304\n",
      "Iteration: 5818, Loss: 1.4995872974395752\n",
      "Iteration: 5819, Loss: 1.4995834827423096\n",
      "Iteration: 5820, Loss: 1.499579668045044\n",
      "Iteration: 5821, Loss: 1.4995758533477783\n",
      "Iteration: 5822, Loss: 1.4995720386505127\n",
      "Iteration: 5823, Loss: 1.499568223953247\n",
      "Iteration: 5824, Loss: 1.4995644092559814\n",
      "Iteration: 5825, Loss: 1.4995605945587158\n",
      "Iteration: 5826, Loss: 1.4995567798614502\n",
      "Iteration: 5827, Loss: 1.4995529651641846\n",
      "Iteration: 5828, Loss: 1.499549150466919\n",
      "Iteration: 5829, Loss: 1.4995453357696533\n",
      "Iteration: 5830, Loss: 1.4995415210723877\n",
      "Iteration: 5831, Loss: 1.499537706375122\n",
      "Iteration: 5832, Loss: 1.4995338916778564\n",
      "Iteration: 5833, Loss: 1.4995300769805908\n",
      "Iteration: 5834, Loss: 1.4995262622833252\n",
      "Iteration: 5835, Loss: 1.4995225667953491\n",
      "Iteration: 5836, Loss: 1.4995187520980835\n",
      "Iteration: 5837, Loss: 1.4995149374008179\n",
      "Iteration: 5838, Loss: 1.4995110034942627\n",
      "Iteration: 5839, Loss: 1.499507188796997\n",
      "Iteration: 5840, Loss: 1.499503493309021\n",
      "Iteration: 5841, Loss: 1.4994996786117554\n",
      "Iteration: 5842, Loss: 1.4994958639144897\n",
      "Iteration: 5843, Loss: 1.4994920492172241\n",
      "Iteration: 5844, Loss: 1.499488353729248\n",
      "Iteration: 5845, Loss: 1.4994845390319824\n",
      "Iteration: 5846, Loss: 1.4994807243347168\n",
      "Iteration: 5847, Loss: 1.4994769096374512\n",
      "Iteration: 5848, Loss: 1.499473214149475\n",
      "Iteration: 5849, Loss: 1.4994693994522095\n",
      "Iteration: 5850, Loss: 1.4994657039642334\n",
      "Iteration: 5851, Loss: 1.4994620084762573\n",
      "Iteration: 5852, Loss: 1.4994581937789917\n",
      "Iteration: 5853, Loss: 1.4994544982910156\n",
      "Iteration: 5854, Loss: 1.49945068359375\n",
      "Iteration: 5855, Loss: 1.499446988105774\n",
      "Iteration: 5856, Loss: 1.4994431734085083\n",
      "Iteration: 5857, Loss: 1.4994394779205322\n",
      "Iteration: 5858, Loss: 1.4994356632232666\n",
      "Iteration: 5859, Loss: 1.4994319677352905\n",
      "Iteration: 5860, Loss: 1.499428153038025\n",
      "Iteration: 5861, Loss: 1.4994244575500488\n",
      "Iteration: 5862, Loss: 1.4994207620620728\n",
      "Iteration: 5863, Loss: 1.4994169473648071\n",
      "Iteration: 5864, Loss: 1.499413251876831\n",
      "Iteration: 5865, Loss: 1.4994094371795654\n",
      "Iteration: 5866, Loss: 1.4994057416915894\n",
      "Iteration: 5867, Loss: 1.4994020462036133\n",
      "Iteration: 5868, Loss: 1.4993983507156372\n",
      "Iteration: 5869, Loss: 1.4993945360183716\n",
      "Iteration: 5870, Loss: 1.4993908405303955\n",
      "Iteration: 5871, Loss: 1.4993871450424194\n",
      "Iteration: 5872, Loss: 1.4993834495544434\n",
      "Iteration: 5873, Loss: 1.4993796348571777\n",
      "Iteration: 5874, Loss: 1.4993759393692017\n",
      "Iteration: 5875, Loss: 1.4993722438812256\n",
      "Iteration: 5876, Loss: 1.4993685483932495\n",
      "Iteration: 5877, Loss: 1.4993648529052734\n",
      "Iteration: 5878, Loss: 1.4993611574172974\n",
      "Iteration: 5879, Loss: 1.4993574619293213\n",
      "Iteration: 5880, Loss: 1.4993537664413452\n",
      "Iteration: 5881, Loss: 1.4993500709533691\n",
      "Iteration: 5882, Loss: 1.4993462562561035\n",
      "Iteration: 5883, Loss: 1.4993425607681274\n",
      "Iteration: 5884, Loss: 1.4993388652801514\n",
      "Iteration: 5885, Loss: 1.4993351697921753\n",
      "Iteration: 5886, Loss: 1.4993314743041992\n",
      "Iteration: 5887, Loss: 1.4993277788162231\n",
      "Iteration: 5888, Loss: 1.499324083328247\n",
      "Iteration: 5889, Loss: 1.499320387840271\n",
      "Iteration: 5890, Loss: 1.499316692352295\n",
      "Iteration: 5891, Loss: 1.4993129968643188\n",
      "Iteration: 5892, Loss: 1.4993093013763428\n",
      "Iteration: 5893, Loss: 1.4993057250976562\n",
      "Iteration: 5894, Loss: 1.4993020296096802\n",
      "Iteration: 5895, Loss: 1.499298334121704\n",
      "Iteration: 5896, Loss: 1.499294638633728\n",
      "Iteration: 5897, Loss: 1.499290943145752\n",
      "Iteration: 5898, Loss: 1.4992873668670654\n",
      "Iteration: 5899, Loss: 1.4992836713790894\n",
      "Iteration: 5900, Loss: 1.4992799758911133\n",
      "Iteration: 5901, Loss: 1.4992763996124268\n",
      "Iteration: 5902, Loss: 1.4992727041244507\n",
      "Iteration: 5903, Loss: 1.4992690086364746\n",
      "Iteration: 5904, Loss: 1.4992653131484985\n",
      "Iteration: 5905, Loss: 1.499261736869812\n",
      "Iteration: 5906, Loss: 1.499258041381836\n",
      "Iteration: 5907, Loss: 1.4992544651031494\n",
      "Iteration: 5908, Loss: 1.4992507696151733\n",
      "Iteration: 5909, Loss: 1.4992470741271973\n",
      "Iteration: 5910, Loss: 1.4992434978485107\n",
      "Iteration: 5911, Loss: 1.4992398023605347\n",
      "Iteration: 5912, Loss: 1.4992362260818481\n",
      "Iteration: 5913, Loss: 1.499232530593872\n",
      "Iteration: 5914, Loss: 1.4992289543151855\n",
      "Iteration: 5915, Loss: 1.4992252588272095\n",
      "Iteration: 5916, Loss: 1.4992215633392334\n",
      "Iteration: 5917, Loss: 1.4992177486419678\n",
      "Iteration: 5918, Loss: 1.4992139339447021\n",
      "Iteration: 5919, Loss: 1.499210238456726\n",
      "Iteration: 5920, Loss: 1.49920654296875\n",
      "Iteration: 5921, Loss: 1.4992029666900635\n",
      "Iteration: 5922, Loss: 1.4991992712020874\n",
      "Iteration: 5923, Loss: 1.4991955757141113\n",
      "Iteration: 5924, Loss: 1.4991918802261353\n",
      "Iteration: 5925, Loss: 1.49918794631958\n",
      "Iteration: 5926, Loss: 1.499184250831604\n",
      "Iteration: 5927, Loss: 1.4991800785064697\n",
      "Iteration: 5928, Loss: 1.4991765022277832\n",
      "Iteration: 5929, Loss: 1.4991722106933594\n",
      "Iteration: 5930, Loss: 1.4991676807403564\n",
      "Iteration: 5931, Loss: 1.4991629123687744\n",
      "Iteration: 5932, Loss: 1.4991576671600342\n",
      "Iteration: 5933, Loss: 1.4991518259048462\n",
      "Iteration: 5934, Loss: 1.4991422891616821\n",
      "Iteration: 5935, Loss: 1.4991254806518555\n",
      "Iteration: 5936, Loss: 1.4990665912628174\n",
      "Iteration: 5937, Loss: 1.4983986616134644\n",
      "Iteration: 5938, Loss: 1.506267786026001\n",
      "Iteration: 5939, Loss: 1.4992426633834839\n",
      "Iteration: 5940, Loss: 1.49946129322052\n",
      "Iteration: 5941, Loss: 1.4997222423553467\n",
      "Iteration: 5942, Loss: 1.49998939037323\n",
      "Iteration: 5943, Loss: 1.5002466440200806\n",
      "Iteration: 5944, Loss: 1.5004874467849731\n",
      "Iteration: 5945, Loss: 1.5007094144821167\n",
      "Iteration: 5946, Loss: 1.5009123086929321\n",
      "Iteration: 5947, Loss: 1.5010967254638672\n",
      "Iteration: 5948, Loss: 1.5012637376785278\n",
      "Iteration: 5949, Loss: 1.5014145374298096\n",
      "Iteration: 5950, Loss: 1.501550555229187\n",
      "Iteration: 5951, Loss: 1.5016728639602661\n",
      "Iteration: 5952, Loss: 1.5017826557159424\n",
      "Iteration: 5953, Loss: 1.5018812417984009\n",
      "Iteration: 5954, Loss: 1.501969575881958\n",
      "Iteration: 5955, Loss: 1.5020484924316406\n",
      "Iteration: 5956, Loss: 1.5021191835403442\n",
      "Iteration: 5957, Loss: 1.502182126045227\n",
      "Iteration: 5958, Loss: 1.5022382736206055\n",
      "Iteration: 5959, Loss: 1.5022882223129272\n",
      "Iteration: 5960, Loss: 1.5023326873779297\n",
      "Iteration: 5961, Loss: 1.5023720264434814\n",
      "Iteration: 5962, Loss: 1.5024068355560303\n",
      "Iteration: 5963, Loss: 1.5024374723434448\n",
      "Iteration: 5964, Loss: 1.5024645328521729\n",
      "Iteration: 5965, Loss: 1.5024882555007935\n",
      "Iteration: 5966, Loss: 1.5025088787078857\n",
      "Iteration: 5967, Loss: 1.5025269985198975\n",
      "Iteration: 5968, Loss: 1.502542495727539\n",
      "Iteration: 5969, Loss: 1.5025559663772583\n",
      "Iteration: 5970, Loss: 1.5025674104690552\n",
      "Iteration: 5971, Loss: 1.5025770664215088\n",
      "Iteration: 5972, Loss: 1.5025851726531982\n",
      "Iteration: 5973, Loss: 1.502591848373413\n",
      "Iteration: 5974, Loss: 1.5025972127914429\n",
      "Iteration: 5975, Loss: 1.5026013851165771\n",
      "Iteration: 5976, Loss: 1.502604603767395\n",
      "Iteration: 5977, Loss: 1.5026068687438965\n",
      "Iteration: 5978, Loss: 1.5026081800460815\n",
      "Iteration: 5979, Loss: 1.5026088953018188\n",
      "Iteration: 5980, Loss: 1.5026088953018188\n",
      "Iteration: 5981, Loss: 1.5026081800460815\n",
      "Iteration: 5982, Loss: 1.502606987953186\n",
      "Iteration: 5983, Loss: 1.5026053190231323\n",
      "Iteration: 5984, Loss: 1.5026031732559204\n",
      "Iteration: 5985, Loss: 1.5026006698608398\n",
      "Iteration: 5986, Loss: 1.5025978088378906\n",
      "Iteration: 5987, Loss: 1.5025945901870728\n",
      "Iteration: 5988, Loss: 1.5025911331176758\n",
      "Iteration: 5989, Loss: 1.5025874376296997\n",
      "Iteration: 5990, Loss: 1.5025835037231445\n",
      "Iteration: 5991, Loss: 1.5025793313980103\n",
      "Iteration: 5992, Loss: 1.5025749206542969\n",
      "Iteration: 5993, Loss: 1.502570390701294\n",
      "Iteration: 5994, Loss: 1.5025657415390015\n",
      "Iteration: 5995, Loss: 1.5025609731674194\n",
      "Iteration: 5996, Loss: 1.5025560855865479\n",
      "Iteration: 5997, Loss: 1.5025510787963867\n",
      "Iteration: 5998, Loss: 1.502545952796936\n",
      "Iteration: 5999, Loss: 1.5025408267974854\n",
      "Iteration: 6000, Loss: 1.5025355815887451\n",
      "Iteration: 6001, Loss: 1.5025302171707153\n",
      "Iteration: 6002, Loss: 1.5025248527526855\n",
      "Iteration: 6003, Loss: 1.5025193691253662\n",
      "Iteration: 6004, Loss: 1.5025138854980469\n",
      "Iteration: 6005, Loss: 1.5025084018707275\n",
      "Iteration: 6006, Loss: 1.5025029182434082\n",
      "Iteration: 6007, Loss: 1.5024973154067993\n",
      "Iteration: 6008, Loss: 1.5024917125701904\n",
      "Iteration: 6009, Loss: 1.5024861097335815\n",
      "Iteration: 6010, Loss: 1.502480387687683\n",
      "Iteration: 6011, Loss: 1.5024747848510742\n",
      "Iteration: 6012, Loss: 1.5024690628051758\n",
      "Iteration: 6013, Loss: 1.502463459968567\n",
      "Iteration: 6014, Loss: 1.5024577379226685\n",
      "Iteration: 6015, Loss: 1.50245201587677\n",
      "Iteration: 6016, Loss: 1.5024462938308716\n",
      "Iteration: 6017, Loss: 1.5024405717849731\n",
      "Iteration: 6018, Loss: 1.5024348497390747\n",
      "Iteration: 6019, Loss: 1.5024291276931763\n",
      "Iteration: 6020, Loss: 1.5024234056472778\n",
      "Iteration: 6021, Loss: 1.5024176836013794\n",
      "Iteration: 6022, Loss: 1.5024120807647705\n",
      "Iteration: 6023, Loss: 1.502406358718872\n",
      "Iteration: 6024, Loss: 1.5024006366729736\n",
      "Iteration: 6025, Loss: 1.5023949146270752\n",
      "Iteration: 6026, Loss: 1.5023891925811768\n",
      "Iteration: 6027, Loss: 1.5023834705352783\n",
      "Iteration: 6028, Loss: 1.5023777484893799\n",
      "Iteration: 6029, Loss: 1.5023720264434814\n",
      "Iteration: 6030, Loss: 1.5023664236068726\n",
      "Iteration: 6031, Loss: 1.5023607015609741\n",
      "Iteration: 6032, Loss: 1.5023549795150757\n",
      "Iteration: 6033, Loss: 1.5023493766784668\n",
      "Iteration: 6034, Loss: 1.5023436546325684\n",
      "Iteration: 6035, Loss: 1.50233793258667\n",
      "Iteration: 6036, Loss: 1.502332329750061\n",
      "Iteration: 6037, Loss: 1.5023267269134521\n",
      "Iteration: 6038, Loss: 1.5023210048675537\n",
      "Iteration: 6039, Loss: 1.5023154020309448\n",
      "Iteration: 6040, Loss: 1.502309799194336\n",
      "Iteration: 6041, Loss: 1.5023040771484375\n",
      "Iteration: 6042, Loss: 1.5022984743118286\n",
      "Iteration: 6043, Loss: 1.5022928714752197\n",
      "Iteration: 6044, Loss: 1.5022872686386108\n",
      "Iteration: 6045, Loss: 1.502281665802002\n",
      "Iteration: 6046, Loss: 1.502276062965393\n",
      "Iteration: 6047, Loss: 1.5022704601287842\n",
      "Iteration: 6048, Loss: 1.5022648572921753\n",
      "Iteration: 6049, Loss: 1.5022592544555664\n",
      "Iteration: 6050, Loss: 1.5022536516189575\n",
      "Iteration: 6051, Loss: 1.5022481679916382\n",
      "Iteration: 6052, Loss: 1.5022425651550293\n",
      "Iteration: 6053, Loss: 1.5022369623184204\n",
      "Iteration: 6054, Loss: 1.502231478691101\n",
      "Iteration: 6055, Loss: 1.5022258758544922\n",
      "Iteration: 6056, Loss: 1.5022203922271729\n",
      "Iteration: 6057, Loss: 1.502214789390564\n",
      "Iteration: 6058, Loss: 1.5022093057632446\n",
      "Iteration: 6059, Loss: 1.5022037029266357\n",
      "Iteration: 6060, Loss: 1.5021982192993164\n",
      "Iteration: 6061, Loss: 1.502192735671997\n",
      "Iteration: 6062, Loss: 1.5021872520446777\n",
      "Iteration: 6063, Loss: 1.5021816492080688\n",
      "Iteration: 6064, Loss: 1.5021761655807495\n",
      "Iteration: 6065, Loss: 1.5021706819534302\n",
      "Iteration: 6066, Loss: 1.5021651983261108\n",
      "Iteration: 6067, Loss: 1.5021597146987915\n",
      "Iteration: 6068, Loss: 1.5021542310714722\n",
      "Iteration: 6069, Loss: 1.5021487474441528\n",
      "Iteration: 6070, Loss: 1.5021432638168335\n",
      "Iteration: 6071, Loss: 1.5021378993988037\n",
      "Iteration: 6072, Loss: 1.5021324157714844\n",
      "Iteration: 6073, Loss: 1.502126932144165\n",
      "Iteration: 6074, Loss: 1.5021214485168457\n",
      "Iteration: 6075, Loss: 1.502116084098816\n",
      "Iteration: 6076, Loss: 1.5021106004714966\n",
      "Iteration: 6077, Loss: 1.5021051168441772\n",
      "Iteration: 6078, Loss: 1.5020997524261475\n",
      "Iteration: 6079, Loss: 1.5020942687988281\n",
      "Iteration: 6080, Loss: 1.5020889043807983\n",
      "Iteration: 6081, Loss: 1.502083420753479\n",
      "Iteration: 6082, Loss: 1.5020780563354492\n",
      "Iteration: 6083, Loss: 1.5020726919174194\n",
      "Iteration: 6084, Loss: 1.5020672082901\n",
      "Iteration: 6085, Loss: 1.5020618438720703\n",
      "Iteration: 6086, Loss: 1.5020564794540405\n",
      "Iteration: 6087, Loss: 1.5020511150360107\n",
      "Iteration: 6088, Loss: 1.5020456314086914\n",
      "Iteration: 6089, Loss: 1.5020402669906616\n",
      "Iteration: 6090, Loss: 1.5020349025726318\n",
      "Iteration: 6091, Loss: 1.502029538154602\n",
      "Iteration: 6092, Loss: 1.5020241737365723\n",
      "Iteration: 6093, Loss: 1.5020188093185425\n",
      "Iteration: 6094, Loss: 1.5020134449005127\n",
      "Iteration: 6095, Loss: 1.502008080482483\n",
      "Iteration: 6096, Loss: 1.5020027160644531\n",
      "Iteration: 6097, Loss: 1.5019973516464233\n",
      "Iteration: 6098, Loss: 1.5019919872283936\n",
      "Iteration: 6099, Loss: 1.5019867420196533\n",
      "Iteration: 6100, Loss: 1.5019813776016235\n",
      "Iteration: 6101, Loss: 1.5019760131835938\n",
      "Iteration: 6102, Loss: 1.501970648765564\n",
      "Iteration: 6103, Loss: 1.5019654035568237\n",
      "Iteration: 6104, Loss: 1.501960039138794\n",
      "Iteration: 6105, Loss: 1.5019546747207642\n",
      "Iteration: 6106, Loss: 1.501949429512024\n",
      "Iteration: 6107, Loss: 1.5019440650939941\n",
      "Iteration: 6108, Loss: 1.501938819885254\n",
      "Iteration: 6109, Loss: 1.5019334554672241\n",
      "Iteration: 6110, Loss: 1.5019282102584839\n",
      "Iteration: 6111, Loss: 1.501922845840454\n",
      "Iteration: 6112, Loss: 1.5019176006317139\n",
      "Iteration: 6113, Loss: 1.501912236213684\n",
      "Iteration: 6114, Loss: 1.5019069910049438\n",
      "Iteration: 6115, Loss: 1.5019017457962036\n",
      "Iteration: 6116, Loss: 1.5018963813781738\n",
      "Iteration: 6117, Loss: 1.5018911361694336\n",
      "Iteration: 6118, Loss: 1.5018858909606934\n",
      "Iteration: 6119, Loss: 1.5018805265426636\n",
      "Iteration: 6120, Loss: 1.5018752813339233\n",
      "Iteration: 6121, Loss: 1.501870036125183\n",
      "Iteration: 6122, Loss: 1.5018647909164429\n",
      "Iteration: 6123, Loss: 1.5018595457077026\n",
      "Iteration: 6124, Loss: 1.5018543004989624\n",
      "Iteration: 6125, Loss: 1.5018490552902222\n",
      "Iteration: 6126, Loss: 1.5018436908721924\n",
      "Iteration: 6127, Loss: 1.5018384456634521\n",
      "Iteration: 6128, Loss: 1.501833200454712\n",
      "Iteration: 6129, Loss: 1.5018279552459717\n",
      "Iteration: 6130, Loss: 1.501822829246521\n",
      "Iteration: 6131, Loss: 1.5018175840377808\n",
      "Iteration: 6132, Loss: 1.5018123388290405\n",
      "Iteration: 6133, Loss: 1.5018070936203003\n",
      "Iteration: 6134, Loss: 1.50180184841156\n",
      "Iteration: 6135, Loss: 1.5017966032028198\n",
      "Iteration: 6136, Loss: 1.5017913579940796\n",
      "Iteration: 6137, Loss: 1.5017861127853394\n",
      "Iteration: 6138, Loss: 1.5017809867858887\n",
      "Iteration: 6139, Loss: 1.5017757415771484\n",
      "Iteration: 6140, Loss: 1.5017704963684082\n",
      "Iteration: 6141, Loss: 1.5017653703689575\n",
      "Iteration: 6142, Loss: 1.5017601251602173\n",
      "Iteration: 6143, Loss: 1.501754879951477\n",
      "Iteration: 6144, Loss: 1.5017497539520264\n",
      "Iteration: 6145, Loss: 1.5017445087432861\n",
      "Iteration: 6146, Loss: 1.501739263534546\n",
      "Iteration: 6147, Loss: 1.5017341375350952\n",
      "Iteration: 6148, Loss: 1.501728892326355\n",
      "Iteration: 6149, Loss: 1.5017237663269043\n",
      "Iteration: 6150, Loss: 1.501718521118164\n",
      "Iteration: 6151, Loss: 1.5017133951187134\n",
      "Iteration: 6152, Loss: 1.5017081499099731\n",
      "Iteration: 6153, Loss: 1.5017030239105225\n",
      "Iteration: 6154, Loss: 1.5016978979110718\n",
      "Iteration: 6155, Loss: 1.5016926527023315\n",
      "Iteration: 6156, Loss: 1.5016875267028809\n",
      "Iteration: 6157, Loss: 1.5016822814941406\n",
      "Iteration: 6158, Loss: 1.50167715549469\n",
      "Iteration: 6159, Loss: 1.5016720294952393\n",
      "Iteration: 6160, Loss: 1.501666784286499\n",
      "Iteration: 6161, Loss: 1.5016616582870483\n",
      "Iteration: 6162, Loss: 1.5016565322875977\n",
      "Iteration: 6163, Loss: 1.501651406288147\n",
      "Iteration: 6164, Loss: 1.5016462802886963\n",
      "Iteration: 6165, Loss: 1.501641035079956\n",
      "Iteration: 6166, Loss: 1.5016359090805054\n",
      "Iteration: 6167, Loss: 1.5016307830810547\n",
      "Iteration: 6168, Loss: 1.501625657081604\n",
      "Iteration: 6169, Loss: 1.5016205310821533\n",
      "Iteration: 6170, Loss: 1.5016154050827026\n",
      "Iteration: 6171, Loss: 1.501610279083252\n",
      "Iteration: 6172, Loss: 1.5016051530838013\n",
      "Iteration: 6173, Loss: 1.5016000270843506\n",
      "Iteration: 6174, Loss: 1.5015949010849\n",
      "Iteration: 6175, Loss: 1.5015897750854492\n",
      "Iteration: 6176, Loss: 1.5015846490859985\n",
      "Iteration: 6177, Loss: 1.5015795230865479\n",
      "Iteration: 6178, Loss: 1.5015743970870972\n",
      "Iteration: 6179, Loss: 1.5015692710876465\n",
      "Iteration: 6180, Loss: 1.5015641450881958\n",
      "Iteration: 6181, Loss: 1.5015590190887451\n",
      "Iteration: 6182, Loss: 1.5015538930892944\n",
      "Iteration: 6183, Loss: 1.5015487670898438\n",
      "Iteration: 6184, Loss: 1.5015437602996826\n",
      "Iteration: 6185, Loss: 1.501538634300232\n",
      "Iteration: 6186, Loss: 1.5015335083007812\n",
      "Iteration: 6187, Loss: 1.5015283823013306\n",
      "Iteration: 6188, Loss: 1.5015232563018799\n",
      "Iteration: 6189, Loss: 1.5015182495117188\n",
      "Iteration: 6190, Loss: 1.501513123512268\n",
      "Iteration: 6191, Loss: 1.5015079975128174\n",
      "Iteration: 6192, Loss: 1.5015029907226562\n",
      "Iteration: 6193, Loss: 1.5014978647232056\n",
      "Iteration: 6194, Loss: 1.5014927387237549\n",
      "Iteration: 6195, Loss: 1.5014877319335938\n",
      "Iteration: 6196, Loss: 1.501482605934143\n",
      "Iteration: 6197, Loss: 1.501477599143982\n",
      "Iteration: 6198, Loss: 1.5014724731445312\n",
      "Iteration: 6199, Loss: 1.5014673471450806\n",
      "Iteration: 6200, Loss: 1.5014623403549194\n",
      "Iteration: 6201, Loss: 1.5014572143554688\n",
      "Iteration: 6202, Loss: 1.5014522075653076\n",
      "Iteration: 6203, Loss: 1.501447081565857\n",
      "Iteration: 6204, Loss: 1.5014420747756958\n",
      "Iteration: 6205, Loss: 1.5014370679855347\n",
      "Iteration: 6206, Loss: 1.501431941986084\n",
      "Iteration: 6207, Loss: 1.5014269351959229\n",
      "Iteration: 6208, Loss: 1.5014218091964722\n",
      "Iteration: 6209, Loss: 1.501416802406311\n",
      "Iteration: 6210, Loss: 1.50141179561615\n",
      "Iteration: 6211, Loss: 1.5014066696166992\n",
      "Iteration: 6212, Loss: 1.501401662826538\n",
      "Iteration: 6213, Loss: 1.501396656036377\n",
      "Iteration: 6214, Loss: 1.5013915300369263\n",
      "Iteration: 6215, Loss: 1.5013865232467651\n",
      "Iteration: 6216, Loss: 1.501381516456604\n",
      "Iteration: 6217, Loss: 1.5013765096664429\n",
      "Iteration: 6218, Loss: 1.5013713836669922\n",
      "Iteration: 6219, Loss: 1.501366376876831\n",
      "Iteration: 6220, Loss: 1.50136137008667\n",
      "Iteration: 6221, Loss: 1.5013563632965088\n",
      "Iteration: 6222, Loss: 1.5013513565063477\n",
      "Iteration: 6223, Loss: 1.5013463497161865\n",
      "Iteration: 6224, Loss: 1.5013412237167358\n",
      "Iteration: 6225, Loss: 1.5013362169265747\n",
      "Iteration: 6226, Loss: 1.5013312101364136\n",
      "Iteration: 6227, Loss: 1.5013262033462524\n",
      "Iteration: 6228, Loss: 1.5013211965560913\n",
      "Iteration: 6229, Loss: 1.5013161897659302\n",
      "Iteration: 6230, Loss: 1.501311182975769\n",
      "Iteration: 6231, Loss: 1.501306176185608\n",
      "Iteration: 6232, Loss: 1.5013011693954468\n",
      "Iteration: 6233, Loss: 1.5012961626052856\n",
      "Iteration: 6234, Loss: 1.5012911558151245\n",
      "Iteration: 6235, Loss: 1.5012861490249634\n",
      "Iteration: 6236, Loss: 1.5012811422348022\n",
      "Iteration: 6237, Loss: 1.5012761354446411\n",
      "Iteration: 6238, Loss: 1.50127112865448\n",
      "Iteration: 6239, Loss: 1.5012662410736084\n",
      "Iteration: 6240, Loss: 1.5012612342834473\n",
      "Iteration: 6241, Loss: 1.5012562274932861\n",
      "Iteration: 6242, Loss: 1.501251220703125\n",
      "Iteration: 6243, Loss: 1.5012462139129639\n",
      "Iteration: 6244, Loss: 1.5012412071228027\n",
      "Iteration: 6245, Loss: 1.5012363195419312\n",
      "Iteration: 6246, Loss: 1.50123131275177\n",
      "Iteration: 6247, Loss: 1.5012263059616089\n",
      "Iteration: 6248, Loss: 1.5012212991714478\n",
      "Iteration: 6249, Loss: 1.5012164115905762\n",
      "Iteration: 6250, Loss: 1.501211404800415\n",
      "Iteration: 6251, Loss: 1.501206398010254\n",
      "Iteration: 6252, Loss: 1.5012015104293823\n",
      "Iteration: 6253, Loss: 1.5011965036392212\n",
      "Iteration: 6254, Loss: 1.50119149684906\n",
      "Iteration: 6255, Loss: 1.5011866092681885\n",
      "Iteration: 6256, Loss: 1.5011816024780273\n",
      "Iteration: 6257, Loss: 1.5011767148971558\n",
      "Iteration: 6258, Loss: 1.5011717081069946\n",
      "Iteration: 6259, Loss: 1.5011667013168335\n",
      "Iteration: 6260, Loss: 1.501161813735962\n",
      "Iteration: 6261, Loss: 1.5011568069458008\n",
      "Iteration: 6262, Loss: 1.5011519193649292\n",
      "Iteration: 6263, Loss: 1.501146912574768\n",
      "Iteration: 6264, Loss: 1.5011420249938965\n",
      "Iteration: 6265, Loss: 1.5011370182037354\n",
      "Iteration: 6266, Loss: 1.5011321306228638\n",
      "Iteration: 6267, Loss: 1.5011272430419922\n",
      "Iteration: 6268, Loss: 1.501122236251831\n",
      "Iteration: 6269, Loss: 1.5011173486709595\n",
      "Iteration: 6270, Loss: 1.5011123418807983\n",
      "Iteration: 6271, Loss: 1.5011074542999268\n",
      "Iteration: 6272, Loss: 1.5011025667190552\n",
      "Iteration: 6273, Loss: 1.501097559928894\n",
      "Iteration: 6274, Loss: 1.5010926723480225\n",
      "Iteration: 6275, Loss: 1.5010877847671509\n",
      "Iteration: 6276, Loss: 1.5010827779769897\n",
      "Iteration: 6277, Loss: 1.5010778903961182\n",
      "Iteration: 6278, Loss: 1.5010730028152466\n",
      "Iteration: 6279, Loss: 1.501068115234375\n",
      "Iteration: 6280, Loss: 1.5010631084442139\n",
      "Iteration: 6281, Loss: 1.5010582208633423\n",
      "Iteration: 6282, Loss: 1.5010533332824707\n",
      "Iteration: 6283, Loss: 1.5010484457015991\n",
      "Iteration: 6284, Loss: 1.5010435581207275\n",
      "Iteration: 6285, Loss: 1.5010385513305664\n",
      "Iteration: 6286, Loss: 1.5010336637496948\n",
      "Iteration: 6287, Loss: 1.5010287761688232\n",
      "Iteration: 6288, Loss: 1.5010238885879517\n",
      "Iteration: 6289, Loss: 1.50101900100708\n",
      "Iteration: 6290, Loss: 1.5010141134262085\n",
      "Iteration: 6291, Loss: 1.501009225845337\n",
      "Iteration: 6292, Loss: 1.5010043382644653\n",
      "Iteration: 6293, Loss: 1.5009994506835938\n",
      "Iteration: 6294, Loss: 1.5009945631027222\n",
      "Iteration: 6295, Loss: 1.5009896755218506\n",
      "Iteration: 6296, Loss: 1.500984787940979\n",
      "Iteration: 6297, Loss: 1.5009799003601074\n",
      "Iteration: 6298, Loss: 1.5009750127792358\n",
      "Iteration: 6299, Loss: 1.5009701251983643\n",
      "Iteration: 6300, Loss: 1.5009652376174927\n",
      "Iteration: 6301, Loss: 1.500960350036621\n",
      "Iteration: 6302, Loss: 1.5009554624557495\n",
      "Iteration: 6303, Loss: 1.500950574874878\n",
      "Iteration: 6304, Loss: 1.5009456872940063\n",
      "Iteration: 6305, Loss: 1.5009409189224243\n",
      "Iteration: 6306, Loss: 1.5009360313415527\n",
      "Iteration: 6307, Loss: 1.5009311437606812\n",
      "Iteration: 6308, Loss: 1.5009262561798096\n",
      "Iteration: 6309, Loss: 1.500921368598938\n",
      "Iteration: 6310, Loss: 1.500916600227356\n",
      "Iteration: 6311, Loss: 1.5009117126464844\n",
      "Iteration: 6312, Loss: 1.5009068250656128\n",
      "Iteration: 6313, Loss: 1.5009019374847412\n",
      "Iteration: 6314, Loss: 1.5008971691131592\n",
      "Iteration: 6315, Loss: 1.5008922815322876\n",
      "Iteration: 6316, Loss: 1.500887393951416\n",
      "Iteration: 6317, Loss: 1.500882625579834\n",
      "Iteration: 6318, Loss: 1.5008777379989624\n",
      "Iteration: 6319, Loss: 1.5008728504180908\n",
      "Iteration: 6320, Loss: 1.5008680820465088\n",
      "Iteration: 6321, Loss: 1.5008631944656372\n",
      "Iteration: 6322, Loss: 1.5008583068847656\n",
      "Iteration: 6323, Loss: 1.5008535385131836\n",
      "Iteration: 6324, Loss: 1.500848650932312\n",
      "Iteration: 6325, Loss: 1.50084388256073\n",
      "Iteration: 6326, Loss: 1.5008389949798584\n",
      "Iteration: 6327, Loss: 1.5008342266082764\n",
      "Iteration: 6328, Loss: 1.5008293390274048\n",
      "Iteration: 6329, Loss: 1.5008245706558228\n",
      "Iteration: 6330, Loss: 1.5008196830749512\n",
      "Iteration: 6331, Loss: 1.5008149147033691\n",
      "Iteration: 6332, Loss: 1.5008100271224976\n",
      "Iteration: 6333, Loss: 1.5008052587509155\n",
      "Iteration: 6334, Loss: 1.5008004903793335\n",
      "Iteration: 6335, Loss: 1.500795602798462\n",
      "Iteration: 6336, Loss: 1.5007908344268799\n",
      "Iteration: 6337, Loss: 1.5007859468460083\n",
      "Iteration: 6338, Loss: 1.5007811784744263\n",
      "Iteration: 6339, Loss: 1.5007764101028442\n",
      "Iteration: 6340, Loss: 1.5007715225219727\n",
      "Iteration: 6341, Loss: 1.5007667541503906\n",
      "Iteration: 6342, Loss: 1.5007619857788086\n",
      "Iteration: 6343, Loss: 1.5007572174072266\n",
      "Iteration: 6344, Loss: 1.500752329826355\n",
      "Iteration: 6345, Loss: 1.500747561454773\n",
      "Iteration: 6346, Loss: 1.500742793083191\n",
      "Iteration: 6347, Loss: 1.5007380247116089\n",
      "Iteration: 6348, Loss: 1.5007331371307373\n",
      "Iteration: 6349, Loss: 1.5007283687591553\n",
      "Iteration: 6350, Loss: 1.5007236003875732\n",
      "Iteration: 6351, Loss: 1.5007188320159912\n",
      "Iteration: 6352, Loss: 1.5007140636444092\n",
      "Iteration: 6353, Loss: 1.5007092952728271\n",
      "Iteration: 6354, Loss: 1.5007044076919556\n",
      "Iteration: 6355, Loss: 1.5006996393203735\n",
      "Iteration: 6356, Loss: 1.5006948709487915\n",
      "Iteration: 6357, Loss: 1.5006901025772095\n",
      "Iteration: 6358, Loss: 1.5006853342056274\n",
      "Iteration: 6359, Loss: 1.5006805658340454\n",
      "Iteration: 6360, Loss: 1.5006757974624634\n",
      "Iteration: 6361, Loss: 1.5006710290908813\n",
      "Iteration: 6362, Loss: 1.5006662607192993\n",
      "Iteration: 6363, Loss: 1.5006614923477173\n",
      "Iteration: 6364, Loss: 1.5006567239761353\n",
      "Iteration: 6365, Loss: 1.5006519556045532\n",
      "Iteration: 6366, Loss: 1.5006471872329712\n",
      "Iteration: 6367, Loss: 1.5006424188613892\n",
      "Iteration: 6368, Loss: 1.5006377696990967\n",
      "Iteration: 6369, Loss: 1.5006330013275146\n",
      "Iteration: 6370, Loss: 1.5006282329559326\n",
      "Iteration: 6371, Loss: 1.5006234645843506\n",
      "Iteration: 6372, Loss: 1.5006186962127686\n",
      "Iteration: 6373, Loss: 1.5006139278411865\n",
      "Iteration: 6374, Loss: 1.5006091594696045\n",
      "Iteration: 6375, Loss: 1.500604510307312\n",
      "Iteration: 6376, Loss: 1.50059974193573\n",
      "Iteration: 6377, Loss: 1.500594973564148\n",
      "Iteration: 6378, Loss: 1.500590205192566\n",
      "Iteration: 6379, Loss: 1.5005855560302734\n",
      "Iteration: 6380, Loss: 1.5005807876586914\n",
      "Iteration: 6381, Loss: 1.5005760192871094\n",
      "Iteration: 6382, Loss: 1.5005712509155273\n",
      "Iteration: 6383, Loss: 1.5005666017532349\n",
      "Iteration: 6384, Loss: 1.5005618333816528\n",
      "Iteration: 6385, Loss: 1.5005570650100708\n",
      "Iteration: 6386, Loss: 1.5005524158477783\n",
      "Iteration: 6387, Loss: 1.5005476474761963\n",
      "Iteration: 6388, Loss: 1.5005429983139038\n",
      "Iteration: 6389, Loss: 1.5005382299423218\n",
      "Iteration: 6390, Loss: 1.5005334615707397\n",
      "Iteration: 6391, Loss: 1.5005288124084473\n",
      "Iteration: 6392, Loss: 1.5005240440368652\n",
      "Iteration: 6393, Loss: 1.5005193948745728\n",
      "Iteration: 6394, Loss: 1.5005146265029907\n",
      "Iteration: 6395, Loss: 1.5005099773406982\n",
      "Iteration: 6396, Loss: 1.5005052089691162\n",
      "Iteration: 6397, Loss: 1.5005005598068237\n",
      "Iteration: 6398, Loss: 1.5004957914352417\n",
      "Iteration: 6399, Loss: 1.5004911422729492\n",
      "Iteration: 6400, Loss: 1.5004864931106567\n",
      "Iteration: 6401, Loss: 1.5004817247390747\n",
      "Iteration: 6402, Loss: 1.5004770755767822\n",
      "Iteration: 6403, Loss: 1.5004723072052002\n",
      "Iteration: 6404, Loss: 1.5004676580429077\n",
      "Iteration: 6405, Loss: 1.5004630088806152\n",
      "Iteration: 6406, Loss: 1.5004582405090332\n",
      "Iteration: 6407, Loss: 1.5004535913467407\n",
      "Iteration: 6408, Loss: 1.5004489421844482\n",
      "Iteration: 6409, Loss: 1.5004441738128662\n",
      "Iteration: 6410, Loss: 1.5004395246505737\n",
      "Iteration: 6411, Loss: 1.5004348754882812\n",
      "Iteration: 6412, Loss: 1.5004302263259888\n",
      "Iteration: 6413, Loss: 1.5004254579544067\n",
      "Iteration: 6414, Loss: 1.5004208087921143\n",
      "Iteration: 6415, Loss: 1.5004161596298218\n",
      "Iteration: 6416, Loss: 1.5004115104675293\n",
      "Iteration: 6417, Loss: 1.5004068613052368\n",
      "Iteration: 6418, Loss: 1.5004020929336548\n",
      "Iteration: 6419, Loss: 1.5003974437713623\n",
      "Iteration: 6420, Loss: 1.5003927946090698\n",
      "Iteration: 6421, Loss: 1.5003881454467773\n",
      "Iteration: 6422, Loss: 1.5003834962844849\n",
      "Iteration: 6423, Loss: 1.5003788471221924\n",
      "Iteration: 6424, Loss: 1.5003741979599\n",
      "Iteration: 6425, Loss: 1.5003695487976074\n",
      "Iteration: 6426, Loss: 1.500364899635315\n",
      "Iteration: 6427, Loss: 1.5003602504730225\n",
      "Iteration: 6428, Loss: 1.50035560131073\n",
      "Iteration: 6429, Loss: 1.5003509521484375\n",
      "Iteration: 6430, Loss: 1.500346302986145\n",
      "Iteration: 6431, Loss: 1.5003416538238525\n",
      "Iteration: 6432, Loss: 1.50033700466156\n",
      "Iteration: 6433, Loss: 1.5003323554992676\n",
      "Iteration: 6434, Loss: 1.500327706336975\n",
      "Iteration: 6435, Loss: 1.5003230571746826\n",
      "Iteration: 6436, Loss: 1.5003184080123901\n",
      "Iteration: 6437, Loss: 1.5003137588500977\n",
      "Iteration: 6438, Loss: 1.5003091096878052\n",
      "Iteration: 6439, Loss: 1.5003045797348022\n",
      "Iteration: 6440, Loss: 1.5002999305725098\n",
      "Iteration: 6441, Loss: 1.5002952814102173\n",
      "Iteration: 6442, Loss: 1.5002906322479248\n",
      "Iteration: 6443, Loss: 1.5002859830856323\n",
      "Iteration: 6444, Loss: 1.5002814531326294\n",
      "Iteration: 6445, Loss: 1.500276803970337\n",
      "Iteration: 6446, Loss: 1.5002721548080444\n",
      "Iteration: 6447, Loss: 1.500267505645752\n",
      "Iteration: 6448, Loss: 1.500262975692749\n",
      "Iteration: 6449, Loss: 1.5002583265304565\n",
      "Iteration: 6450, Loss: 1.500253677368164\n",
      "Iteration: 6451, Loss: 1.5002491474151611\n",
      "Iteration: 6452, Loss: 1.5002444982528687\n",
      "Iteration: 6453, Loss: 1.5002398490905762\n",
      "Iteration: 6454, Loss: 1.5002353191375732\n",
      "Iteration: 6455, Loss: 1.5002306699752808\n",
      "Iteration: 6456, Loss: 1.5002261400222778\n",
      "Iteration: 6457, Loss: 1.5002214908599854\n",
      "Iteration: 6458, Loss: 1.5002168416976929\n",
      "Iteration: 6459, Loss: 1.50021231174469\n",
      "Iteration: 6460, Loss: 1.5002076625823975\n",
      "Iteration: 6461, Loss: 1.5002031326293945\n",
      "Iteration: 6462, Loss: 1.500198483467102\n",
      "Iteration: 6463, Loss: 1.5001939535140991\n",
      "Iteration: 6464, Loss: 1.5001893043518066\n",
      "Iteration: 6465, Loss: 1.5001847743988037\n",
      "Iteration: 6466, Loss: 1.5001801252365112\n",
      "Iteration: 6467, Loss: 1.5001755952835083\n",
      "Iteration: 6468, Loss: 1.5001710653305054\n",
      "Iteration: 6469, Loss: 1.500166416168213\n",
      "Iteration: 6470, Loss: 1.50016188621521\n",
      "Iteration: 6471, Loss: 1.5001572370529175\n",
      "Iteration: 6472, Loss: 1.5001527070999146\n",
      "Iteration: 6473, Loss: 1.5001481771469116\n",
      "Iteration: 6474, Loss: 1.5001435279846191\n",
      "Iteration: 6475, Loss: 1.5001389980316162\n",
      "Iteration: 6476, Loss: 1.5001344680786133\n",
      "Iteration: 6477, Loss: 1.5001299381256104\n",
      "Iteration: 6478, Loss: 1.5001252889633179\n",
      "Iteration: 6479, Loss: 1.500120759010315\n",
      "Iteration: 6480, Loss: 1.500116229057312\n",
      "Iteration: 6481, Loss: 1.500111699104309\n",
      "Iteration: 6482, Loss: 1.5001070499420166\n",
      "Iteration: 6483, Loss: 1.5001025199890137\n",
      "Iteration: 6484, Loss: 1.5000979900360107\n",
      "Iteration: 6485, Loss: 1.5000934600830078\n",
      "Iteration: 6486, Loss: 1.5000889301300049\n",
      "Iteration: 6487, Loss: 1.500084400177002\n",
      "Iteration: 6488, Loss: 1.500079870223999\n",
      "Iteration: 6489, Loss: 1.5000752210617065\n",
      "Iteration: 6490, Loss: 1.5000706911087036\n",
      "Iteration: 6491, Loss: 1.5000661611557007\n",
      "Iteration: 6492, Loss: 1.5000616312026978\n",
      "Iteration: 6493, Loss: 1.5000571012496948\n",
      "Iteration: 6494, Loss: 1.500052571296692\n",
      "Iteration: 6495, Loss: 1.500048041343689\n",
      "Iteration: 6496, Loss: 1.500043511390686\n",
      "Iteration: 6497, Loss: 1.500038981437683\n",
      "Iteration: 6498, Loss: 1.5000344514846802\n",
      "Iteration: 6499, Loss: 1.5000299215316772\n",
      "Iteration: 6500, Loss: 1.5000253915786743\n",
      "Iteration: 6501, Loss: 1.5000208616256714\n",
      "Iteration: 6502, Loss: 1.500016450881958\n",
      "Iteration: 6503, Loss: 1.500011920928955\n",
      "Iteration: 6504, Loss: 1.5000073909759521\n",
      "Iteration: 6505, Loss: 1.5000028610229492\n",
      "Iteration: 6506, Loss: 1.4999983310699463\n",
      "Iteration: 6507, Loss: 1.4999938011169434\n",
      "Iteration: 6508, Loss: 1.4999892711639404\n",
      "Iteration: 6509, Loss: 1.499984860420227\n",
      "Iteration: 6510, Loss: 1.4999803304672241\n",
      "Iteration: 6511, Loss: 1.4999758005142212\n",
      "Iteration: 6512, Loss: 1.4999712705612183\n",
      "Iteration: 6513, Loss: 1.4999668598175049\n",
      "Iteration: 6514, Loss: 1.499962329864502\n",
      "Iteration: 6515, Loss: 1.499957799911499\n",
      "Iteration: 6516, Loss: 1.499953269958496\n",
      "Iteration: 6517, Loss: 1.4999488592147827\n",
      "Iteration: 6518, Loss: 1.4999443292617798\n",
      "Iteration: 6519, Loss: 1.4999397993087769\n",
      "Iteration: 6520, Loss: 1.4999353885650635\n",
      "Iteration: 6521, Loss: 1.4999308586120605\n",
      "Iteration: 6522, Loss: 1.4999264478683472\n",
      "Iteration: 6523, Loss: 1.4999219179153442\n",
      "Iteration: 6524, Loss: 1.4999173879623413\n",
      "Iteration: 6525, Loss: 1.499912977218628\n",
      "Iteration: 6526, Loss: 1.499908447265625\n",
      "Iteration: 6527, Loss: 1.4999040365219116\n",
      "Iteration: 6528, Loss: 1.4998995065689087\n",
      "Iteration: 6529, Loss: 1.4998950958251953\n",
      "Iteration: 6530, Loss: 1.4998905658721924\n",
      "Iteration: 6531, Loss: 1.499886155128479\n",
      "Iteration: 6532, Loss: 1.499881625175476\n",
      "Iteration: 6533, Loss: 1.4998772144317627\n",
      "Iteration: 6534, Loss: 1.4998726844787598\n",
      "Iteration: 6535, Loss: 1.4998682737350464\n",
      "Iteration: 6536, Loss: 1.499863862991333\n",
      "Iteration: 6537, Loss: 1.49985933303833\n",
      "Iteration: 6538, Loss: 1.4998549222946167\n",
      "Iteration: 6539, Loss: 1.4998505115509033\n",
      "Iteration: 6540, Loss: 1.4998459815979004\n",
      "Iteration: 6541, Loss: 1.499841570854187\n",
      "Iteration: 6542, Loss: 1.4998371601104736\n",
      "Iteration: 6543, Loss: 1.4998326301574707\n",
      "Iteration: 6544, Loss: 1.4998282194137573\n",
      "Iteration: 6545, Loss: 1.499823808670044\n",
      "Iteration: 6546, Loss: 1.499819278717041\n",
      "Iteration: 6547, Loss: 1.4998148679733276\n",
      "Iteration: 6548, Loss: 1.4998104572296143\n",
      "Iteration: 6549, Loss: 1.4998060464859009\n",
      "Iteration: 6550, Loss: 1.4998016357421875\n",
      "Iteration: 6551, Loss: 1.4997971057891846\n",
      "Iteration: 6552, Loss: 1.4997926950454712\n",
      "Iteration: 6553, Loss: 1.4997882843017578\n",
      "Iteration: 6554, Loss: 1.4997838735580444\n",
      "Iteration: 6555, Loss: 1.499779462814331\n",
      "Iteration: 6556, Loss: 1.4997750520706177\n",
      "Iteration: 6557, Loss: 1.4997706413269043\n",
      "Iteration: 6558, Loss: 1.499766230583191\n",
      "Iteration: 6559, Loss: 1.4997618198394775\n",
      "Iteration: 6560, Loss: 1.4997574090957642\n",
      "Iteration: 6561, Loss: 1.4997529983520508\n",
      "Iteration: 6562, Loss: 1.4997485876083374\n",
      "Iteration: 6563, Loss: 1.499744176864624\n",
      "Iteration: 6564, Loss: 1.4997397661209106\n",
      "Iteration: 6565, Loss: 1.4997353553771973\n",
      "Iteration: 6566, Loss: 1.4997309446334839\n",
      "Iteration: 6567, Loss: 1.4997265338897705\n",
      "Iteration: 6568, Loss: 1.4997221231460571\n",
      "Iteration: 6569, Loss: 1.4997177124023438\n",
      "Iteration: 6570, Loss: 1.4997133016586304\n",
      "Iteration: 6571, Loss: 1.499708890914917\n",
      "Iteration: 6572, Loss: 1.4997044801712036\n",
      "Iteration: 6573, Loss: 1.4997001886367798\n",
      "Iteration: 6574, Loss: 1.4996957778930664\n",
      "Iteration: 6575, Loss: 1.499691367149353\n",
      "Iteration: 6576, Loss: 1.4996869564056396\n",
      "Iteration: 6577, Loss: 1.4996825456619263\n",
      "Iteration: 6578, Loss: 1.4996782541275024\n",
      "Iteration: 6579, Loss: 1.499673843383789\n",
      "Iteration: 6580, Loss: 1.4996694326400757\n",
      "Iteration: 6581, Loss: 1.4996650218963623\n",
      "Iteration: 6582, Loss: 1.4996607303619385\n",
      "Iteration: 6583, Loss: 1.499656319618225\n",
      "Iteration: 6584, Loss: 1.4996519088745117\n",
      "Iteration: 6585, Loss: 1.499647617340088\n",
      "Iteration: 6586, Loss: 1.4996432065963745\n",
      "Iteration: 6587, Loss: 1.4996387958526611\n",
      "Iteration: 6588, Loss: 1.4996345043182373\n",
      "Iteration: 6589, Loss: 1.499630093574524\n",
      "Iteration: 6590, Loss: 1.4996258020401\n",
      "Iteration: 6591, Loss: 1.4996213912963867\n",
      "Iteration: 6592, Loss: 1.4996169805526733\n",
      "Iteration: 6593, Loss: 1.4996126890182495\n",
      "Iteration: 6594, Loss: 1.4996082782745361\n",
      "Iteration: 6595, Loss: 1.4996039867401123\n",
      "Iteration: 6596, Loss: 1.499599575996399\n",
      "Iteration: 6597, Loss: 1.499595284461975\n",
      "Iteration: 6598, Loss: 1.4995908737182617\n",
      "Iteration: 6599, Loss: 1.499586582183838\n",
      "Iteration: 6600, Loss: 1.499582290649414\n",
      "Iteration: 6601, Loss: 1.4995778799057007\n",
      "Iteration: 6602, Loss: 1.4995735883712769\n",
      "Iteration: 6603, Loss: 1.4995691776275635\n",
      "Iteration: 6604, Loss: 1.4995648860931396\n",
      "Iteration: 6605, Loss: 1.4995605945587158\n",
      "Iteration: 6606, Loss: 1.4995561838150024\n",
      "Iteration: 6607, Loss: 1.4995518922805786\n",
      "Iteration: 6608, Loss: 1.4995476007461548\n",
      "Iteration: 6609, Loss: 1.4995431900024414\n",
      "Iteration: 6610, Loss: 1.4995388984680176\n",
      "Iteration: 6611, Loss: 1.4995346069335938\n",
      "Iteration: 6612, Loss: 1.49953031539917\n",
      "Iteration: 6613, Loss: 1.4995259046554565\n",
      "Iteration: 6614, Loss: 1.4995216131210327\n",
      "Iteration: 6615, Loss: 1.4995173215866089\n",
      "Iteration: 6616, Loss: 1.499513030052185\n",
      "Iteration: 6617, Loss: 1.4995086193084717\n",
      "Iteration: 6618, Loss: 1.4995043277740479\n",
      "Iteration: 6619, Loss: 1.499500036239624\n",
      "Iteration: 6620, Loss: 1.4994957447052002\n",
      "Iteration: 6621, Loss: 1.4994914531707764\n",
      "Iteration: 6622, Loss: 1.4994871616363525\n",
      "Iteration: 6623, Loss: 1.4994828701019287\n",
      "Iteration: 6624, Loss: 1.4994785785675049\n",
      "Iteration: 6625, Loss: 1.499474287033081\n",
      "Iteration: 6626, Loss: 1.4994699954986572\n",
      "Iteration: 6627, Loss: 1.4994657039642334\n",
      "Iteration: 6628, Loss: 1.4994614124298096\n",
      "Iteration: 6629, Loss: 1.4994571208953857\n",
      "Iteration: 6630, Loss: 1.499452829360962\n",
      "Iteration: 6631, Loss: 1.499448537826538\n",
      "Iteration: 6632, Loss: 1.4994442462921143\n",
      "Iteration: 6633, Loss: 1.4994399547576904\n",
      "Iteration: 6634, Loss: 1.4994356632232666\n",
      "Iteration: 6635, Loss: 1.4994313716888428\n",
      "Iteration: 6636, Loss: 1.499427080154419\n",
      "Iteration: 6637, Loss: 1.4994227886199951\n",
      "Iteration: 6638, Loss: 1.4994184970855713\n",
      "Iteration: 6639, Loss: 1.4994142055511475\n",
      "Iteration: 6640, Loss: 1.4994100332260132\n",
      "Iteration: 6641, Loss: 1.4994057416915894\n",
      "Iteration: 6642, Loss: 1.4994014501571655\n",
      "Iteration: 6643, Loss: 1.4993971586227417\n",
      "Iteration: 6644, Loss: 1.4993929862976074\n",
      "Iteration: 6645, Loss: 1.4993886947631836\n",
      "Iteration: 6646, Loss: 1.4993844032287598\n",
      "Iteration: 6647, Loss: 1.499380111694336\n",
      "Iteration: 6648, Loss: 1.4993759393692017\n",
      "Iteration: 6649, Loss: 1.4993716478347778\n",
      "Iteration: 6650, Loss: 1.499367356300354\n",
      "Iteration: 6651, Loss: 1.4993631839752197\n",
      "Iteration: 6652, Loss: 1.499358892440796\n",
      "Iteration: 6653, Loss: 1.499354600906372\n",
      "Iteration: 6654, Loss: 1.4993504285812378\n",
      "Iteration: 6655, Loss: 1.499346137046814\n",
      "Iteration: 6656, Loss: 1.4993419647216797\n",
      "Iteration: 6657, Loss: 1.4993376731872559\n",
      "Iteration: 6658, Loss: 1.499333381652832\n",
      "Iteration: 6659, Loss: 1.4993292093276978\n",
      "Iteration: 6660, Loss: 1.499324917793274\n",
      "Iteration: 6661, Loss: 1.4993207454681396\n",
      "Iteration: 6662, Loss: 1.4993164539337158\n",
      "Iteration: 6663, Loss: 1.4993122816085815\n",
      "Iteration: 6664, Loss: 1.4993079900741577\n",
      "Iteration: 6665, Loss: 1.4993038177490234\n",
      "Iteration: 6666, Loss: 1.4992996454238892\n",
      "Iteration: 6667, Loss: 1.4992953538894653\n",
      "Iteration: 6668, Loss: 1.499291181564331\n",
      "Iteration: 6669, Loss: 1.4992868900299072\n",
      "Iteration: 6670, Loss: 1.499282717704773\n",
      "Iteration: 6671, Loss: 1.4992785453796387\n",
      "Iteration: 6672, Loss: 1.4992742538452148\n",
      "Iteration: 6673, Loss: 1.4992700815200806\n",
      "Iteration: 6674, Loss: 1.4992659091949463\n",
      "Iteration: 6675, Loss: 1.4992616176605225\n",
      "Iteration: 6676, Loss: 1.4992574453353882\n",
      "Iteration: 6677, Loss: 1.499253273010254\n",
      "Iteration: 6678, Loss: 1.4992491006851196\n",
      "Iteration: 6679, Loss: 1.4992448091506958\n",
      "Iteration: 6680, Loss: 1.4992406368255615\n",
      "Iteration: 6681, Loss: 1.4992364645004272\n",
      "Iteration: 6682, Loss: 1.499232292175293\n",
      "Iteration: 6683, Loss: 1.4992281198501587\n",
      "Iteration: 6684, Loss: 1.4992239475250244\n",
      "Iteration: 6685, Loss: 1.4992196559906006\n",
      "Iteration: 6686, Loss: 1.4992154836654663\n",
      "Iteration: 6687, Loss: 1.499211311340332\n",
      "Iteration: 6688, Loss: 1.4992071390151978\n",
      "Iteration: 6689, Loss: 1.4992029666900635\n",
      "Iteration: 6690, Loss: 1.4991987943649292\n",
      "Iteration: 6691, Loss: 1.499194622039795\n",
      "Iteration: 6692, Loss: 1.4991904497146606\n",
      "Iteration: 6693, Loss: 1.4991862773895264\n",
      "Iteration: 6694, Loss: 1.499182105064392\n",
      "Iteration: 6695, Loss: 1.4991779327392578\n",
      "Iteration: 6696, Loss: 1.4991737604141235\n",
      "Iteration: 6697, Loss: 1.4991695880889893\n",
      "Iteration: 6698, Loss: 1.499165415763855\n",
      "Iteration: 6699, Loss: 1.4991612434387207\n",
      "Iteration: 6700, Loss: 1.4991570711135864\n",
      "Iteration: 6701, Loss: 1.4991528987884521\n",
      "Iteration: 6702, Loss: 1.4991487264633179\n",
      "Iteration: 6703, Loss: 1.4991446733474731\n",
      "Iteration: 6704, Loss: 1.4991405010223389\n",
      "Iteration: 6705, Loss: 1.4991363286972046\n",
      "Iteration: 6706, Loss: 1.4991321563720703\n",
      "Iteration: 6707, Loss: 1.499127984046936\n",
      "Iteration: 6708, Loss: 1.4991239309310913\n",
      "Iteration: 6709, Loss: 1.499119758605957\n",
      "Iteration: 6710, Loss: 1.4991155862808228\n",
      "Iteration: 6711, Loss: 1.4991114139556885\n",
      "Iteration: 6712, Loss: 1.4991073608398438\n",
      "Iteration: 6713, Loss: 1.4991031885147095\n",
      "Iteration: 6714, Loss: 1.4990990161895752\n",
      "Iteration: 6715, Loss: 1.4990949630737305\n",
      "Iteration: 6716, Loss: 1.4990907907485962\n",
      "Iteration: 6717, Loss: 1.499086618423462\n",
      "Iteration: 6718, Loss: 1.4990825653076172\n",
      "Iteration: 6719, Loss: 1.499078392982483\n",
      "Iteration: 6720, Loss: 1.4990742206573486\n",
      "Iteration: 6721, Loss: 1.499070167541504\n",
      "Iteration: 6722, Loss: 1.4990659952163696\n",
      "Iteration: 6723, Loss: 1.499061942100525\n",
      "Iteration: 6724, Loss: 1.4990577697753906\n",
      "Iteration: 6725, Loss: 1.499053716659546\n",
      "Iteration: 6726, Loss: 1.4990495443344116\n",
      "Iteration: 6727, Loss: 1.499045491218567\n",
      "Iteration: 6728, Loss: 1.4990413188934326\n",
      "Iteration: 6729, Loss: 1.499037265777588\n",
      "Iteration: 6730, Loss: 1.4990330934524536\n",
      "Iteration: 6731, Loss: 1.4990290403366089\n",
      "Iteration: 6732, Loss: 1.4990249872207642\n",
      "Iteration: 6733, Loss: 1.4990208148956299\n",
      "Iteration: 6734, Loss: 1.4990167617797852\n",
      "Iteration: 6735, Loss: 1.4990125894546509\n",
      "Iteration: 6736, Loss: 1.4990085363388062\n",
      "Iteration: 6737, Loss: 1.4990044832229614\n",
      "Iteration: 6738, Loss: 1.4990003108978271\n",
      "Iteration: 6739, Loss: 1.4989962577819824\n",
      "Iteration: 6740, Loss: 1.4989922046661377\n",
      "Iteration: 6741, Loss: 1.498988151550293\n",
      "Iteration: 6742, Loss: 1.4989839792251587\n",
      "Iteration: 6743, Loss: 1.498979926109314\n",
      "Iteration: 6744, Loss: 1.4989758729934692\n",
      "Iteration: 6745, Loss: 1.4989718198776245\n",
      "Iteration: 6746, Loss: 1.4989677667617798\n",
      "Iteration: 6747, Loss: 1.4989635944366455\n",
      "Iteration: 6748, Loss: 1.4989595413208008\n",
      "Iteration: 6749, Loss: 1.498955488204956\n",
      "Iteration: 6750, Loss: 1.4989514350891113\n",
      "Iteration: 6751, Loss: 1.4989473819732666\n",
      "Iteration: 6752, Loss: 1.4989433288574219\n",
      "Iteration: 6753, Loss: 1.4989392757415771\n",
      "Iteration: 6754, Loss: 1.4989352226257324\n",
      "Iteration: 6755, Loss: 1.4989311695098877\n",
      "Iteration: 6756, Loss: 1.498927116394043\n",
      "Iteration: 6757, Loss: 1.4989229440689087\n",
      "Iteration: 6758, Loss: 1.498918890953064\n",
      "Iteration: 6759, Loss: 1.4989149570465088\n",
      "Iteration: 6760, Loss: 1.498910903930664\n",
      "Iteration: 6761, Loss: 1.4989068508148193\n",
      "Iteration: 6762, Loss: 1.4989027976989746\n",
      "Iteration: 6763, Loss: 1.4988987445831299\n",
      "Iteration: 6764, Loss: 1.4988946914672852\n",
      "Iteration: 6765, Loss: 1.4988906383514404\n",
      "Iteration: 6766, Loss: 1.4988865852355957\n",
      "Iteration: 6767, Loss: 1.498882532119751\n",
      "Iteration: 6768, Loss: 1.4988784790039062\n",
      "Iteration: 6769, Loss: 1.498874545097351\n",
      "Iteration: 6770, Loss: 1.4988704919815063\n",
      "Iteration: 6771, Loss: 1.4988664388656616\n",
      "Iteration: 6772, Loss: 1.498862385749817\n",
      "Iteration: 6773, Loss: 1.4988583326339722\n",
      "Iteration: 6774, Loss: 1.498854398727417\n",
      "Iteration: 6775, Loss: 1.4988503456115723\n",
      "Iteration: 6776, Loss: 1.4988462924957275\n",
      "Iteration: 6777, Loss: 1.4988423585891724\n",
      "Iteration: 6778, Loss: 1.4988383054733276\n",
      "Iteration: 6779, Loss: 1.498834252357483\n",
      "Iteration: 6780, Loss: 1.4988303184509277\n",
      "Iteration: 6781, Loss: 1.498826265335083\n",
      "Iteration: 6782, Loss: 1.4988222122192383\n",
      "Iteration: 6783, Loss: 1.498818278312683\n",
      "Iteration: 6784, Loss: 1.4988142251968384\n",
      "Iteration: 6785, Loss: 1.4988102912902832\n",
      "Iteration: 6786, Loss: 1.4988062381744385\n",
      "Iteration: 6787, Loss: 1.4988021850585938\n",
      "Iteration: 6788, Loss: 1.4987982511520386\n",
      "Iteration: 6789, Loss: 1.4987941980361938\n",
      "Iteration: 6790, Loss: 1.4987902641296387\n",
      "Iteration: 6791, Loss: 1.498786211013794\n",
      "Iteration: 6792, Loss: 1.4987822771072388\n",
      "Iteration: 6793, Loss: 1.4987783432006836\n",
      "Iteration: 6794, Loss: 1.4987742900848389\n",
      "Iteration: 6795, Loss: 1.4987703561782837\n",
      "Iteration: 6796, Loss: 1.498766303062439\n",
      "Iteration: 6797, Loss: 1.4987623691558838\n",
      "Iteration: 6798, Loss: 1.4987584352493286\n",
      "Iteration: 6799, Loss: 1.4987543821334839\n",
      "Iteration: 6800, Loss: 1.4987504482269287\n",
      "Iteration: 6801, Loss: 1.4987465143203735\n",
      "Iteration: 6802, Loss: 1.4987424612045288\n",
      "Iteration: 6803, Loss: 1.4987385272979736\n",
      "Iteration: 6804, Loss: 1.4987345933914185\n",
      "Iteration: 6805, Loss: 1.4987305402755737\n",
      "Iteration: 6806, Loss: 1.4987266063690186\n",
      "Iteration: 6807, Loss: 1.4987226724624634\n",
      "Iteration: 6808, Loss: 1.4987187385559082\n",
      "Iteration: 6809, Loss: 1.498714804649353\n",
      "Iteration: 6810, Loss: 1.4987107515335083\n",
      "Iteration: 6811, Loss: 1.4987068176269531\n",
      "Iteration: 6812, Loss: 1.498702883720398\n",
      "Iteration: 6813, Loss: 1.4986989498138428\n",
      "Iteration: 6814, Loss: 1.4986950159072876\n",
      "Iteration: 6815, Loss: 1.4986910820007324\n",
      "Iteration: 6816, Loss: 1.4986871480941772\n",
      "Iteration: 6817, Loss: 1.498683214187622\n",
      "Iteration: 6818, Loss: 1.498679280281067\n",
      "Iteration: 6819, Loss: 1.4986753463745117\n",
      "Iteration: 6820, Loss: 1.4986714124679565\n",
      "Iteration: 6821, Loss: 1.4986674785614014\n",
      "Iteration: 6822, Loss: 1.4986635446548462\n",
      "Iteration: 6823, Loss: 1.498659610748291\n",
      "Iteration: 6824, Loss: 1.4986556768417358\n",
      "Iteration: 6825, Loss: 1.4986517429351807\n",
      "Iteration: 6826, Loss: 1.4986478090286255\n",
      "Iteration: 6827, Loss: 1.4986438751220703\n",
      "Iteration: 6828, Loss: 1.4986399412155151\n",
      "Iteration: 6829, Loss: 1.49863600730896\n",
      "Iteration: 6830, Loss: 1.4986320734024048\n",
      "Iteration: 6831, Loss: 1.4986282587051392\n",
      "Iteration: 6832, Loss: 1.498624324798584\n",
      "Iteration: 6833, Loss: 1.4986203908920288\n",
      "Iteration: 6834, Loss: 1.4986164569854736\n",
      "Iteration: 6835, Loss: 1.4986125230789185\n",
      "Iteration: 6836, Loss: 1.4986087083816528\n",
      "Iteration: 6837, Loss: 1.4986047744750977\n",
      "Iteration: 6838, Loss: 1.4986008405685425\n",
      "Iteration: 6839, Loss: 1.4985969066619873\n",
      "Iteration: 6840, Loss: 1.4985930919647217\n",
      "Iteration: 6841, Loss: 1.4985891580581665\n",
      "Iteration: 6842, Loss: 1.4985852241516113\n",
      "Iteration: 6843, Loss: 1.4985814094543457\n",
      "Iteration: 6844, Loss: 1.4985774755477905\n",
      "Iteration: 6845, Loss: 1.498573660850525\n",
      "Iteration: 6846, Loss: 1.4985697269439697\n",
      "Iteration: 6847, Loss: 1.4985657930374146\n",
      "Iteration: 6848, Loss: 1.498561978340149\n",
      "Iteration: 6849, Loss: 1.4985580444335938\n",
      "Iteration: 6850, Loss: 1.4985542297363281\n",
      "Iteration: 6851, Loss: 1.498550295829773\n",
      "Iteration: 6852, Loss: 1.4985464811325073\n",
      "Iteration: 6853, Loss: 1.4985425472259521\n",
      "Iteration: 6854, Loss: 1.4985387325286865\n",
      "Iteration: 6855, Loss: 1.4985347986221313\n",
      "Iteration: 6856, Loss: 1.4985309839248657\n",
      "Iteration: 6857, Loss: 1.4985271692276\n",
      "Iteration: 6858, Loss: 1.498523235321045\n",
      "Iteration: 6859, Loss: 1.4985194206237793\n",
      "Iteration: 6860, Loss: 1.4985154867172241\n",
      "Iteration: 6861, Loss: 1.4985116720199585\n",
      "Iteration: 6862, Loss: 1.4985078573226929\n",
      "Iteration: 6863, Loss: 1.4985039234161377\n",
      "Iteration: 6864, Loss: 1.498500108718872\n",
      "Iteration: 6865, Loss: 1.4984962940216064\n",
      "Iteration: 6866, Loss: 1.4984924793243408\n",
      "Iteration: 6867, Loss: 1.4984885454177856\n",
      "Iteration: 6868, Loss: 1.49848473072052\n",
      "Iteration: 6869, Loss: 1.4984809160232544\n",
      "Iteration: 6870, Loss: 1.4984771013259888\n",
      "Iteration: 6871, Loss: 1.4984732866287231\n",
      "Iteration: 6872, Loss: 1.498469352722168\n",
      "Iteration: 6873, Loss: 1.4984655380249023\n",
      "Iteration: 6874, Loss: 1.4984617233276367\n",
      "Iteration: 6875, Loss: 1.498457908630371\n",
      "Iteration: 6876, Loss: 1.4984540939331055\n",
      "Iteration: 6877, Loss: 1.4984502792358398\n",
      "Iteration: 6878, Loss: 1.4984464645385742\n",
      "Iteration: 6879, Loss: 1.4984426498413086\n",
      "Iteration: 6880, Loss: 1.498438835144043\n",
      "Iteration: 6881, Loss: 1.4984350204467773\n",
      "Iteration: 6882, Loss: 1.4984312057495117\n",
      "Iteration: 6883, Loss: 1.498427391052246\n",
      "Iteration: 6884, Loss: 1.4984235763549805\n",
      "Iteration: 6885, Loss: 1.4984197616577148\n",
      "Iteration: 6886, Loss: 1.4984159469604492\n",
      "Iteration: 6887, Loss: 1.4984121322631836\n",
      "Iteration: 6888, Loss: 1.498408317565918\n",
      "Iteration: 6889, Loss: 1.4984045028686523\n",
      "Iteration: 6890, Loss: 1.4984006881713867\n",
      "Iteration: 6891, Loss: 1.498396873474121\n",
      "Iteration: 6892, Loss: 1.498393177986145\n",
      "Iteration: 6893, Loss: 1.4983893632888794\n",
      "Iteration: 6894, Loss: 1.4983855485916138\n",
      "Iteration: 6895, Loss: 1.4983817338943481\n",
      "Iteration: 6896, Loss: 1.4983779191970825\n",
      "Iteration: 6897, Loss: 1.4983742237091064\n",
      "Iteration: 6898, Loss: 1.4983704090118408\n",
      "Iteration: 6899, Loss: 1.4983665943145752\n",
      "Iteration: 6900, Loss: 1.4983627796173096\n",
      "Iteration: 6901, Loss: 1.4983590841293335\n",
      "Iteration: 6902, Loss: 1.4983552694320679\n",
      "Iteration: 6903, Loss: 1.4983514547348022\n",
      "Iteration: 6904, Loss: 1.4983477592468262\n",
      "Iteration: 6905, Loss: 1.4983439445495605\n",
      "Iteration: 6906, Loss: 1.4983402490615845\n",
      "Iteration: 6907, Loss: 1.4983364343643188\n",
      "Iteration: 6908, Loss: 1.4983326196670532\n",
      "Iteration: 6909, Loss: 1.4983289241790771\n",
      "Iteration: 6910, Loss: 1.4983251094818115\n",
      "Iteration: 6911, Loss: 1.4983214139938354\n",
      "Iteration: 6912, Loss: 1.4983175992965698\n",
      "Iteration: 6913, Loss: 1.4983139038085938\n",
      "Iteration: 6914, Loss: 1.4983100891113281\n",
      "Iteration: 6915, Loss: 1.498306393623352\n",
      "Iteration: 6916, Loss: 1.498302698135376\n",
      "Iteration: 6917, Loss: 1.4982988834381104\n",
      "Iteration: 6918, Loss: 1.4982951879501343\n",
      "Iteration: 6919, Loss: 1.4982913732528687\n",
      "Iteration: 6920, Loss: 1.4982876777648926\n",
      "Iteration: 6921, Loss: 1.4982839822769165\n",
      "Iteration: 6922, Loss: 1.4982801675796509\n",
      "Iteration: 6923, Loss: 1.4982764720916748\n",
      "Iteration: 6924, Loss: 1.4982727766036987\n",
      "Iteration: 6925, Loss: 1.498268961906433\n",
      "Iteration: 6926, Loss: 1.498265266418457\n",
      "Iteration: 6927, Loss: 1.498261570930481\n",
      "Iteration: 6928, Loss: 1.4982578754425049\n",
      "Iteration: 6929, Loss: 1.4982540607452393\n",
      "Iteration: 6930, Loss: 1.4982503652572632\n",
      "Iteration: 6931, Loss: 1.498246669769287\n",
      "Iteration: 6932, Loss: 1.498242974281311\n",
      "Iteration: 6933, Loss: 1.498239278793335\n",
      "Iteration: 6934, Loss: 1.4982355833053589\n",
      "Iteration: 6935, Loss: 1.4982317686080933\n",
      "Iteration: 6936, Loss: 1.4982280731201172\n",
      "Iteration: 6937, Loss: 1.4982243776321411\n",
      "Iteration: 6938, Loss: 1.498220682144165\n",
      "Iteration: 6939, Loss: 1.498216986656189\n",
      "Iteration: 6940, Loss: 1.498213291168213\n",
      "Iteration: 6941, Loss: 1.4982095956802368\n",
      "Iteration: 6942, Loss: 1.4982059001922607\n",
      "Iteration: 6943, Loss: 1.4982022047042847\n",
      "Iteration: 6944, Loss: 1.4981985092163086\n",
      "Iteration: 6945, Loss: 1.4981948137283325\n",
      "Iteration: 6946, Loss: 1.4981911182403564\n",
      "Iteration: 6947, Loss: 1.4981874227523804\n",
      "Iteration: 6948, Loss: 1.4981837272644043\n",
      "Iteration: 6949, Loss: 1.4981801509857178\n",
      "Iteration: 6950, Loss: 1.4981764554977417\n",
      "Iteration: 6951, Loss: 1.4981727600097656\n",
      "Iteration: 6952, Loss: 1.4981690645217896\n",
      "Iteration: 6953, Loss: 1.4981653690338135\n",
      "Iteration: 6954, Loss: 1.4981616735458374\n",
      "Iteration: 6955, Loss: 1.4981580972671509\n",
      "Iteration: 6956, Loss: 1.4981544017791748\n",
      "Iteration: 6957, Loss: 1.4981507062911987\n",
      "Iteration: 6958, Loss: 1.4981470108032227\n",
      "Iteration: 6959, Loss: 1.4981433153152466\n",
      "Iteration: 6960, Loss: 1.49813973903656\n",
      "Iteration: 6961, Loss: 1.498136043548584\n",
      "Iteration: 6962, Loss: 1.498132348060608\n",
      "Iteration: 6963, Loss: 1.4981287717819214\n",
      "Iteration: 6964, Loss: 1.4981250762939453\n",
      "Iteration: 6965, Loss: 1.4981213808059692\n",
      "Iteration: 6966, Loss: 1.4981178045272827\n",
      "Iteration: 6967, Loss: 1.498113989830017\n",
      "Iteration: 6968, Loss: 1.4981104135513306\n",
      "Iteration: 6969, Loss: 1.4981067180633545\n",
      "Iteration: 6970, Loss: 1.498103141784668\n",
      "Iteration: 6971, Loss: 1.498099446296692\n",
      "Iteration: 6972, Loss: 1.4980958700180054\n",
      "Iteration: 6973, Loss: 1.4980921745300293\n",
      "Iteration: 6974, Loss: 1.4980884790420532\n",
      "Iteration: 6975, Loss: 1.4980849027633667\n",
      "Iteration: 6976, Loss: 1.4980812072753906\n",
      "Iteration: 6977, Loss: 1.498077630996704\n",
      "Iteration: 6978, Loss: 1.4980740547180176\n",
      "Iteration: 6979, Loss: 1.4980703592300415\n",
      "Iteration: 6980, Loss: 1.498066782951355\n",
      "Iteration: 6981, Loss: 1.4980629682540894\n",
      "Iteration: 6982, Loss: 1.4980593919754028\n",
      "Iteration: 6983, Loss: 1.4980558156967163\n",
      "Iteration: 6984, Loss: 1.4980521202087402\n",
      "Iteration: 6985, Loss: 1.4980485439300537\n",
      "Iteration: 6986, Loss: 1.4980449676513672\n",
      "Iteration: 6987, Loss: 1.4980412721633911\n",
      "Iteration: 6988, Loss: 1.4980376958847046\n",
      "Iteration: 6989, Loss: 1.498034119606018\n",
      "Iteration: 6990, Loss: 1.498030424118042\n",
      "Iteration: 6991, Loss: 1.4980268478393555\n",
      "Iteration: 6992, Loss: 1.498023271560669\n",
      "Iteration: 6993, Loss: 1.4980196952819824\n",
      "Iteration: 6994, Loss: 1.4980157613754272\n",
      "Iteration: 6995, Loss: 1.4980121850967407\n",
      "Iteration: 6996, Loss: 1.4980086088180542\n",
      "Iteration: 6997, Loss: 1.4980050325393677\n",
      "Iteration: 6998, Loss: 1.4980014562606812\n",
      "Iteration: 6999, Loss: 1.4979976415634155\n",
      "Iteration: 7000, Loss: 1.497994065284729\n",
      "Iteration: 7001, Loss: 1.4979904890060425\n",
      "Iteration: 7002, Loss: 1.4979867935180664\n",
      "Iteration: 7003, Loss: 1.4979832172393799\n",
      "Iteration: 7004, Loss: 1.4979796409606934\n",
      "Iteration: 7005, Loss: 1.4979759454727173\n",
      "Iteration: 7006, Loss: 1.4979721307754517\n",
      "Iteration: 7007, Loss: 1.4979685544967651\n",
      "Iteration: 7008, Loss: 1.497964859008789\n",
      "Iteration: 7009, Loss: 1.4979612827301025\n",
      "Iteration: 7010, Loss: 1.4979575872421265\n",
      "Iteration: 7011, Loss: 1.49795401096344\n",
      "Iteration: 7012, Loss: 1.4979504346847534\n",
      "Iteration: 7013, Loss: 1.4979465007781982\n",
      "Iteration: 7014, Loss: 1.4979428052902222\n",
      "Iteration: 7015, Loss: 1.4979392290115356\n",
      "Iteration: 7016, Loss: 1.4979355335235596\n",
      "Iteration: 7017, Loss: 1.4979318380355835\n",
      "Iteration: 7018, Loss: 1.4979280233383179\n",
      "Iteration: 7019, Loss: 1.4979243278503418\n",
      "Iteration: 7020, Loss: 1.4979206323623657\n",
      "Iteration: 7021, Loss: 1.4979166984558105\n",
      "Iteration: 7022, Loss: 1.4979130029678345\n",
      "Iteration: 7023, Loss: 1.4979090690612793\n",
      "Iteration: 7024, Loss: 1.4979053735733032\n",
      "Iteration: 7025, Loss: 1.4979013204574585\n",
      "Iteration: 7026, Loss: 1.4978973865509033\n",
      "Iteration: 7027, Loss: 1.4978935718536377\n",
      "Iteration: 7028, Loss: 1.4978893995285034\n",
      "Iteration: 7029, Loss: 1.4978851079940796\n",
      "Iteration: 7030, Loss: 1.4978809356689453\n",
      "Iteration: 7031, Loss: 1.4978766441345215\n",
      "Iteration: 7032, Loss: 1.4978721141815186\n",
      "Iteration: 7033, Loss: 1.4978673458099365\n",
      "Iteration: 7034, Loss: 1.497862696647644\n",
      "Iteration: 7035, Loss: 1.4978581666946411\n",
      "Iteration: 7036, Loss: 1.4978525638580322\n",
      "Iteration: 7037, Loss: 1.4978466033935547\n",
      "Iteration: 7038, Loss: 1.4978408813476562\n",
      "Iteration: 7039, Loss: 1.497833490371704\n",
      "Iteration: 7040, Loss: 1.4978238344192505\n",
      "Iteration: 7041, Loss: 1.4978135824203491\n",
      "Iteration: 7042, Loss: 1.4977976083755493\n",
      "Iteration: 7043, Loss: 1.497772216796875\n",
      "Iteration: 7044, Loss: 1.497731328010559\n",
      "Iteration: 7045, Loss: 1.4976202249526978\n",
      "Iteration: 7046, Loss: 1.4972987174987793\n",
      "Iteration: 7047, Loss: 1.494752049446106\n",
      "Iteration: 7048, Loss: 1.2555075883865356\n",
      "Iteration: 7049, Loss: 1.2468292713165283\n",
      "Iteration: 7050, Loss: 1.2479037046432495\n",
      "Iteration: 7051, Loss: 1.247869610786438\n",
      "Iteration: 7052, Loss: 1.2431378364562988\n",
      "Iteration: 7053, Loss: 1.248288631439209\n",
      "Iteration: 7054, Loss: 1.248638391494751\n",
      "Iteration: 7055, Loss: 1.248795509338379\n",
      "Iteration: 7056, Loss: 1.2489434480667114\n",
      "Iteration: 7057, Loss: 1.249080777168274\n",
      "Iteration: 7058, Loss: 1.2492071390151978\n",
      "Iteration: 7059, Loss: 1.2493228912353516\n",
      "Iteration: 7060, Loss: 1.2494280338287354\n",
      "Iteration: 7061, Loss: 1.2495237588882446\n",
      "Iteration: 7062, Loss: 1.2496107816696167\n",
      "Iteration: 7063, Loss: 1.2496906518936157\n",
      "Iteration: 7064, Loss: 1.2497648000717163\n",
      "Iteration: 7065, Loss: 1.2498359680175781\n",
      "Iteration: 7066, Loss: 1.2499074935913086\n",
      "Iteration: 7067, Loss: 1.2499818801879883\n",
      "Iteration: 7068, Loss: 1.2500591278076172\n",
      "Iteration: 7069, Loss: 1.2501330375671387\n",
      "Iteration: 7070, Loss: 1.2501968145370483\n",
      "Iteration: 7071, Loss: 1.2502378225326538\n",
      "Iteration: 7072, Loss: 1.250253677368164\n",
      "Iteration: 7073, Loss: 1.2502522468566895\n",
      "Iteration: 7074, Loss: 1.2502446174621582\n",
      "Iteration: 7075, Loss: 1.250240683555603\n",
      "Iteration: 7076, Loss: 1.250237226486206\n",
      "Iteration: 7077, Loss: 1.250238060951233\n",
      "Iteration: 7078, Loss: 1.2502418756484985\n",
      "Iteration: 7079, Loss: 1.2502464056015015\n",
      "Iteration: 7080, Loss: 1.2502515316009521\n",
      "Iteration: 7081, Loss: 1.2502567768096924\n",
      "Iteration: 7082, Loss: 1.2502615451812744\n",
      "Iteration: 7083, Loss: 1.2502655982971191\n",
      "Iteration: 7084, Loss: 1.2502689361572266\n",
      "Iteration: 7085, Loss: 1.2502716779708862\n",
      "Iteration: 7086, Loss: 1.2502737045288086\n",
      "Iteration: 7087, Loss: 1.2502750158309937\n",
      "Iteration: 7088, Loss: 1.250275731086731\n",
      "Iteration: 7089, Loss: 1.25027596950531\n",
      "Iteration: 7090, Loss: 1.2502756118774414\n",
      "Iteration: 7091, Loss: 1.2502747774124146\n",
      "Iteration: 7092, Loss: 1.2502734661102295\n",
      "Iteration: 7093, Loss: 1.2502719163894653\n",
      "Iteration: 7094, Loss: 1.250269889831543\n",
      "Iteration: 7095, Loss: 1.2502673864364624\n",
      "Iteration: 7096, Loss: 1.2502648830413818\n",
      "Iteration: 7097, Loss: 1.2502620220184326\n",
      "Iteration: 7098, Loss: 1.2502588033676147\n",
      "Iteration: 7099, Loss: 1.2502553462982178\n",
      "Iteration: 7100, Loss: 1.2502518892288208\n",
      "Iteration: 7101, Loss: 1.2502479553222656\n",
      "Iteration: 7102, Loss: 1.250244140625\n",
      "Iteration: 7103, Loss: 1.2502400875091553\n",
      "Iteration: 7104, Loss: 1.250235915184021\n",
      "Iteration: 7105, Loss: 1.2502316236495972\n",
      "Iteration: 7106, Loss: 1.2502272129058838\n",
      "Iteration: 7107, Loss: 1.2502228021621704\n",
      "Iteration: 7108, Loss: 1.250218152999878\n",
      "Iteration: 7109, Loss: 1.2502135038375854\n",
      "Iteration: 7110, Loss: 1.250208854675293\n",
      "Iteration: 7111, Loss: 1.2502039670944214\n",
      "Iteration: 7112, Loss: 1.2501991987228394\n",
      "Iteration: 7113, Loss: 1.2501943111419678\n",
      "Iteration: 7114, Loss: 1.2501893043518066\n",
      "Iteration: 7115, Loss: 1.250184416770935\n",
      "Iteration: 7116, Loss: 1.250179409980774\n",
      "Iteration: 7117, Loss: 1.2501744031906128\n",
      "Iteration: 7118, Loss: 1.250169277191162\n",
      "Iteration: 7119, Loss: 1.2501641511917114\n",
      "Iteration: 7120, Loss: 1.2501591444015503\n",
      "Iteration: 7121, Loss: 1.2501540184020996\n",
      "Iteration: 7122, Loss: 1.2501487731933594\n",
      "Iteration: 7123, Loss: 1.2501436471939087\n",
      "Iteration: 7124, Loss: 1.250138521194458\n",
      "Iteration: 7125, Loss: 1.2501332759857178\n",
      "Iteration: 7126, Loss: 1.2501282691955566\n",
      "Iteration: 7127, Loss: 1.2501230239868164\n",
      "Iteration: 7128, Loss: 1.2501177787780762\n",
      "Iteration: 7129, Loss: 1.2501126527786255\n",
      "Iteration: 7130, Loss: 1.2501074075698853\n",
      "Iteration: 7131, Loss: 1.250102162361145\n",
      "Iteration: 7132, Loss: 1.2500970363616943\n",
      "Iteration: 7133, Loss: 1.250091791152954\n",
      "Iteration: 7134, Loss: 1.2500865459442139\n",
      "Iteration: 7135, Loss: 1.2500813007354736\n",
      "Iteration: 7136, Loss: 1.250076174736023\n",
      "Iteration: 7137, Loss: 1.2500709295272827\n",
      "Iteration: 7138, Loss: 1.2500656843185425\n",
      "Iteration: 7139, Loss: 1.2500604391098022\n",
      "Iteration: 7140, Loss: 1.250055193901062\n",
      "Iteration: 7141, Loss: 1.2500499486923218\n",
      "Iteration: 7142, Loss: 1.2500447034835815\n",
      "Iteration: 7143, Loss: 1.2500394582748413\n",
      "Iteration: 7144, Loss: 1.250034213066101\n",
      "Iteration: 7145, Loss: 1.2500290870666504\n",
      "Iteration: 7146, Loss: 1.2500238418579102\n",
      "Iteration: 7147, Loss: 1.2500187158584595\n",
      "Iteration: 7148, Loss: 1.2500134706497192\n",
      "Iteration: 7149, Loss: 1.250008225440979\n",
      "Iteration: 7150, Loss: 1.2500029802322388\n",
      "Iteration: 7151, Loss: 1.2499977350234985\n",
      "Iteration: 7152, Loss: 1.2499926090240479\n",
      "Iteration: 7153, Loss: 1.2499873638153076\n",
      "Iteration: 7154, Loss: 1.249982237815857\n",
      "Iteration: 7155, Loss: 1.2499769926071167\n",
      "Iteration: 7156, Loss: 1.249971866607666\n",
      "Iteration: 7157, Loss: 1.2499666213989258\n",
      "Iteration: 7158, Loss: 1.249961495399475\n",
      "Iteration: 7159, Loss: 1.2499563694000244\n",
      "Iteration: 7160, Loss: 1.2499511241912842\n",
      "Iteration: 7161, Loss: 1.2499459981918335\n",
      "Iteration: 7162, Loss: 1.2499407529830933\n",
      "Iteration: 7163, Loss: 1.2499356269836426\n",
      "Iteration: 7164, Loss: 1.2499303817749023\n",
      "Iteration: 7165, Loss: 1.2499253749847412\n",
      "Iteration: 7166, Loss: 1.249920129776001\n",
      "Iteration: 7167, Loss: 1.2499150037765503\n",
      "Iteration: 7168, Loss: 1.2499098777770996\n",
      "Iteration: 7169, Loss: 1.2499046325683594\n",
      "Iteration: 7170, Loss: 1.2498995065689087\n",
      "Iteration: 7171, Loss: 1.2498944997787476\n",
      "Iteration: 7172, Loss: 1.2498892545700073\n",
      "Iteration: 7173, Loss: 1.2498841285705566\n",
      "Iteration: 7174, Loss: 1.249879002571106\n",
      "Iteration: 7175, Loss: 1.2498738765716553\n",
      "Iteration: 7176, Loss: 1.2498687505722046\n",
      "Iteration: 7177, Loss: 1.249863624572754\n",
      "Iteration: 7178, Loss: 1.2498584985733032\n",
      "Iteration: 7179, Loss: 1.2498533725738525\n",
      "Iteration: 7180, Loss: 1.2498482465744019\n",
      "Iteration: 7181, Loss: 1.2498431205749512\n",
      "Iteration: 7182, Loss: 1.2498379945755005\n",
      "Iteration: 7183, Loss: 1.2498328685760498\n",
      "Iteration: 7184, Loss: 1.2498277425765991\n",
      "Iteration: 7185, Loss: 1.249822735786438\n",
      "Iteration: 7186, Loss: 1.2498176097869873\n",
      "Iteration: 7187, Loss: 1.2498126029968262\n",
      "Iteration: 7188, Loss: 1.2498074769973755\n",
      "Iteration: 7189, Loss: 1.2498023509979248\n",
      "Iteration: 7190, Loss: 1.2497972249984741\n",
      "Iteration: 7191, Loss: 1.249792218208313\n",
      "Iteration: 7192, Loss: 1.2497872114181519\n",
      "Iteration: 7193, Loss: 1.2497820854187012\n",
      "Iteration: 7194, Loss: 1.2497769594192505\n",
      "Iteration: 7195, Loss: 1.2497719526290894\n",
      "Iteration: 7196, Loss: 1.2497669458389282\n",
      "Iteration: 7197, Loss: 1.2497618198394775\n",
      "Iteration: 7198, Loss: 1.2497568130493164\n",
      "Iteration: 7199, Loss: 1.2497516870498657\n",
      "Iteration: 7200, Loss: 1.2497466802597046\n",
      "Iteration: 7201, Loss: 1.249741554260254\n",
      "Iteration: 7202, Loss: 1.2497365474700928\n",
      "Iteration: 7203, Loss: 1.2497315406799316\n",
      "Iteration: 7204, Loss: 1.2497265338897705\n",
      "Iteration: 7205, Loss: 1.2497214078903198\n",
      "Iteration: 7206, Loss: 1.2497164011001587\n",
      "Iteration: 7207, Loss: 1.2497113943099976\n",
      "Iteration: 7208, Loss: 1.2497062683105469\n",
      "Iteration: 7209, Loss: 1.2497012615203857\n",
      "Iteration: 7210, Loss: 1.2496962547302246\n",
      "Iteration: 7211, Loss: 1.2496912479400635\n",
      "Iteration: 7212, Loss: 1.2496862411499023\n",
      "Iteration: 7213, Loss: 1.2496812343597412\n",
      "Iteration: 7214, Loss: 1.24967622756958\n",
      "Iteration: 7215, Loss: 1.249671220779419\n",
      "Iteration: 7216, Loss: 1.2496662139892578\n",
      "Iteration: 7217, Loss: 1.2496612071990967\n",
      "Iteration: 7218, Loss: 1.2496562004089355\n",
      "Iteration: 7219, Loss: 1.2496511936187744\n",
      "Iteration: 7220, Loss: 1.2496461868286133\n",
      "Iteration: 7221, Loss: 1.2496411800384521\n",
      "Iteration: 7222, Loss: 1.249636173248291\n",
      "Iteration: 7223, Loss: 1.2496312856674194\n",
      "Iteration: 7224, Loss: 1.2496262788772583\n",
      "Iteration: 7225, Loss: 1.2496212720870972\n",
      "Iteration: 7226, Loss: 1.2496163845062256\n",
      "Iteration: 7227, Loss: 1.2496113777160645\n",
      "Iteration: 7228, Loss: 1.2496063709259033\n",
      "Iteration: 7229, Loss: 1.2496014833450317\n",
      "Iteration: 7230, Loss: 1.2495964765548706\n",
      "Iteration: 7231, Loss: 1.2495914697647095\n",
      "Iteration: 7232, Loss: 1.249586582183838\n",
      "Iteration: 7233, Loss: 1.2495815753936768\n",
      "Iteration: 7234, Loss: 1.2495765686035156\n",
      "Iteration: 7235, Loss: 1.249571681022644\n",
      "Iteration: 7236, Loss: 1.249566674232483\n",
      "Iteration: 7237, Loss: 1.2495616674423218\n",
      "Iteration: 7238, Loss: 1.2495568990707397\n",
      "Iteration: 7239, Loss: 1.2495518922805786\n",
      "Iteration: 7240, Loss: 1.249547004699707\n",
      "Iteration: 7241, Loss: 1.249541997909546\n",
      "Iteration: 7242, Loss: 1.2495371103286743\n",
      "Iteration: 7243, Loss: 1.2495321035385132\n",
      "Iteration: 7244, Loss: 1.2495273351669312\n",
      "Iteration: 7245, Loss: 1.24952232837677\n",
      "Iteration: 7246, Loss: 1.2495174407958984\n",
      "Iteration: 7247, Loss: 1.2495125532150269\n",
      "Iteration: 7248, Loss: 1.2495075464248657\n",
      "Iteration: 7249, Loss: 1.2495026588439941\n",
      "Iteration: 7250, Loss: 1.2494977712631226\n",
      "Iteration: 7251, Loss: 1.249492883682251\n",
      "Iteration: 7252, Loss: 1.2494879961013794\n",
      "Iteration: 7253, Loss: 1.2494831085205078\n",
      "Iteration: 7254, Loss: 1.2494782209396362\n",
      "Iteration: 7255, Loss: 1.2494733333587646\n",
      "Iteration: 7256, Loss: 1.2494683265686035\n",
      "Iteration: 7257, Loss: 1.249463438987732\n",
      "Iteration: 7258, Loss: 1.2494585514068604\n",
      "Iteration: 7259, Loss: 1.2494537830352783\n",
      "Iteration: 7260, Loss: 1.2494488954544067\n",
      "Iteration: 7261, Loss: 1.2494440078735352\n",
      "Iteration: 7262, Loss: 1.2494391202926636\n",
      "Iteration: 7263, Loss: 1.249434232711792\n",
      "Iteration: 7264, Loss: 1.2494293451309204\n",
      "Iteration: 7265, Loss: 1.2494244575500488\n",
      "Iteration: 7266, Loss: 1.2494195699691772\n",
      "Iteration: 7267, Loss: 1.2494146823883057\n",
      "Iteration: 7268, Loss: 1.2494099140167236\n",
      "Iteration: 7269, Loss: 1.2494051456451416\n",
      "Iteration: 7270, Loss: 1.24940025806427\n",
      "Iteration: 7271, Loss: 1.2493953704833984\n",
      "Iteration: 7272, Loss: 1.2493904829025269\n",
      "Iteration: 7273, Loss: 1.2493857145309448\n",
      "Iteration: 7274, Loss: 1.2493810653686523\n",
      "Iteration: 7275, Loss: 1.2493761777877808\n",
      "Iteration: 7276, Loss: 1.2493712902069092\n",
      "Iteration: 7277, Loss: 1.2493665218353271\n",
      "Iteration: 7278, Loss: 1.2493616342544556\n",
      "Iteration: 7279, Loss: 1.249356746673584\n",
      "Iteration: 7280, Loss: 1.249351978302002\n",
      "Iteration: 7281, Loss: 1.2493470907211304\n",
      "Iteration: 7282, Loss: 1.2493423223495483\n",
      "Iteration: 7283, Loss: 1.2493374347686768\n",
      "Iteration: 7284, Loss: 1.2493326663970947\n",
      "Iteration: 7285, Loss: 1.2493278980255127\n",
      "Iteration: 7286, Loss: 1.2493231296539307\n",
      "Iteration: 7287, Loss: 1.2493183612823486\n",
      "Iteration: 7288, Loss: 1.249313473701477\n",
      "Iteration: 7289, Loss: 1.249308705329895\n",
      "Iteration: 7290, Loss: 1.249303936958313\n",
      "Iteration: 7291, Loss: 1.2492990493774414\n",
      "Iteration: 7292, Loss: 1.2492942810058594\n",
      "Iteration: 7293, Loss: 1.2492895126342773\n",
      "Iteration: 7294, Loss: 1.2492846250534058\n",
      "Iteration: 7295, Loss: 1.2492799758911133\n",
      "Iteration: 7296, Loss: 1.2492752075195312\n",
      "Iteration: 7297, Loss: 1.2492704391479492\n",
      "Iteration: 7298, Loss: 1.2492656707763672\n",
      "Iteration: 7299, Loss: 1.2492609024047852\n",
      "Iteration: 7300, Loss: 1.2492560148239136\n",
      "Iteration: 7301, Loss: 1.2492512464523315\n",
      "Iteration: 7302, Loss: 1.2492464780807495\n",
      "Iteration: 7303, Loss: 1.2492417097091675\n",
      "Iteration: 7304, Loss: 1.2492369413375854\n",
      "Iteration: 7305, Loss: 1.249232292175293\n",
      "Iteration: 7306, Loss: 1.249227523803711\n",
      "Iteration: 7307, Loss: 1.249222755432129\n",
      "Iteration: 7308, Loss: 1.2492179870605469\n",
      "Iteration: 7309, Loss: 1.2492132186889648\n",
      "Iteration: 7310, Loss: 1.2492085695266724\n",
      "Iteration: 7311, Loss: 1.2492038011550903\n",
      "Iteration: 7312, Loss: 1.2491990327835083\n",
      "Iteration: 7313, Loss: 1.2491942644119263\n",
      "Iteration: 7314, Loss: 1.2491894960403442\n",
      "Iteration: 7315, Loss: 1.2491848468780518\n",
      "Iteration: 7316, Loss: 1.2491801977157593\n",
      "Iteration: 7317, Loss: 1.2491754293441772\n",
      "Iteration: 7318, Loss: 1.2491706609725952\n",
      "Iteration: 7319, Loss: 1.2491661310195923\n",
      "Iteration: 7320, Loss: 1.2491613626480103\n",
      "Iteration: 7321, Loss: 1.2491565942764282\n",
      "Iteration: 7322, Loss: 1.2491519451141357\n",
      "Iteration: 7323, Loss: 1.2491471767425537\n",
      "Iteration: 7324, Loss: 1.2491425275802612\n",
      "Iteration: 7325, Loss: 1.2491377592086792\n",
      "Iteration: 7326, Loss: 1.2491332292556763\n",
      "Iteration: 7327, Loss: 1.2491284608840942\n",
      "Iteration: 7328, Loss: 1.2491238117218018\n",
      "Iteration: 7329, Loss: 1.2491190433502197\n",
      "Iteration: 7330, Loss: 1.2491143941879272\n",
      "Iteration: 7331, Loss: 1.2491096258163452\n",
      "Iteration: 7332, Loss: 1.2491049766540527\n",
      "Iteration: 7333, Loss: 1.2491003274917603\n",
      "Iteration: 7334, Loss: 1.2490955591201782\n",
      "Iteration: 7335, Loss: 1.2490909099578857\n",
      "Iteration: 7336, Loss: 1.2490863800048828\n",
      "Iteration: 7337, Loss: 1.2490816116333008\n",
      "Iteration: 7338, Loss: 1.2490768432617188\n",
      "Iteration: 7339, Loss: 1.2490721940994263\n",
      "Iteration: 7340, Loss: 1.2490676641464233\n",
      "Iteration: 7341, Loss: 1.2490630149841309\n",
      "Iteration: 7342, Loss: 1.2490583658218384\n",
      "Iteration: 7343, Loss: 1.2490535974502563\n",
      "Iteration: 7344, Loss: 1.2490489482879639\n",
      "Iteration: 7345, Loss: 1.2490442991256714\n",
      "Iteration: 7346, Loss: 1.249039649963379\n",
      "Iteration: 7347, Loss: 1.2490350008010864\n",
      "Iteration: 7348, Loss: 1.249030351638794\n",
      "Iteration: 7349, Loss: 1.2490257024765015\n",
      "Iteration: 7350, Loss: 1.249021053314209\n",
      "Iteration: 7351, Loss: 1.249016523361206\n",
      "Iteration: 7352, Loss: 1.2490118741989136\n",
      "Iteration: 7353, Loss: 1.249007225036621\n",
      "Iteration: 7354, Loss: 1.2490025758743286\n",
      "Iteration: 7355, Loss: 1.2489980459213257\n",
      "Iteration: 7356, Loss: 1.2489933967590332\n",
      "Iteration: 7357, Loss: 1.2489887475967407\n",
      "Iteration: 7358, Loss: 1.2489840984344482\n",
      "Iteration: 7359, Loss: 1.2489794492721558\n",
      "Iteration: 7360, Loss: 1.2489750385284424\n",
      "Iteration: 7361, Loss: 1.24897038936615\n",
      "Iteration: 7362, Loss: 1.2489657402038574\n",
      "Iteration: 7363, Loss: 1.248961091041565\n",
      "Iteration: 7364, Loss: 1.248956561088562\n",
      "Iteration: 7365, Loss: 1.2489519119262695\n",
      "Iteration: 7366, Loss: 1.2489473819732666\n",
      "Iteration: 7367, Loss: 1.2489428520202637\n",
      "Iteration: 7368, Loss: 1.2489382028579712\n",
      "Iteration: 7369, Loss: 1.2489336729049683\n",
      "Iteration: 7370, Loss: 1.2489290237426758\n",
      "Iteration: 7371, Loss: 1.2489246129989624\n",
      "Iteration: 7372, Loss: 1.24891996383667\n",
      "Iteration: 7373, Loss: 1.248915433883667\n",
      "Iteration: 7374, Loss: 1.2489107847213745\n",
      "Iteration: 7375, Loss: 1.2489063739776611\n",
      "Iteration: 7376, Loss: 1.2489018440246582\n",
      "Iteration: 7377, Loss: 1.2488973140716553\n",
      "Iteration: 7378, Loss: 1.2488927841186523\n",
      "Iteration: 7379, Loss: 1.2488882541656494\n",
      "Iteration: 7380, Loss: 1.248883605003357\n",
      "Iteration: 7381, Loss: 1.248879075050354\n",
      "Iteration: 7382, Loss: 1.248874545097351\n",
      "Iteration: 7383, Loss: 1.2488700151443481\n",
      "Iteration: 7384, Loss: 1.2488653659820557\n",
      "Iteration: 7385, Loss: 1.2488608360290527\n",
      "Iteration: 7386, Loss: 1.2488563060760498\n",
      "Iteration: 7387, Loss: 1.2488517761230469\n",
      "Iteration: 7388, Loss: 1.248847246170044\n",
      "Iteration: 7389, Loss: 1.248842716217041\n",
      "Iteration: 7390, Loss: 1.248838186264038\n",
      "Iteration: 7391, Loss: 1.2488337755203247\n",
      "Iteration: 7392, Loss: 1.2488292455673218\n",
      "Iteration: 7393, Loss: 1.2488247156143188\n",
      "Iteration: 7394, Loss: 1.248820185661316\n",
      "Iteration: 7395, Loss: 1.248815655708313\n",
      "Iteration: 7396, Loss: 1.24881112575531\n",
      "Iteration: 7397, Loss: 1.2488065958023071\n",
      "Iteration: 7398, Loss: 1.2488020658493042\n",
      "Iteration: 7399, Loss: 1.2487976551055908\n",
      "Iteration: 7400, Loss: 1.248793125152588\n",
      "Iteration: 7401, Loss: 1.248788595199585\n",
      "Iteration: 7402, Loss: 1.248784065246582\n",
      "Iteration: 7403, Loss: 1.2487796545028687\n",
      "Iteration: 7404, Loss: 1.2487752437591553\n",
      "Iteration: 7405, Loss: 1.2487707138061523\n",
      "Iteration: 7406, Loss: 1.2487661838531494\n",
      "Iteration: 7407, Loss: 1.248761773109436\n",
      "Iteration: 7408, Loss: 1.248757243156433\n",
      "Iteration: 7409, Loss: 1.2487527132034302\n",
      "Iteration: 7410, Loss: 1.2487483024597168\n",
      "Iteration: 7411, Loss: 1.2487437725067139\n",
      "Iteration: 7412, Loss: 1.2487393617630005\n",
      "Iteration: 7413, Loss: 1.2487348318099976\n",
      "Iteration: 7414, Loss: 1.2487304210662842\n",
      "Iteration: 7415, Loss: 1.2487258911132812\n",
      "Iteration: 7416, Loss: 1.2487215995788574\n",
      "Iteration: 7417, Loss: 1.2487170696258545\n",
      "Iteration: 7418, Loss: 1.2487126588821411\n",
      "Iteration: 7419, Loss: 1.2487081289291382\n",
      "Iteration: 7420, Loss: 1.2487037181854248\n",
      "Iteration: 7421, Loss: 1.2486993074417114\n",
      "Iteration: 7422, Loss: 1.2486947774887085\n",
      "Iteration: 7423, Loss: 1.2486903667449951\n",
      "Iteration: 7424, Loss: 1.2486859560012817\n",
      "Iteration: 7425, Loss: 1.2486814260482788\n",
      "Iteration: 7426, Loss: 1.2486770153045654\n",
      "Iteration: 7427, Loss: 1.2486727237701416\n",
      "Iteration: 7428, Loss: 1.2486683130264282\n",
      "Iteration: 7429, Loss: 1.2486639022827148\n",
      "Iteration: 7430, Loss: 1.248659372329712\n",
      "Iteration: 7431, Loss: 1.2486549615859985\n",
      "Iteration: 7432, Loss: 1.2486505508422852\n",
      "Iteration: 7433, Loss: 1.2486461400985718\n",
      "Iteration: 7434, Loss: 1.2486417293548584\n",
      "Iteration: 7435, Loss: 1.248637318611145\n",
      "Iteration: 7436, Loss: 1.2486329078674316\n",
      "Iteration: 7437, Loss: 1.2486284971237183\n",
      "Iteration: 7438, Loss: 1.2486242055892944\n",
      "Iteration: 7439, Loss: 1.248619794845581\n",
      "Iteration: 7440, Loss: 1.2486153841018677\n",
      "Iteration: 7441, Loss: 1.2486109733581543\n",
      "Iteration: 7442, Loss: 1.248606562614441\n",
      "Iteration: 7443, Loss: 1.2486021518707275\n",
      "Iteration: 7444, Loss: 1.2485977411270142\n",
      "Iteration: 7445, Loss: 1.2485933303833008\n",
      "Iteration: 7446, Loss: 1.2485891580581665\n",
      "Iteration: 7447, Loss: 1.2485847473144531\n",
      "Iteration: 7448, Loss: 1.2485803365707397\n",
      "Iteration: 7449, Loss: 1.2485759258270264\n",
      "Iteration: 7450, Loss: 1.2485716342926025\n",
      "Iteration: 7451, Loss: 1.2485673427581787\n",
      "Iteration: 7452, Loss: 1.2485629320144653\n",
      "Iteration: 7453, Loss: 1.248558521270752\n",
      "Iteration: 7454, Loss: 1.2485541105270386\n",
      "Iteration: 7455, Loss: 1.2485498189926147\n",
      "Iteration: 7456, Loss: 1.248545527458191\n",
      "Iteration: 7457, Loss: 1.2485411167144775\n",
      "Iteration: 7458, Loss: 1.2485368251800537\n",
      "Iteration: 7459, Loss: 1.2485324144363403\n",
      "Iteration: 7460, Loss: 1.2485281229019165\n",
      "Iteration: 7461, Loss: 1.2485237121582031\n",
      "Iteration: 7462, Loss: 1.2485194206237793\n",
      "Iteration: 7463, Loss: 1.248515009880066\n",
      "Iteration: 7464, Loss: 1.2485108375549316\n",
      "Iteration: 7465, Loss: 1.2485065460205078\n",
      "Iteration: 7466, Loss: 1.2485021352767944\n",
      "Iteration: 7467, Loss: 1.2484978437423706\n",
      "Iteration: 7468, Loss: 1.2484935522079468\n",
      "Iteration: 7469, Loss: 1.2484891414642334\n",
      "Iteration: 7470, Loss: 1.2484848499298096\n",
      "Iteration: 7471, Loss: 1.2484806776046753\n",
      "Iteration: 7472, Loss: 1.248476266860962\n",
      "Iteration: 7473, Loss: 1.248471975326538\n",
      "Iteration: 7474, Loss: 1.2484676837921143\n",
      "Iteration: 7475, Loss: 1.2484633922576904\n",
      "Iteration: 7476, Loss: 1.2484592199325562\n",
      "Iteration: 7477, Loss: 1.2484549283981323\n",
      "Iteration: 7478, Loss: 1.2484506368637085\n",
      "Iteration: 7479, Loss: 1.2484463453292847\n",
      "Iteration: 7480, Loss: 1.2484419345855713\n",
      "Iteration: 7481, Loss: 1.2484376430511475\n",
      "Iteration: 7482, Loss: 1.2484334707260132\n",
      "Iteration: 7483, Loss: 1.2484291791915894\n",
      "Iteration: 7484, Loss: 1.248425006866455\n",
      "Iteration: 7485, Loss: 1.2484207153320312\n",
      "Iteration: 7486, Loss: 1.2484164237976074\n",
      "Iteration: 7487, Loss: 1.2484121322631836\n",
      "Iteration: 7488, Loss: 1.2484078407287598\n",
      "Iteration: 7489, Loss: 1.2484036684036255\n",
      "Iteration: 7490, Loss: 1.2483993768692017\n",
      "Iteration: 7491, Loss: 1.2483952045440674\n",
      "Iteration: 7492, Loss: 1.2483909130096436\n",
      "Iteration: 7493, Loss: 1.2483866214752197\n",
      "Iteration: 7494, Loss: 1.248382329940796\n",
      "Iteration: 7495, Loss: 1.2483781576156616\n",
      "Iteration: 7496, Loss: 1.2483738660812378\n",
      "Iteration: 7497, Loss: 1.248369574546814\n",
      "Iteration: 7498, Loss: 1.2483654022216797\n",
      "Iteration: 7499, Loss: 1.2483611106872559\n",
      "Iteration: 7500, Loss: 1.2483569383621216\n",
      "Iteration: 7501, Loss: 1.2483527660369873\n",
      "Iteration: 7502, Loss: 1.2483484745025635\n",
      "Iteration: 7503, Loss: 1.2483443021774292\n",
      "Iteration: 7504, Loss: 1.2483400106430054\n",
      "Iteration: 7505, Loss: 1.248335838317871\n",
      "Iteration: 7506, Loss: 1.2483316659927368\n",
      "Iteration: 7507, Loss: 1.2483274936676025\n",
      "Iteration: 7508, Loss: 1.2483233213424683\n",
      "Iteration: 7509, Loss: 1.2483190298080444\n",
      "Iteration: 7510, Loss: 1.2483148574829102\n",
      "Iteration: 7511, Loss: 1.2483106851577759\n",
      "Iteration: 7512, Loss: 1.2483065128326416\n",
      "Iteration: 7513, Loss: 1.2483023405075073\n",
      "Iteration: 7514, Loss: 1.248298168182373\n",
      "Iteration: 7515, Loss: 1.2482938766479492\n",
      "Iteration: 7516, Loss: 1.248289704322815\n",
      "Iteration: 7517, Loss: 1.2482856512069702\n",
      "Iteration: 7518, Loss: 1.248281478881836\n",
      "Iteration: 7519, Loss: 1.2482774257659912\n",
      "Iteration: 7520, Loss: 1.248273253440857\n",
      "Iteration: 7521, Loss: 1.2482690811157227\n",
      "Iteration: 7522, Loss: 1.2482649087905884\n",
      "Iteration: 7523, Loss: 1.248260736465454\n",
      "Iteration: 7524, Loss: 1.2482565641403198\n",
      "Iteration: 7525, Loss: 1.2482523918151855\n",
      "Iteration: 7526, Loss: 1.2482482194900513\n",
      "Iteration: 7527, Loss: 1.248244047164917\n",
      "Iteration: 7528, Loss: 1.2482398748397827\n",
      "Iteration: 7529, Loss: 1.248235821723938\n",
      "Iteration: 7530, Loss: 1.2482316493988037\n",
      "Iteration: 7531, Loss: 1.2482274770736694\n",
      "Iteration: 7532, Loss: 1.2482233047485352\n",
      "Iteration: 7533, Loss: 1.2482192516326904\n",
      "Iteration: 7534, Loss: 1.2482150793075562\n",
      "Iteration: 7535, Loss: 1.2482109069824219\n",
      "Iteration: 7536, Loss: 1.2482067346572876\n",
      "Iteration: 7537, Loss: 1.2482026815414429\n",
      "Iteration: 7538, Loss: 1.2481985092163086\n",
      "Iteration: 7539, Loss: 1.2481944561004639\n",
      "Iteration: 7540, Loss: 1.2481904029846191\n",
      "Iteration: 7541, Loss: 1.2481863498687744\n",
      "Iteration: 7542, Loss: 1.2481821775436401\n",
      "Iteration: 7543, Loss: 1.2481781244277954\n",
      "Iteration: 7544, Loss: 1.2481739521026611\n",
      "Iteration: 7545, Loss: 1.2481698989868164\n",
      "Iteration: 7546, Loss: 1.2481657266616821\n",
      "Iteration: 7547, Loss: 1.2481616735458374\n",
      "Iteration: 7548, Loss: 1.2481576204299927\n",
      "Iteration: 7549, Loss: 1.248153567314148\n",
      "Iteration: 7550, Loss: 1.2481495141983032\n",
      "Iteration: 7551, Loss: 1.248145341873169\n",
      "Iteration: 7552, Loss: 1.2481412887573242\n",
      "Iteration: 7553, Loss: 1.2481372356414795\n",
      "Iteration: 7554, Loss: 1.2481330633163452\n",
      "Iteration: 7555, Loss: 1.2481290102005005\n",
      "Iteration: 7556, Loss: 1.2481249570846558\n",
      "Iteration: 7557, Loss: 1.248120903968811\n",
      "Iteration: 7558, Loss: 1.2481168508529663\n",
      "Iteration: 7559, Loss: 1.248112678527832\n",
      "Iteration: 7560, Loss: 1.2481087446212769\n",
      "Iteration: 7561, Loss: 1.2481046915054321\n",
      "Iteration: 7562, Loss: 1.2481006383895874\n",
      "Iteration: 7563, Loss: 1.2480965852737427\n",
      "Iteration: 7564, Loss: 1.248092532157898\n",
      "Iteration: 7565, Loss: 1.2480884790420532\n",
      "Iteration: 7566, Loss: 1.2480844259262085\n",
      "Iteration: 7567, Loss: 1.2480803728103638\n",
      "Iteration: 7568, Loss: 1.248076319694519\n",
      "Iteration: 7569, Loss: 1.2480722665786743\n",
      "Iteration: 7570, Loss: 1.2480682134628296\n",
      "Iteration: 7571, Loss: 1.2480641603469849\n",
      "Iteration: 7572, Loss: 1.2480601072311401\n",
      "Iteration: 7573, Loss: 1.2480562925338745\n",
      "Iteration: 7574, Loss: 1.2480522394180298\n",
      "Iteration: 7575, Loss: 1.2480483055114746\n",
      "Iteration: 7576, Loss: 1.2480442523956299\n",
      "Iteration: 7577, Loss: 1.2480401992797852\n",
      "Iteration: 7578, Loss: 1.2480361461639404\n",
      "Iteration: 7579, Loss: 1.2480322122573853\n",
      "Iteration: 7580, Loss: 1.2480281591415405\n",
      "Iteration: 7581, Loss: 1.2480241060256958\n",
      "Iteration: 7582, Loss: 1.2480201721191406\n",
      "Iteration: 7583, Loss: 1.248016357421875\n",
      "Iteration: 7584, Loss: 1.2480123043060303\n",
      "Iteration: 7585, Loss: 1.248008370399475\n",
      "Iteration: 7586, Loss: 1.2480043172836304\n",
      "Iteration: 7587, Loss: 1.2480003833770752\n",
      "Iteration: 7588, Loss: 1.2479963302612305\n",
      "Iteration: 7589, Loss: 1.2479923963546753\n",
      "Iteration: 7590, Loss: 1.2479883432388306\n",
      "Iteration: 7591, Loss: 1.2479844093322754\n",
      "Iteration: 7592, Loss: 1.2479805946350098\n",
      "Iteration: 7593, Loss: 1.247976541519165\n",
      "Iteration: 7594, Loss: 1.2479726076126099\n",
      "Iteration: 7595, Loss: 1.2479685544967651\n",
      "Iteration: 7596, Loss: 1.24796462059021\n",
      "Iteration: 7597, Loss: 1.2479606866836548\n",
      "Iteration: 7598, Loss: 1.2479567527770996\n",
      "Iteration: 7599, Loss: 1.2479526996612549\n",
      "Iteration: 7600, Loss: 1.2479487657546997\n",
      "Iteration: 7601, Loss: 1.2479448318481445\n",
      "Iteration: 7602, Loss: 1.2479408979415894\n",
      "Iteration: 7603, Loss: 1.2479369640350342\n",
      "Iteration: 7604, Loss: 1.247933030128479\n",
      "Iteration: 7605, Loss: 1.2479289770126343\n",
      "Iteration: 7606, Loss: 1.247925043106079\n",
      "Iteration: 7607, Loss: 1.247921109199524\n",
      "Iteration: 7608, Loss: 1.2479171752929688\n",
      "Iteration: 7609, Loss: 1.2479133605957031\n",
      "Iteration: 7610, Loss: 1.2479095458984375\n",
      "Iteration: 7611, Loss: 1.2479056119918823\n",
      "Iteration: 7612, Loss: 1.2479016780853271\n",
      "Iteration: 7613, Loss: 1.247897744178772\n",
      "Iteration: 7614, Loss: 1.2478939294815063\n",
      "Iteration: 7615, Loss: 1.2478899955749512\n",
      "Iteration: 7616, Loss: 1.247886061668396\n",
      "Iteration: 7617, Loss: 1.2478821277618408\n",
      "Iteration: 7618, Loss: 1.2478781938552856\n",
      "Iteration: 7619, Loss: 1.2478742599487305\n",
      "Iteration: 7620, Loss: 1.2478704452514648\n",
      "Iteration: 7621, Loss: 1.2478665113449097\n",
      "Iteration: 7622, Loss: 1.2478625774383545\n",
      "Iteration: 7623, Loss: 1.2478587627410889\n",
      "Iteration: 7624, Loss: 1.2478549480438232\n",
      "Iteration: 7625, Loss: 1.247851014137268\n",
      "Iteration: 7626, Loss: 1.2478471994400024\n",
      "Iteration: 7627, Loss: 1.2478432655334473\n",
      "Iteration: 7628, Loss: 1.247839331626892\n",
      "Iteration: 7629, Loss: 1.2478355169296265\n",
      "Iteration: 7630, Loss: 1.2478315830230713\n",
      "Iteration: 7631, Loss: 1.2478277683258057\n",
      "Iteration: 7632, Loss: 1.2478240728378296\n",
      "Iteration: 7633, Loss: 1.247820258140564\n",
      "Iteration: 7634, Loss: 1.2478164434432983\n",
      "Iteration: 7635, Loss: 1.2478125095367432\n",
      "Iteration: 7636, Loss: 1.2478086948394775\n",
      "Iteration: 7637, Loss: 1.2478047609329224\n",
      "Iteration: 7638, Loss: 1.2478009462356567\n",
      "Iteration: 7639, Loss: 1.2477971315383911\n",
      "Iteration: 7640, Loss: 1.247793197631836\n",
      "Iteration: 7641, Loss: 1.2477893829345703\n",
      "Iteration: 7642, Loss: 1.2477856874465942\n",
      "Iteration: 7643, Loss: 1.2477816343307495\n",
      "Iteration: 7644, Loss: 1.2477779388427734\n",
      "Iteration: 7645, Loss: 1.2477742433547974\n",
      "Iteration: 7646, Loss: 1.2477704286575317\n",
      "Iteration: 7647, Loss: 1.2477666139602661\n",
      "Iteration: 7648, Loss: 1.2477627992630005\n",
      "Iteration: 7649, Loss: 1.2477589845657349\n",
      "Iteration: 7650, Loss: 1.2477551698684692\n",
      "Iteration: 7651, Loss: 1.2477513551712036\n",
      "Iteration: 7652, Loss: 1.247747540473938\n",
      "Iteration: 7653, Loss: 1.247743844985962\n",
      "Iteration: 7654, Loss: 1.2477400302886963\n",
      "Iteration: 7655, Loss: 1.2477362155914307\n",
      "Iteration: 7656, Loss: 1.247732162475586\n",
      "Iteration: 7657, Loss: 1.2477285861968994\n",
      "Iteration: 7658, Loss: 1.2477246522903442\n",
      "Iteration: 7659, Loss: 1.2477209568023682\n",
      "Iteration: 7660, Loss: 1.2477171421051025\n",
      "Iteration: 7661, Loss: 1.247713327407837\n",
      "Iteration: 7662, Loss: 1.2477095127105713\n",
      "Iteration: 7663, Loss: 1.2477056980133057\n",
      "Iteration: 7664, Loss: 1.2477021217346191\n",
      "Iteration: 7665, Loss: 1.2476983070373535\n",
      "Iteration: 7666, Loss: 1.247694492340088\n",
      "Iteration: 7667, Loss: 1.2476907968521118\n",
      "Iteration: 7668, Loss: 1.2476869821548462\n",
      "Iteration: 7669, Loss: 1.2476831674575806\n",
      "Iteration: 7670, Loss: 1.2476794719696045\n",
      "Iteration: 7671, Loss: 1.2476756572723389\n",
      "Iteration: 7672, Loss: 1.2476719617843628\n",
      "Iteration: 7673, Loss: 1.2476681470870972\n",
      "Iteration: 7674, Loss: 1.2476643323898315\n",
      "Iteration: 7675, Loss: 1.2476606369018555\n",
      "Iteration: 7676, Loss: 1.2476568222045898\n",
      "Iteration: 7677, Loss: 1.2476531267166138\n",
      "Iteration: 7678, Loss: 1.2476494312286377\n",
      "Iteration: 7679, Loss: 1.247645616531372\n",
      "Iteration: 7680, Loss: 1.247641921043396\n",
      "Iteration: 7681, Loss: 1.2476381063461304\n",
      "Iteration: 7682, Loss: 1.2476342916488647\n",
      "Iteration: 7683, Loss: 1.2476307153701782\n",
      "Iteration: 7684, Loss: 1.247626781463623\n",
      "Iteration: 7685, Loss: 1.247623085975647\n",
      "Iteration: 7686, Loss: 1.247619390487671\n",
      "Iteration: 7687, Loss: 1.2476154565811157\n",
      "Iteration: 7688, Loss: 1.2476118803024292\n",
      "Iteration: 7689, Loss: 1.2476081848144531\n",
      "Iteration: 7690, Loss: 1.247604489326477\n",
      "Iteration: 7691, Loss: 1.2476006746292114\n",
      "Iteration: 7692, Loss: 1.2475969791412354\n",
      "Iteration: 7693, Loss: 1.2475931644439697\n",
      "Iteration: 7694, Loss: 1.2475895881652832\n",
      "Iteration: 7695, Loss: 1.247585654258728\n",
      "Iteration: 7696, Loss: 1.2475818395614624\n",
      "Iteration: 7697, Loss: 1.2475781440734863\n",
      "Iteration: 7698, Loss: 1.2475743293762207\n",
      "Iteration: 7699, Loss: 1.2475706338882446\n",
      "Iteration: 7700, Loss: 1.2475669384002686\n",
      "Iteration: 7701, Loss: 1.2475632429122925\n",
      "Iteration: 7702, Loss: 1.2475595474243164\n",
      "Iteration: 7703, Loss: 1.2475558519363403\n",
      "Iteration: 7704, Loss: 1.2475521564483643\n",
      "Iteration: 7705, Loss: 1.2475483417510986\n",
      "Iteration: 7706, Loss: 1.247544527053833\n",
      "Iteration: 7707, Loss: 1.247540831565857\n",
      "Iteration: 7708, Loss: 1.2475370168685913\n",
      "Iteration: 7709, Loss: 1.2475332021713257\n",
      "Iteration: 7710, Loss: 1.24752938747406\n",
      "Iteration: 7711, Loss: 1.2475255727767944\n",
      "Iteration: 7712, Loss: 1.247521996498108\n",
      "Iteration: 7713, Loss: 1.2475180625915527\n",
      "Iteration: 7714, Loss: 1.247514247894287\n",
      "Iteration: 7715, Loss: 1.2475104331970215\n",
      "Iteration: 7716, Loss: 1.2475064992904663\n",
      "Iteration: 7717, Loss: 1.2475028038024902\n",
      "Iteration: 7718, Loss: 1.2474989891052246\n",
      "Iteration: 7719, Loss: 1.2474952936172485\n",
      "Iteration: 7720, Loss: 1.2474913597106934\n",
      "Iteration: 7721, Loss: 1.2474874258041382\n",
      "Iteration: 7722, Loss: 1.2474836111068726\n",
      "Iteration: 7723, Loss: 1.2474795579910278\n",
      "Iteration: 7724, Loss: 1.2474757432937622\n",
      "Iteration: 7725, Loss: 1.2474716901779175\n",
      "Iteration: 7726, Loss: 1.2474677562713623\n",
      "Iteration: 7727, Loss: 1.2474637031555176\n",
      "Iteration: 7728, Loss: 1.2474596500396729\n",
      "Iteration: 7729, Loss: 1.2474559545516968\n",
      "Iteration: 7730, Loss: 1.2474520206451416\n",
      "Iteration: 7731, Loss: 1.2474478483200073\n",
      "Iteration: 7732, Loss: 1.2474437952041626\n",
      "Iteration: 7733, Loss: 1.2474396228790283\n",
      "Iteration: 7734, Loss: 1.247435212135315\n",
      "Iteration: 7735, Loss: 1.2474310398101807\n",
      "Iteration: 7736, Loss: 1.2474265098571777\n",
      "Iteration: 7737, Loss: 1.247422218322754\n",
      "Iteration: 7738, Loss: 1.2474175691604614\n",
      "Iteration: 7739, Loss: 1.247413158416748\n",
      "Iteration: 7740, Loss: 1.247408390045166\n",
      "Iteration: 7741, Loss: 1.2474033832550049\n",
      "Iteration: 7742, Loss: 1.2473983764648438\n",
      "Iteration: 7743, Loss: 1.2473934888839722\n",
      "Iteration: 7744, Loss: 1.247388243675232\n",
      "Iteration: 7745, Loss: 1.2473828792572021\n",
      "Iteration: 7746, Loss: 1.2473770380020142\n",
      "Iteration: 7747, Loss: 1.2473708391189575\n",
      "Iteration: 7748, Loss: 1.24736487865448\n",
      "Iteration: 7749, Loss: 1.247357964515686\n",
      "Iteration: 7750, Loss: 1.2473505735397339\n",
      "Iteration: 7751, Loss: 1.2473421096801758\n",
      "Iteration: 7752, Loss: 1.2473336458206177\n",
      "Iteration: 7753, Loss: 1.2473236322402954\n",
      "Iteration: 7754, Loss: 1.2473118305206299\n",
      "Iteration: 7755, Loss: 1.2472981214523315\n",
      "Iteration: 7756, Loss: 1.2472831010818481\n",
      "Iteration: 7757, Loss: 1.2472625970840454\n",
      "Iteration: 7758, Loss: 1.247235655784607\n",
      "Iteration: 7759, Loss: 1.2471998929977417\n",
      "Iteration: 7760, Loss: 1.2471460103988647\n",
      "Iteration: 7761, Loss: 1.2470673322677612\n",
      "Iteration: 7762, Loss: 1.2469394207000732\n",
      "Iteration: 7763, Loss: 1.2466719150543213\n",
      "Iteration: 7764, Loss: 1.2460333108901978\n",
      "Iteration: 7765, Loss: 1.2438552379608154\n",
      "Iteration: 7766, Loss: 1.2311980724334717\n",
      "Iteration: 7767, Loss: 1.2443327903747559\n",
      "Iteration: 7768, Loss: 1.2469849586486816\n",
      "Iteration: 7769, Loss: 1.2472525835037231\n",
      "Iteration: 7770, Loss: 1.247289776802063\n",
      "Iteration: 7771, Loss: 1.2472903728485107\n",
      "Iteration: 7772, Loss: 1.2472667694091797\n",
      "Iteration: 7773, Loss: 1.2472161054611206\n",
      "Iteration: 7774, Loss: 1.2471097707748413\n",
      "Iteration: 7775, Loss: 1.2468786239624023\n",
      "Iteration: 7776, Loss: 1.2464312314987183\n",
      "Iteration: 7777, Loss: 1.245050072669983\n",
      "Iteration: 7778, Loss: 1.239415168762207\n",
      "Iteration: 7779, Loss: 1.231199026107788\n",
      "Iteration: 7780, Loss: 1.2364890575408936\n",
      "Iteration: 7781, Loss: 1.2309551239013672\n",
      "Iteration: 7782, Loss: 1.230964183807373\n",
      "Iteration: 7783, Loss: 1.2333109378814697\n",
      "Iteration: 7784, Loss: 1.2299742698669434\n",
      "Iteration: 7785, Loss: 1.2285096645355225\n",
      "Iteration: 7786, Loss: 1.2302056550979614\n",
      "Iteration: 7787, Loss: 1.2305824756622314\n",
      "Iteration: 7788, Loss: 1.23000967502594\n",
      "Iteration: 7789, Loss: 1.229750156402588\n",
      "Iteration: 7790, Loss: 1.2300190925598145\n",
      "Iteration: 7791, Loss: 1.230186104774475\n",
      "Iteration: 7792, Loss: 1.2301630973815918\n",
      "Iteration: 7793, Loss: 1.2299813032150269\n",
      "Iteration: 7794, Loss: 1.2294306755065918\n",
      "Iteration: 7795, Loss: 1.2287132740020752\n",
      "Iteration: 7796, Loss: 1.2285183668136597\n",
      "Iteration: 7797, Loss: 1.2290167808532715\n",
      "Iteration: 7798, Loss: 1.229264736175537\n",
      "Iteration: 7799, Loss: 1.2287331819534302\n",
      "Iteration: 7800, Loss: 1.2283939123153687\n",
      "Iteration: 7801, Loss: 1.228837251663208\n",
      "Iteration: 7802, Loss: 1.229020595550537\n",
      "Iteration: 7803, Loss: 1.2285445928573608\n",
      "Iteration: 7804, Loss: 1.228441834449768\n",
      "Iteration: 7805, Loss: 1.228824257850647\n",
      "Iteration: 7806, Loss: 1.2287187576293945\n",
      "Iteration: 7807, Loss: 1.2283852100372314\n",
      "Iteration: 7808, Loss: 1.2285548448562622\n",
      "Iteration: 7809, Loss: 1.228693962097168\n",
      "Iteration: 7810, Loss: 1.2284610271453857\n",
      "Iteration: 7811, Loss: 1.2283903360366821\n",
      "Iteration: 7812, Loss: 1.228569507598877\n",
      "Iteration: 7813, Loss: 1.2285302877426147\n",
      "Iteration: 7814, Loss: 1.2283638715744019\n",
      "Iteration: 7815, Loss: 1.2284398078918457\n",
      "Iteration: 7816, Loss: 1.2285126447677612\n",
      "Iteration: 7817, Loss: 1.2283964157104492\n",
      "Iteration: 7818, Loss: 1.2283552885055542\n",
      "Iteration: 7819, Loss: 1.2284395694732666\n",
      "Iteration: 7820, Loss: 1.2284197807312012\n",
      "Iteration: 7821, Loss: 1.2283380031585693\n",
      "Iteration: 7822, Loss: 1.2283613681793213\n",
      "Iteration: 7823, Loss: 1.2284002304077148\n",
      "Iteration: 7824, Loss: 1.2283519506454468\n",
      "Iteration: 7825, Loss: 1.228318214416504\n",
      "Iteration: 7826, Loss: 1.2283508777618408\n",
      "Iteration: 7827, Loss: 1.2283536195755005\n",
      "Iteration: 7828, Loss: 1.2283129692077637\n",
      "Iteration: 7829, Loss: 1.2283066511154175\n",
      "Iteration: 7830, Loss: 1.2283289432525635\n",
      "Iteration: 7831, Loss: 1.2283153533935547\n",
      "Iteration: 7832, Loss: 1.2282878160476685\n",
      "Iteration: 7833, Loss: 1.2282922267913818\n",
      "Iteration: 7834, Loss: 1.228300929069519\n",
      "Iteration: 7835, Loss: 1.2282828092575073\n",
      "Iteration: 7836, Loss: 1.2282673120498657\n",
      "Iteration: 7837, Loss: 1.2282730340957642\n",
      "Iteration: 7838, Loss: 1.228271245956421\n",
      "Iteration: 7839, Loss: 1.2282545566558838\n",
      "Iteration: 7840, Loss: 1.2282440662384033\n",
      "Iteration: 7841, Loss: 1.2282428741455078\n",
      "Iteration: 7842, Loss: 1.228230595588684\n",
      "Iteration: 7843, Loss: 1.2282034158706665\n",
      "Iteration: 7844, Loss: 1.2281612157821655\n",
      "Iteration: 7845, Loss: 1.2280210256576538\n",
      "Iteration: 7846, Loss: 1.2264450788497925\n",
      "Iteration: 7847, Loss: 1.145655632019043\n",
      "Iteration: 7848, Loss: 1.2296686172485352\n",
      "Iteration: 7849, Loss: 1.2310279607772827\n",
      "Iteration: 7850, Loss: 1.2299789190292358\n",
      "Iteration: 7851, Loss: 1.2284239530563354\n",
      "Iteration: 7852, Loss: 1.228863000869751\n",
      "Iteration: 7853, Loss: 1.2298086881637573\n",
      "Iteration: 7854, Loss: 1.2297236919403076\n",
      "Iteration: 7855, Loss: 1.2289601564407349\n",
      "Iteration: 7856, Loss: 1.2286677360534668\n",
      "Iteration: 7857, Loss: 1.2290892601013184\n",
      "Iteration: 7858, Loss: 1.2295490503311157\n",
      "Iteration: 7859, Loss: 1.2295774221420288\n",
      "Iteration: 7860, Loss: 1.2292463779449463\n",
      "Iteration: 7861, Loss: 1.2289961576461792\n",
      "Iteration: 7862, Loss: 1.2290716171264648\n",
      "Iteration: 7863, Loss: 1.229300856590271\n",
      "Iteration: 7864, Loss: 1.2293990850448608\n",
      "Iteration: 7865, Loss: 1.2292726039886475\n",
      "Iteration: 7866, Loss: 1.2290546894073486\n",
      "Iteration: 7867, Loss: 1.2289506196975708\n",
      "Iteration: 7868, Loss: 1.2290149927139282\n",
      "Iteration: 7869, Loss: 1.2291077375411987\n",
      "Iteration: 7870, Loss: 1.2290822267532349\n",
      "Iteration: 7871, Loss: 1.2289352416992188\n",
      "Iteration: 7872, Loss: 1.228813648223877\n",
      "Iteration: 7873, Loss: 1.2287983894348145\n",
      "Iteration: 7874, Loss: 1.228827714920044\n",
      "Iteration: 7875, Loss: 1.2287893295288086\n",
      "Iteration: 7876, Loss: 1.2286500930786133\n",
      "Iteration: 7877, Loss: 1.228467583656311\n",
      "Iteration: 7878, Loss: 1.2283076047897339\n",
      "Iteration: 7879, Loss: 1.2281428575515747\n",
      "Iteration: 7880, Loss: 1.2278618812561035\n",
      "Iteration: 7881, Loss: 1.2274370193481445\n",
      "Iteration: 7882, Loss: 1.226818323135376\n",
      "Iteration: 7883, Loss: 1.226036548614502\n",
      "Iteration: 7884, Loss: 1.2250436544418335\n",
      "Iteration: 7885, Loss: 1.2238496541976929\n",
      "Iteration: 7886, Loss: 1.2222074270248413\n",
      "Iteration: 7887, Loss: 1.2196838855743408\n",
      "Iteration: 7888, Loss: 1.2167059183120728\n",
      "Iteration: 7889, Loss: 1.2137973308563232\n",
      "Iteration: 7890, Loss: 1.211800217628479\n",
      "Iteration: 7891, Loss: 1.2112469673156738\n",
      "Iteration: 7892, Loss: 1.2112315893173218\n",
      "Iteration: 7893, Loss: 1.210961937904358\n",
      "Iteration: 7894, Loss: 1.2107324600219727\n",
      "Iteration: 7895, Loss: 1.2104607820510864\n",
      "Iteration: 7896, Loss: 1.2108948230743408\n",
      "Iteration: 7897, Loss: 1.2109653949737549\n",
      "Iteration: 7898, Loss: 1.2102787494659424\n",
      "Iteration: 7899, Loss: 1.2112723588943481\n",
      "Iteration: 7900, Loss: 1.2102599143981934\n",
      "Iteration: 7901, Loss: 1.210909128189087\n",
      "Iteration: 7902, Loss: 1.2105365991592407\n",
      "Iteration: 7903, Loss: 1.2106150388717651\n",
      "Iteration: 7904, Loss: 1.2107417583465576\n",
      "Iteration: 7905, Loss: 1.210418701171875\n",
      "Iteration: 7906, Loss: 1.2108380794525146\n",
      "Iteration: 7907, Loss: 1.210296392440796\n",
      "Iteration: 7908, Loss: 1.210660696029663\n",
      "Iteration: 7909, Loss: 1.210492491722107\n",
      "Iteration: 7910, Loss: 1.210317850112915\n",
      "Iteration: 7911, Loss: 1.2107411623001099\n",
      "Iteration: 7912, Loss: 1.2102670669555664\n",
      "Iteration: 7913, Loss: 1.2104147672653198\n",
      "Iteration: 7914, Loss: 1.2104004621505737\n",
      "Iteration: 7915, Loss: 1.2103345394134521\n",
      "Iteration: 7916, Loss: 1.2102452516555786\n",
      "Iteration: 7917, Loss: 1.2104394435882568\n",
      "Iteration: 7918, Loss: 1.2101548910140991\n",
      "Iteration: 7919, Loss: 1.2103142738342285\n",
      "Iteration: 7920, Loss: 1.21025812625885\n",
      "Iteration: 7921, Loss: 1.2102106809616089\n",
      "Iteration: 7922, Loss: 1.210187315940857\n",
      "Iteration: 7923, Loss: 1.210260033607483\n",
      "Iteration: 7924, Loss: 1.2101218700408936\n",
      "Iteration: 7925, Loss: 1.210190773010254\n",
      "Iteration: 7926, Loss: 1.2101691961288452\n",
      "Iteration: 7927, Loss: 1.2101153135299683\n",
      "Iteration: 7928, Loss: 1.2101408243179321\n",
      "Iteration: 7929, Loss: 1.2101281881332397\n",
      "Iteration: 7930, Loss: 1.210089087486267\n",
      "Iteration: 7931, Loss: 1.2101045846939087\n",
      "Iteration: 7932, Loss: 1.2100931406021118\n",
      "Iteration: 7933, Loss: 1.2100532054901123\n",
      "Iteration: 7934, Loss: 1.210086703300476\n",
      "Iteration: 7935, Loss: 1.2100425958633423\n",
      "Iteration: 7936, Loss: 1.2100480794906616\n",
      "Iteration: 7937, Loss: 1.2100417613983154\n",
      "Iteration: 7938, Loss: 1.2100248336791992\n",
      "Iteration: 7939, Loss: 1.2100192308425903\n",
      "Iteration: 7940, Loss: 1.2100163698196411\n",
      "Iteration: 7941, Loss: 1.2099971771240234\n",
      "Iteration: 7942, Loss: 1.2099989652633667\n",
      "Iteration: 7943, Loss: 1.209986686706543\n",
      "Iteration: 7944, Loss: 1.2099757194519043\n",
      "Iteration: 7945, Loss: 1.209976077079773\n",
      "Iteration: 7946, Loss: 1.209958553314209\n",
      "Iteration: 7947, Loss: 1.2099595069885254\n",
      "Iteration: 7948, Loss: 1.2099478244781494\n",
      "Iteration: 7949, Loss: 1.2099400758743286\n",
      "Iteration: 7950, Loss: 1.2099359035491943\n",
      "Iteration: 7951, Loss: 1.2099248170852661\n",
      "Iteration: 7952, Loss: 1.209920883178711\n",
      "Iteration: 7953, Loss: 1.2099120616912842\n",
      "Iteration: 7954, Loss: 1.2099052667617798\n",
      "Iteration: 7955, Loss: 1.2098990678787231\n",
      "Iteration: 7956, Loss: 1.2098910808563232\n",
      "Iteration: 7957, Loss: 1.2098850011825562\n",
      "Iteration: 7958, Loss: 1.2098779678344727\n",
      "Iteration: 7959, Loss: 1.2098709344863892\n",
      "Iteration: 7960, Loss: 1.2098644971847534\n",
      "Iteration: 7961, Loss: 1.2098571062088013\n",
      "Iteration: 7962, Loss: 1.209850549697876\n",
      "Iteration: 7963, Loss: 1.2098439931869507\n",
      "Iteration: 7964, Loss: 1.2098368406295776\n",
      "Iteration: 7965, Loss: 1.209830641746521\n",
      "Iteration: 7966, Loss: 1.2098232507705688\n",
      "Iteration: 7967, Loss: 1.2098169326782227\n",
      "Iteration: 7968, Loss: 1.2098102569580078\n",
      "Iteration: 7969, Loss: 1.2098032236099243\n",
      "Iteration: 7970, Loss: 1.2097969055175781\n",
      "Iteration: 7971, Loss: 1.2097899913787842\n",
      "Iteration: 7972, Loss: 1.2097835540771484\n",
      "Iteration: 7973, Loss: 1.2097768783569336\n",
      "Iteration: 7974, Loss: 1.2097712755203247\n",
      "Iteration: 7975, Loss: 1.2097649574279785\n",
      "Iteration: 7976, Loss: 1.209757685661316\n",
      "Iteration: 7977, Loss: 1.2097517251968384\n",
      "Iteration: 7978, Loss: 1.2097442150115967\n",
      "Iteration: 7979, Loss: 1.209739089012146\n",
      "Iteration: 7980, Loss: 1.2097312211990356\n",
      "Iteration: 7981, Loss: 1.2097257375717163\n",
      "Iteration: 7982, Loss: 1.2097183465957642\n",
      "Iteration: 7983, Loss: 1.209712266921997\n",
      "Iteration: 7984, Loss: 1.2097057104110718\n",
      "Iteration: 7985, Loss: 1.209699034690857\n",
      "Iteration: 7986, Loss: 1.2096925973892212\n",
      "Iteration: 7987, Loss: 1.209686040878296\n",
      "Iteration: 7988, Loss: 1.2096798419952393\n",
      "Iteration: 7989, Loss: 1.2096734046936035\n",
      "Iteration: 7990, Loss: 1.2096673250198364\n",
      "Iteration: 7991, Loss: 1.2096607685089111\n",
      "Iteration: 7992, Loss: 1.209654450416565\n",
      "Iteration: 7993, Loss: 1.2096481323242188\n",
      "Iteration: 7994, Loss: 1.209641695022583\n",
      "Iteration: 7995, Loss: 1.2096354961395264\n",
      "Iteration: 7996, Loss: 1.209628939628601\n",
      "Iteration: 7997, Loss: 1.2096233367919922\n",
      "Iteration: 7998, Loss: 1.2096164226531982\n",
      "Iteration: 7999, Loss: 1.2096107006072998\n",
      "Iteration: 8000, Loss: 1.2096041440963745\n",
      "Iteration: 8001, Loss: 1.2095978260040283\n",
      "Iteration: 8002, Loss: 1.2095916271209717\n",
      "Iteration: 8003, Loss: 1.209585428237915\n",
      "Iteration: 8004, Loss: 1.2095792293548584\n",
      "Iteration: 8005, Loss: 1.2095730304718018\n",
      "Iteration: 8006, Loss: 1.2095669507980347\n",
      "Iteration: 8007, Loss: 1.209560751914978\n",
      "Iteration: 8008, Loss: 1.209554672241211\n",
      "Iteration: 8009, Loss: 1.2095484733581543\n",
      "Iteration: 8010, Loss: 1.2095423936843872\n",
      "Iteration: 8011, Loss: 1.2095363140106201\n",
      "Iteration: 8012, Loss: 1.209530234336853\n",
      "Iteration: 8013, Loss: 1.2095245122909546\n",
      "Iteration: 8014, Loss: 1.2095180749893188\n",
      "Iteration: 8015, Loss: 1.2095121145248413\n",
      "Iteration: 8016, Loss: 1.2095061540603638\n",
      "Iteration: 8017, Loss: 1.2094999551773071\n",
      "Iteration: 8018, Loss: 1.2094942331314087\n",
      "Iteration: 8019, Loss: 1.2094879150390625\n",
      "Iteration: 8020, Loss: 1.209481954574585\n",
      "Iteration: 8021, Loss: 1.209476113319397\n",
      "Iteration: 8022, Loss: 1.2094700336456299\n",
      "Iteration: 8023, Loss: 1.2094640731811523\n",
      "Iteration: 8024, Loss: 1.209458351135254\n",
      "Iteration: 8025, Loss: 1.2094521522521973\n",
      "Iteration: 8026, Loss: 1.2094464302062988\n",
      "Iteration: 8027, Loss: 1.2094405889511108\n",
      "Iteration: 8028, Loss: 1.2094343900680542\n",
      "Iteration: 8029, Loss: 1.2094290256500244\n",
      "Iteration: 8030, Loss: 1.2094227075576782\n",
      "Iteration: 8031, Loss: 1.2094169855117798\n",
      "Iteration: 8032, Loss: 1.2094111442565918\n",
      "Iteration: 8033, Loss: 1.2094051837921143\n",
      "Iteration: 8034, Loss: 1.2093994617462158\n",
      "Iteration: 8035, Loss: 1.2093936204910278\n",
      "Iteration: 8036, Loss: 1.2093878984451294\n",
      "Iteration: 8037, Loss: 1.2093820571899414\n",
      "Iteration: 8038, Loss: 1.2093762159347534\n",
      "Iteration: 8039, Loss: 1.2093703746795654\n",
      "Iteration: 8040, Loss: 1.209364414215088\n",
      "Iteration: 8041, Loss: 1.209359049797058\n",
      "Iteration: 8042, Loss: 1.2093533277511597\n",
      "Iteration: 8043, Loss: 1.2093474864959717\n",
      "Iteration: 8044, Loss: 1.2093417644500732\n",
      "Iteration: 8045, Loss: 1.2093361616134644\n",
      "Iteration: 8046, Loss: 1.209330439567566\n",
      "Iteration: 8047, Loss: 1.209324836730957\n",
      "Iteration: 8048, Loss: 1.209318995475769\n",
      "Iteration: 8049, Loss: 1.2093135118484497\n",
      "Iteration: 8050, Loss: 1.2093079090118408\n",
      "Iteration: 8051, Loss: 1.209302306175232\n",
      "Iteration: 8052, Loss: 1.209296703338623\n",
      "Iteration: 8053, Loss: 1.2092911005020142\n",
      "Iteration: 8054, Loss: 1.2092857360839844\n",
      "Iteration: 8055, Loss: 1.209280014038086\n",
      "Iteration: 8056, Loss: 1.209274411201477\n",
      "Iteration: 8057, Loss: 1.2092688083648682\n",
      "Iteration: 8058, Loss: 1.209263563156128\n",
      "Iteration: 8059, Loss: 1.209257960319519\n",
      "Iteration: 8060, Loss: 1.2092524766921997\n",
      "Iteration: 8061, Loss: 1.2092469930648804\n",
      "Iteration: 8062, Loss: 1.209241509437561\n",
      "Iteration: 8063, Loss: 1.2092362642288208\n",
      "Iteration: 8064, Loss: 1.2092307806015015\n",
      "Iteration: 8065, Loss: 1.2092252969741821\n",
      "Iteration: 8066, Loss: 1.2092199325561523\n",
      "Iteration: 8067, Loss: 1.2092148065567017\n",
      "Iteration: 8068, Loss: 1.2092093229293823\n",
      "Iteration: 8069, Loss: 1.209204077720642\n",
      "Iteration: 8070, Loss: 1.2091987133026123\n",
      "Iteration: 8071, Loss: 1.2091931104660034\n",
      "Iteration: 8072, Loss: 1.2091879844665527\n",
      "Iteration: 8073, Loss: 1.2091825008392334\n",
      "Iteration: 8074, Loss: 1.2091772556304932\n",
      "Iteration: 8075, Loss: 1.209172248840332\n",
      "Iteration: 8076, Loss: 1.2091668844223022\n",
      "Iteration: 8077, Loss: 1.2091615200042725\n",
      "Iteration: 8078, Loss: 1.2091563940048218\n",
      "Iteration: 8079, Loss: 1.209152102470398\n",
      "Iteration: 8080, Loss: 1.2091461420059204\n",
      "Iteration: 8081, Loss: 1.209141731262207\n",
      "Iteration: 8082, Loss: 1.2091363668441772\n",
      "Iteration: 8083, Loss: 1.2091306447982788\n",
      "Iteration: 8084, Loss: 1.2091257572174072\n",
      "Iteration: 8085, Loss: 1.2091208696365356\n",
      "Iteration: 8086, Loss: 1.2091153860092163\n",
      "Iteration: 8087, Loss: 1.2091103792190552\n",
      "Iteration: 8088, Loss: 1.2091052532196045\n",
      "Iteration: 8089, Loss: 1.2091000080108643\n",
      "Iteration: 8090, Loss: 1.2090953588485718\n",
      "Iteration: 8091, Loss: 1.2090903520584106\n",
      "Iteration: 8092, Loss: 1.2090851068496704\n",
      "Iteration: 8093, Loss: 1.2090801000595093\n",
      "Iteration: 8094, Loss: 1.2090762853622437\n",
      "Iteration: 8095, Loss: 1.2090703248977661\n",
      "Iteration: 8096, Loss: 1.2090662717819214\n",
      "Iteration: 8097, Loss: 1.209060549736023\n",
      "Iteration: 8098, Loss: 1.2090563774108887\n",
      "Iteration: 8099, Loss: 1.2090508937835693\n",
      "Iteration: 8100, Loss: 1.2090463638305664\n",
      "Iteration: 8101, Loss: 1.209041714668274\n",
      "Iteration: 8102, Loss: 1.2090363502502441\n",
      "Iteration: 8103, Loss: 1.2090322971343994\n",
      "Iteration: 8104, Loss: 1.2090269327163696\n",
      "Iteration: 8105, Loss: 1.2090224027633667\n",
      "Iteration: 8106, Loss: 1.2090173959732056\n",
      "Iteration: 8107, Loss: 1.2090129852294922\n",
      "Iteration: 8108, Loss: 1.2090082168579102\n",
      "Iteration: 8109, Loss: 1.2090034484863281\n",
      "Iteration: 8110, Loss: 1.208999514579773\n",
      "Iteration: 8111, Loss: 1.2089941501617432\n",
      "Iteration: 8112, Loss: 1.2089899778366089\n",
      "Iteration: 8113, Loss: 1.2089852094650269\n",
      "Iteration: 8114, Loss: 1.2089804410934448\n",
      "Iteration: 8115, Loss: 1.208976149559021\n",
      "Iteration: 8116, Loss: 1.2089711427688599\n",
      "Iteration: 8117, Loss: 1.2089672088623047\n",
      "Iteration: 8118, Loss: 1.2089625597000122\n",
      "Iteration: 8119, Loss: 1.2089582681655884\n",
      "Iteration: 8120, Loss: 1.2089555263519287\n",
      "Iteration: 8121, Loss: 1.2089492082595825\n",
      "Iteration: 8122, Loss: 1.208946704864502\n",
      "Iteration: 8123, Loss: 1.2089406251907349\n",
      "Iteration: 8124, Loss: 1.208937168121338\n",
      "Iteration: 8125, Loss: 1.2089320421218872\n",
      "Iteration: 8126, Loss: 1.208928108215332\n",
      "Iteration: 8127, Loss: 1.2089236974716187\n",
      "Iteration: 8128, Loss: 1.2089190483093262\n",
      "Iteration: 8129, Loss: 1.2089154720306396\n",
      "Iteration: 8130, Loss: 1.2089104652404785\n",
      "Iteration: 8131, Loss: 1.2089065313339233\n",
      "Iteration: 8132, Loss: 1.208902359008789\n",
      "Iteration: 8133, Loss: 1.2088980674743652\n",
      "Iteration: 8134, Loss: 1.2088937759399414\n",
      "Iteration: 8135, Loss: 1.2088894844055176\n",
      "Iteration: 8136, Loss: 1.2088860273361206\n",
      "Iteration: 8137, Loss: 1.2088812589645386\n",
      "Iteration: 8138, Loss: 1.208877444267273\n",
      "Iteration: 8139, Loss: 1.2088731527328491\n",
      "Iteration: 8140, Loss: 1.208869218826294\n",
      "Iteration: 8141, Loss: 1.2088650465011597\n",
      "Iteration: 8142, Loss: 1.2088611125946045\n",
      "Iteration: 8143, Loss: 1.2088574171066284\n",
      "Iteration: 8144, Loss: 1.2088531255722046\n",
      "Iteration: 8145, Loss: 1.2088491916656494\n",
      "Iteration: 8146, Loss: 1.2088451385498047\n",
      "Iteration: 8147, Loss: 1.20884108543396\n",
      "Iteration: 8148, Loss: 1.2088373899459839\n",
      "Iteration: 8149, Loss: 1.2088333368301392\n",
      "Iteration: 8150, Loss: 1.2088295221328735\n",
      "Iteration: 8151, Loss: 1.208825707435608\n",
      "Iteration: 8152, Loss: 1.2088218927383423\n",
      "Iteration: 8153, Loss: 1.2088180780410767\n",
      "Iteration: 8154, Loss: 1.208814024925232\n",
      "Iteration: 8155, Loss: 1.2088103294372559\n",
      "Iteration: 8156, Loss: 1.2088063955307007\n",
      "Iteration: 8157, Loss: 1.2088029384613037\n",
      "Iteration: 8158, Loss: 1.2087990045547485\n",
      "Iteration: 8159, Loss: 1.2087950706481934\n",
      "Iteration: 8160, Loss: 1.2087913751602173\n",
      "Iteration: 8161, Loss: 1.2087876796722412\n",
      "Iteration: 8162, Loss: 1.2087838649749756\n",
      "Iteration: 8163, Loss: 1.20878005027771\n",
      "Iteration: 8164, Loss: 1.2087763547897339\n",
      "Iteration: 8165, Loss: 1.2087725400924683\n",
      "Iteration: 8166, Loss: 1.2087689638137817\n",
      "Iteration: 8167, Loss: 1.2087651491165161\n",
      "Iteration: 8168, Loss: 1.2087615728378296\n",
      "Iteration: 8169, Loss: 1.2087581157684326\n",
      "Iteration: 8170, Loss: 1.208754301071167\n",
      "Iteration: 8171, Loss: 1.2087507247924805\n",
      "Iteration: 8172, Loss: 1.2087472677230835\n",
      "Iteration: 8173, Loss: 1.2087435722351074\n",
      "Iteration: 8174, Loss: 1.208739995956421\n",
      "Iteration: 8175, Loss: 1.2087363004684448\n",
      "Iteration: 8176, Loss: 1.2087326049804688\n",
      "Iteration: 8177, Loss: 1.2087290287017822\n",
      "Iteration: 8178, Loss: 1.2087255716323853\n",
      "Iteration: 8179, Loss: 1.2087218761444092\n",
      "Iteration: 8180, Loss: 1.2087185382843018\n",
      "Iteration: 8181, Loss: 1.2087149620056152\n",
      "Iteration: 8182, Loss: 1.2087129354476929\n",
      "Iteration: 8183, Loss: 1.2087081670761108\n",
      "Iteration: 8184, Loss: 1.2087057828903198\n",
      "Iteration: 8185, Loss: 1.2087019681930542\n",
      "Iteration: 8186, Loss: 1.2086979150772095\n",
      "Iteration: 8187, Loss: 1.2086944580078125\n",
      "Iteration: 8188, Loss: 1.2086917161941528\n",
      "Iteration: 8189, Loss: 1.208687424659729\n",
      "Iteration: 8190, Loss: 1.2086842060089111\n",
      "Iteration: 8191, Loss: 1.2086812257766724\n",
      "Iteration: 8192, Loss: 1.2086771726608276\n",
      "Iteration: 8193, Loss: 1.2086737155914307\n",
      "Iteration: 8194, Loss: 1.2086708545684814\n",
      "Iteration: 8195, Loss: 1.208667516708374\n",
      "Iteration: 8196, Loss: 1.2086637020111084\n",
      "Iteration: 8197, Loss: 1.2086608409881592\n",
      "Iteration: 8198, Loss: 1.2086570262908936\n",
      "Iteration: 8199, Loss: 1.2086535692214966\n",
      "Iteration: 8200, Loss: 1.2086507081985474\n",
      "Iteration: 8201, Loss: 1.2086470127105713\n",
      "Iteration: 8202, Loss: 1.2086437940597534\n",
      "Iteration: 8203, Loss: 1.208640456199646\n",
      "Iteration: 8204, Loss: 1.2086374759674072\n",
      "Iteration: 8205, Loss: 1.2086337804794312\n",
      "Iteration: 8206, Loss: 1.2086306810379028\n",
      "Iteration: 8207, Loss: 1.208627462387085\n",
      "Iteration: 8208, Loss: 1.2086241245269775\n",
      "Iteration: 8209, Loss: 1.2086209058761597\n",
      "Iteration: 8210, Loss: 1.2086176872253418\n",
      "Iteration: 8211, Loss: 1.2086142301559448\n",
      "Iteration: 8212, Loss: 1.2086111307144165\n",
      "Iteration: 8213, Loss: 1.2086081504821777\n",
      "Iteration: 8214, Loss: 1.2086049318313599\n",
      "Iteration: 8215, Loss: 1.208601713180542\n",
      "Iteration: 8216, Loss: 1.2085986137390137\n",
      "Iteration: 8217, Loss: 1.2085953950881958\n",
      "Iteration: 8218, Loss: 1.208592176437378\n",
      "Iteration: 8219, Loss: 1.2085891962051392\n",
      "Iteration: 8220, Loss: 1.2085860967636108\n",
      "Iteration: 8221, Loss: 1.2085835933685303\n",
      "Iteration: 8222, Loss: 1.2085801362991333\n",
      "Iteration: 8223, Loss: 1.2085775136947632\n",
      "Iteration: 8224, Loss: 1.2085739374160767\n",
      "Iteration: 8225, Loss: 1.208570957183838\n",
      "Iteration: 8226, Loss: 1.20856773853302\n",
      "Iteration: 8227, Loss: 1.2085649967193604\n",
      "Iteration: 8228, Loss: 1.2085615396499634\n",
      "Iteration: 8229, Loss: 1.2085587978363037\n",
      "Iteration: 8230, Loss: 1.2085555791854858\n",
      "Iteration: 8231, Loss: 1.208552598953247\n",
      "Iteration: 8232, Loss: 1.2085493803024292\n",
      "Iteration: 8233, Loss: 1.208546757698059\n",
      "Iteration: 8234, Loss: 1.2085434198379517\n",
      "Iteration: 8235, Loss: 1.208540678024292\n",
      "Iteration: 8236, Loss: 1.2085376977920532\n",
      "Iteration: 8237, Loss: 1.2085347175598145\n",
      "Iteration: 8238, Loss: 1.2085316181182861\n",
      "Iteration: 8239, Loss: 1.208528995513916\n",
      "Iteration: 8240, Loss: 1.2085257768630981\n",
      "Iteration: 8241, Loss: 1.2085230350494385\n",
      "Iteration: 8242, Loss: 1.2085198163986206\n",
      "Iteration: 8243, Loss: 1.2085174322128296\n",
      "Iteration: 8244, Loss: 1.2085140943527222\n",
      "Iteration: 8245, Loss: 1.208511471748352\n",
      "Iteration: 8246, Loss: 1.2085083723068237\n",
      "Iteration: 8247, Loss: 1.2085057497024536\n",
      "Iteration: 8248, Loss: 1.2085027694702148\n",
      "Iteration: 8249, Loss: 1.2085000276565552\n",
      "Iteration: 8250, Loss: 1.208497166633606\n",
      "Iteration: 8251, Loss: 1.2084944248199463\n",
      "Iteration: 8252, Loss: 1.2084914445877075\n",
      "Iteration: 8253, Loss: 1.2084887027740479\n",
      "Iteration: 8254, Loss: 1.2084861993789673\n",
      "Iteration: 8255, Loss: 1.2084834575653076\n",
      "Iteration: 8256, Loss: 1.2084805965423584\n",
      "Iteration: 8257, Loss: 1.2084778547286987\n",
      "Iteration: 8258, Loss: 1.2084749937057495\n",
      "Iteration: 8259, Loss: 1.2084722518920898\n",
      "Iteration: 8260, Loss: 1.2084695100784302\n",
      "Iteration: 8261, Loss: 1.20846688747406\n",
      "Iteration: 8262, Loss: 1.2084641456604004\n",
      "Iteration: 8263, Loss: 1.2084614038467407\n",
      "Iteration: 8264, Loss: 1.2084587812423706\n",
      "Iteration: 8265, Loss: 1.2084559202194214\n",
      "Iteration: 8266, Loss: 1.2084532976150513\n",
      "Iteration: 8267, Loss: 1.2084506750106812\n",
      "Iteration: 8268, Loss: 1.2084481716156006\n",
      "Iteration: 8269, Loss: 1.2084453105926514\n",
      "Iteration: 8270, Loss: 1.2084428071975708\n",
      "Iteration: 8271, Loss: 1.2084400653839111\n",
      "Iteration: 8272, Loss: 1.208437442779541\n",
      "Iteration: 8273, Loss: 1.2084349393844604\n",
      "Iteration: 8274, Loss: 1.2084323167800903\n",
      "Iteration: 8275, Loss: 1.2084298133850098\n",
      "Iteration: 8276, Loss: 1.2084277868270874\n",
      "Iteration: 8277, Loss: 1.208424687385559\n",
      "Iteration: 8278, Loss: 1.2084226608276367\n",
      "Iteration: 8279, Loss: 1.2084194421768188\n",
      "Iteration: 8280, Loss: 1.2084171772003174\n",
      "Iteration: 8281, Loss: 1.2084144353866577\n",
      "Iteration: 8282, Loss: 1.2084120512008667\n",
      "Iteration: 8283, Loss: 1.2084095478057861\n",
      "Iteration: 8284, Loss: 1.208406925201416\n",
      "Iteration: 8285, Loss: 1.2084044218063354\n",
      "Iteration: 8286, Loss: 1.2084019184112549\n",
      "Iteration: 8287, Loss: 1.2084003686904907\n",
      "Iteration: 8288, Loss: 1.2083970308303833\n",
      "Iteration: 8289, Loss: 1.2083957195281982\n",
      "Iteration: 8290, Loss: 1.20839262008667\n",
      "Iteration: 8291, Loss: 1.2083903551101685\n",
      "Iteration: 8292, Loss: 1.2083896398544312\n",
      "Iteration: 8293, Loss: 1.2083851099014282\n",
      "Iteration: 8294, Loss: 1.2083847522735596\n",
      "Iteration: 8295, Loss: 1.2083805799484253\n",
      "Iteration: 8296, Loss: 1.2083779573440552\n",
      "Iteration: 8297, Loss: 1.2083767652511597\n",
      "Iteration: 8298, Loss: 1.2083724737167358\n",
      "Iteration: 8299, Loss: 1.2083715200424194\n",
      "Iteration: 8300, Loss: 1.2083686590194702\n",
      "Iteration: 8301, Loss: 1.2083654403686523\n",
      "Iteration: 8302, Loss: 1.2083642482757568\n",
      "Iteration: 8303, Loss: 1.2083606719970703\n",
      "Iteration: 8304, Loss: 1.2083590030670166\n",
      "Iteration: 8305, Loss: 1.208356499671936\n",
      "Iteration: 8306, Loss: 1.2083543539047241\n",
      "Iteration: 8307, Loss: 1.20835280418396\n",
      "Iteration: 8308, Loss: 1.2083491086959839\n",
      "Iteration: 8309, Loss: 1.2083476781845093\n",
      "Iteration: 8310, Loss: 1.2083449363708496\n",
      "Iteration: 8311, Loss: 1.20834219455719\n",
      "Iteration: 8312, Loss: 1.2083406448364258\n",
      "Iteration: 8313, Loss: 1.2083381414413452\n",
      "Iteration: 8314, Loss: 1.2083371877670288\n",
      "Iteration: 8315, Loss: 1.2083330154418945\n",
      "Iteration: 8316, Loss: 1.2083327770233154\n",
      "Iteration: 8317, Loss: 1.2083290815353394\n",
      "Iteration: 8318, Loss: 1.2083266973495483\n",
      "Iteration: 8319, Loss: 1.2083253860473633\n",
      "Iteration: 8320, Loss: 1.2083215713500977\n",
      "Iteration: 8321, Loss: 1.2083200216293335\n",
      "Iteration: 8322, Loss: 1.2083181142807007\n",
      "Iteration: 8323, Loss: 1.2083147764205933\n",
      "Iteration: 8324, Loss: 1.2083134651184082\n",
      "Iteration: 8325, Loss: 1.2083109617233276\n",
      "Iteration: 8326, Loss: 1.208308219909668\n",
      "Iteration: 8327, Loss: 1.2083061933517456\n",
      "Iteration: 8328, Loss: 1.2083039283752441\n",
      "Iteration: 8329, Loss: 1.2083017826080322\n",
      "Iteration: 8330, Loss: 1.2082993984222412\n",
      "Iteration: 8331, Loss: 1.2082974910736084\n",
      "Iteration: 8332, Loss: 1.208295226097107\n",
      "Iteration: 8333, Loss: 1.2082929611206055\n",
      "Iteration: 8334, Loss: 1.2082910537719727\n",
      "Iteration: 8335, Loss: 1.2082886695861816\n",
      "Iteration: 8336, Loss: 1.2082866430282593\n",
      "Iteration: 8337, Loss: 1.2082843780517578\n",
      "Iteration: 8338, Loss: 1.2082825899124146\n",
      "Iteration: 8339, Loss: 1.2082804441452026\n",
      "Iteration: 8340, Loss: 1.2082782983779907\n",
      "Iteration: 8341, Loss: 1.2082761526107788\n",
      "Iteration: 8342, Loss: 1.208274006843567\n",
      "Iteration: 8343, Loss: 1.2082719802856445\n",
      "Iteration: 8344, Loss: 1.2082698345184326\n",
      "Iteration: 8345, Loss: 1.2082676887512207\n",
      "Iteration: 8346, Loss: 1.2082656621932983\n",
      "Iteration: 8347, Loss: 1.2082635164260864\n",
      "Iteration: 8348, Loss: 1.208261489868164\n",
      "Iteration: 8349, Loss: 1.2082594633102417\n",
      "Iteration: 8350, Loss: 1.2082574367523193\n",
      "Iteration: 8351, Loss: 1.2082552909851074\n",
      "Iteration: 8352, Loss: 1.208253264427185\n",
      "Iteration: 8353, Loss: 1.2082515954971313\n",
      "Iteration: 8354, Loss: 1.208249568939209\n",
      "Iteration: 8355, Loss: 1.2082475423812866\n",
      "Iteration: 8356, Loss: 1.2082455158233643\n",
      "Iteration: 8357, Loss: 1.208243489265442\n",
      "Iteration: 8358, Loss: 1.2082414627075195\n",
      "Iteration: 8359, Loss: 1.2082395553588867\n",
      "Iteration: 8360, Loss: 1.2082374095916748\n",
      "Iteration: 8361, Loss: 1.208235502243042\n",
      "Iteration: 8362, Loss: 1.2082335948944092\n",
      "Iteration: 8363, Loss: 1.2082315683364868\n",
      "Iteration: 8364, Loss: 1.2082295417785645\n",
      "Iteration: 8365, Loss: 1.2082276344299316\n",
      "Iteration: 8366, Loss: 1.2082256078720093\n",
      "Iteration: 8367, Loss: 1.2082237005233765\n",
      "Iteration: 8368, Loss: 1.2082217931747437\n",
      "Iteration: 8369, Loss: 1.2082197666168213\n",
      "Iteration: 8370, Loss: 1.2082182168960571\n",
      "Iteration: 8371, Loss: 1.2082165479660034\n",
      "Iteration: 8372, Loss: 1.2082147598266602\n",
      "Iteration: 8373, Loss: 1.2082122564315796\n",
      "Iteration: 8374, Loss: 1.2082117795944214\n",
      "Iteration: 8375, Loss: 1.208212971687317\n",
      "Iteration: 8376, Loss: 1.2082120180130005\n",
      "Iteration: 8377, Loss: 1.2082058191299438\n",
      "Iteration: 8378, Loss: 1.2082031965255737\n",
      "Iteration: 8379, Loss: 1.2082030773162842\n",
      "Iteration: 8380, Loss: 1.208202838897705\n",
      "Iteration: 8381, Loss: 1.208199381828308\n",
      "Iteration: 8382, Loss: 1.2081955671310425\n",
      "Iteration: 8383, Loss: 1.2081941366195679\n",
      "Iteration: 8384, Loss: 1.2081935405731201\n",
      "Iteration: 8385, Loss: 1.2081923484802246\n",
      "Iteration: 8386, Loss: 1.2081888914108276\n",
      "Iteration: 8387, Loss: 1.2081862688064575\n",
      "Iteration: 8388, Loss: 1.2081844806671143\n",
      "Iteration: 8389, Loss: 1.208183765411377\n",
      "Iteration: 8390, Loss: 1.2081807851791382\n",
      "Iteration: 8391, Loss: 1.2081800699234009\n",
      "Iteration: 8392, Loss: 1.2081812620162964\n",
      "Iteration: 8393, Loss: 1.2081811428070068\n",
      "Iteration: 8394, Loss: 1.2081774473190308\n",
      "Iteration: 8395, Loss: 1.2081729173660278\n",
      "Iteration: 8396, Loss: 1.2081698179244995\n",
      "Iteration: 8397, Loss: 1.208168625831604\n",
      "Iteration: 8398, Loss: 1.2081687450408936\n",
      "Iteration: 8399, Loss: 1.2081679105758667\n",
      "Iteration: 8400, Loss: 1.2081639766693115\n",
      "Iteration: 8401, Loss: 1.2081608772277832\n",
      "Iteration: 8402, Loss: 1.2081609964370728\n",
      "Iteration: 8403, Loss: 1.2081613540649414\n",
      "Iteration: 8404, Loss: 1.20815908908844\n",
      "Iteration: 8405, Loss: 1.2081549167633057\n",
      "Iteration: 8406, Loss: 1.208151936531067\n",
      "Iteration: 8407, Loss: 1.208152413368225\n",
      "Iteration: 8408, Loss: 1.2081547975540161\n",
      "Iteration: 8409, Loss: 1.208156704902649\n",
      "Iteration: 8410, Loss: 1.2081568241119385\n",
      "Iteration: 8411, Loss: 1.2081555128097534\n",
      "Iteration: 8412, Loss: 1.2081527709960938\n",
      "Iteration: 8413, Loss: 1.2081493139266968\n",
      "Iteration: 8414, Loss: 1.2081453800201416\n",
      "Iteration: 8415, Loss: 1.2081419229507446\n",
      "Iteration: 8416, Loss: 1.2081388235092163\n",
      "Iteration: 8417, Loss: 1.2081358432769775\n",
      "Iteration: 8418, Loss: 1.2081334590911865\n",
      "Iteration: 8419, Loss: 1.2081313133239746\n",
      "Iteration: 8420, Loss: 1.2081292867660522\n",
      "Iteration: 8421, Loss: 1.208127737045288\n",
      "Iteration: 8422, Loss: 1.208126187324524\n",
      "Iteration: 8423, Loss: 1.2081246376037598\n",
      "Iteration: 8424, Loss: 1.2081235647201538\n",
      "Iteration: 8425, Loss: 1.2081232070922852\n",
      "Iteration: 8426, Loss: 1.2081220149993896\n",
      "Iteration: 8427, Loss: 1.2081204652786255\n",
      "Iteration: 8428, Loss: 1.2081197500228882\n",
      "Iteration: 8429, Loss: 1.2081210613250732\n",
      "Iteration: 8430, Loss: 1.208126425743103\n",
      "Iteration: 8431, Loss: 1.208139181137085\n",
      "Iteration: 8432, Loss: 1.2081704139709473\n",
      "Iteration: 8433, Loss: 1.2082412242889404\n",
      "Iteration: 8434, Loss: 1.2083814144134521\n",
      "Iteration: 8435, Loss: 1.2086412906646729\n",
      "Iteration: 8436, Loss: 1.2090115547180176\n",
      "Iteration: 8437, Loss: 1.2091838121414185\n",
      "Iteration: 8438, Loss: 1.2089189291000366\n",
      "Iteration: 8439, Loss: 1.208425760269165\n",
      "Iteration: 8440, Loss: 1.2081328630447388\n",
      "Iteration: 8441, Loss: 1.208141565322876\n",
      "Iteration: 8442, Loss: 1.208328127861023\n",
      "Iteration: 8443, Loss: 1.2084964513778687\n",
      "Iteration: 8444, Loss: 1.208432674407959\n",
      "Iteration: 8445, Loss: 1.2082194089889526\n",
      "Iteration: 8446, Loss: 1.2081074714660645\n",
      "Iteration: 8447, Loss: 1.2081912755966187\n",
      "Iteration: 8448, Loss: 1.2082929611206055\n",
      "Iteration: 8449, Loss: 1.2082252502441406\n",
      "Iteration: 8450, Loss: 1.2081183195114136\n",
      "Iteration: 8451, Loss: 1.2081345319747925\n",
      "Iteration: 8452, Loss: 1.208211898803711\n",
      "Iteration: 8453, Loss: 1.2081845998764038\n",
      "Iteration: 8454, Loss: 1.2081129550933838\n",
      "Iteration: 8455, Loss: 1.2081172466278076\n",
      "Iteration: 8456, Loss: 1.20815908908844\n",
      "Iteration: 8457, Loss: 1.208150029182434\n",
      "Iteration: 8458, Loss: 1.208104133605957\n",
      "Iteration: 8459, Loss: 1.2081021070480347\n",
      "Iteration: 8460, Loss: 1.208131194114685\n",
      "Iteration: 8461, Loss: 1.2081284523010254\n",
      "Iteration: 8462, Loss: 1.2080988883972168\n",
      "Iteration: 8463, Loss: 1.2080867290496826\n",
      "Iteration: 8464, Loss: 1.208103895187378\n",
      "Iteration: 8465, Loss: 1.2081117630004883\n",
      "Iteration: 8466, Loss: 1.2080987691879272\n",
      "Iteration: 8467, Loss: 1.2080798149108887\n",
      "Iteration: 8468, Loss: 1.2080787420272827\n",
      "Iteration: 8469, Loss: 1.2080882787704468\n",
      "Iteration: 8470, Loss: 1.2080940008163452\n",
      "Iteration: 8471, Loss: 1.208086609840393\n",
      "Iteration: 8472, Loss: 1.208074688911438\n",
      "Iteration: 8473, Loss: 1.208067774772644\n",
      "Iteration: 8474, Loss: 1.208070993423462\n",
      "Iteration: 8475, Loss: 1.2080843448638916\n",
      "Iteration: 8476, Loss: 1.2080999612808228\n",
      "Iteration: 8477, Loss: 1.208115816116333\n",
      "Iteration: 8478, Loss: 1.2081081867218018\n",
      "Iteration: 8479, Loss: 1.2080988883972168\n",
      "Iteration: 8480, Loss: 1.2080793380737305\n",
      "Iteration: 8481, Loss: 1.2080684900283813\n",
      "Iteration: 8482, Loss: 1.208057165145874\n",
      "Iteration: 8483, Loss: 1.2080553770065308\n",
      "Iteration: 8484, Loss: 1.2080543041229248\n",
      "Iteration: 8485, Loss: 1.2080588340759277\n",
      "Iteration: 8486, Loss: 1.208064079284668\n",
      "Iteration: 8487, Loss: 1.2080729007720947\n",
      "Iteration: 8488, Loss: 1.2080848217010498\n",
      "Iteration: 8489, Loss: 1.2081032991409302\n",
      "Iteration: 8490, Loss: 1.2081327438354492\n",
      "Iteration: 8491, Loss: 1.2081809043884277\n",
      "Iteration: 8492, Loss: 1.208260416984558\n",
      "Iteration: 8493, Loss: 1.2083847522735596\n",
      "Iteration: 8494, Loss: 1.2085665464401245\n",
      "Iteration: 8495, Loss: 1.2087947130203247\n",
      "Iteration: 8496, Loss: 1.2089745998382568\n",
      "Iteration: 8497, Loss: 1.2089786529541016\n",
      "Iteration: 8498, Loss: 1.208723783493042\n",
      "Iteration: 8499, Loss: 1.208351492881775\n",
      "Iteration: 8500, Loss: 1.2080808877944946\n",
      "Iteration: 8501, Loss: 1.2080652713775635\n",
      "Iteration: 8502, Loss: 1.2082258462905884\n",
      "Iteration: 8503, Loss: 1.2082940340042114\n",
      "Iteration: 8504, Loss: 1.208167552947998\n",
      "Iteration: 8505, Loss: 1.208046793937683\n",
      "Iteration: 8506, Loss: 1.2081022262573242\n",
      "Iteration: 8507, Loss: 1.2081851959228516\n",
      "Iteration: 8508, Loss: 1.2081265449523926\n",
      "Iteration: 8509, Loss: 1.2080469131469727\n",
      "Iteration: 8510, Loss: 1.2080848217010498\n",
      "Iteration: 8511, Loss: 1.2081327438354492\n",
      "Iteration: 8512, Loss: 1.2080849409103394\n",
      "Iteration: 8513, Loss: 1.2080408334732056\n",
      "Iteration: 8514, Loss: 1.2080739736557007\n",
      "Iteration: 8515, Loss: 1.2080974578857422\n",
      "Iteration: 8516, Loss: 1.2080589532852173\n",
      "Iteration: 8517, Loss: 1.2080358266830444\n",
      "Iteration: 8518, Loss: 1.2080671787261963\n",
      "Iteration: 8519, Loss: 1.208083152770996\n",
      "Iteration: 8520, Loss: 1.2080504894256592\n",
      "Iteration: 8521, Loss: 1.2080270051956177\n",
      "Iteration: 8522, Loss: 1.2080410718917847\n",
      "Iteration: 8523, Loss: 1.2080599069595337\n",
      "Iteration: 8524, Loss: 1.2080471515655518\n",
      "Iteration: 8525, Loss: 1.208024263381958\n",
      "Iteration: 8526, Loss: 1.2080165147781372\n",
      "Iteration: 8527, Loss: 1.208025574684143\n",
      "Iteration: 8528, Loss: 1.2080291509628296\n",
      "Iteration: 8529, Loss: 1.208026647567749\n",
      "Iteration: 8530, Loss: 1.2080150842666626\n",
      "Iteration: 8531, Loss: 1.208009123802185\n",
      "Iteration: 8532, Loss: 1.2080073356628418\n",
      "Iteration: 8533, Loss: 1.2080119848251343\n",
      "Iteration: 8534, Loss: 1.2080141305923462\n",
      "Iteration: 8535, Loss: 1.2080148458480835\n",
      "Iteration: 8536, Loss: 1.2080107927322388\n",
      "Iteration: 8537, Loss: 1.2080062627792358\n",
      "Iteration: 8538, Loss: 1.2080005407333374\n",
      "Iteration: 8539, Loss: 1.2079970836639404\n",
      "Iteration: 8540, Loss: 1.2079943418502808\n",
      "Iteration: 8541, Loss: 1.2079932689666748\n",
      "Iteration: 8542, Loss: 1.2079945802688599\n",
      "Iteration: 8543, Loss: 1.2079980373382568\n",
      "Iteration: 8544, Loss: 1.2080042362213135\n",
      "Iteration: 8545, Loss: 1.2080135345458984\n",
      "Iteration: 8546, Loss: 1.208030104637146\n",
      "Iteration: 8547, Loss: 1.20805823802948\n",
      "Iteration: 8548, Loss: 1.208082914352417\n",
      "Iteration: 8549, Loss: 1.2080795764923096\n",
      "Iteration: 8550, Loss: 1.208054780960083\n",
      "Iteration: 8551, Loss: 1.2080349922180176\n",
      "Iteration: 8552, Loss: 1.208033800125122\n",
      "Iteration: 8553, Loss: 1.2080496549606323\n",
      "Iteration: 8554, Loss: 1.208092212677002\n",
      "Iteration: 8555, Loss: 1.2081810235977173\n",
      "Iteration: 8556, Loss: 1.2083542346954346\n",
      "Iteration: 8557, Loss: 1.208651065826416\n",
      "Iteration: 8558, Loss: 1.2090671062469482\n",
      "Iteration: 8559, Loss: 1.209390640258789\n",
      "Iteration: 8560, Loss: 1.2093104124069214\n",
      "Iteration: 8561, Loss: 1.208726406097412\n",
      "Iteration: 8562, Loss: 1.2081389427185059\n",
      "Iteration: 8563, Loss: 1.2079886198043823\n",
      "Iteration: 8564, Loss: 1.208256721496582\n",
      "Iteration: 8565, Loss: 1.2084095478057861\n",
      "Iteration: 8566, Loss: 1.20814847946167\n",
      "Iteration: 8567, Loss: 1.2079914808273315\n",
      "Iteration: 8568, Loss: 1.208170771598816\n",
      "Iteration: 8569, Loss: 1.208199381828308\n",
      "Iteration: 8570, Loss: 1.2080148458480835\n",
      "Iteration: 8571, Loss: 1.2080515623092651\n",
      "Iteration: 8572, Loss: 1.2081488370895386\n",
      "Iteration: 8573, Loss: 1.208035945892334\n",
      "Iteration: 8574, Loss: 1.2080039978027344\n",
      "Iteration: 8575, Loss: 1.2080917358398438\n",
      "Iteration: 8576, Loss: 1.2080364227294922\n",
      "Iteration: 8577, Loss: 1.2079769372940063\n",
      "Iteration: 8578, Loss: 1.2080355882644653\n",
      "Iteration: 8579, Loss: 1.2080233097076416\n",
      "Iteration: 8580, Loss: 1.2079541683197021\n",
      "Iteration: 8581, Loss: 1.207970380783081\n",
      "Iteration: 8582, Loss: 1.2079823017120361\n",
      "Iteration: 8583, Loss: 1.2079126834869385\n",
      "Iteration: 8584, Loss: 1.207849144935608\n",
      "Iteration: 8585, Loss: 1.207765817642212\n",
      "Iteration: 8586, Loss: 1.2073570489883423\n",
      "Iteration: 8587, Loss: 1.2031985521316528\n",
      "Iteration: 8588, Loss: 1.082046627998352\n",
      "Iteration: 8589, Loss: 1.082741379737854\n",
      "Iteration: 8590, Loss: 1.0206204652786255\n",
      "Iteration: 8591, Loss: 1.19410240650177\n",
      "Iteration: 8592, Loss: 0.9928881525993347\n",
      "Iteration: 8593, Loss: 0.9960111975669861\n",
      "Iteration: 8594, Loss: 0.9972839951515198\n",
      "Iteration: 8595, Loss: 0.9979884624481201\n",
      "Iteration: 8596, Loss: 0.9984689354896545\n",
      "Iteration: 8597, Loss: 0.9988120198249817\n",
      "Iteration: 8598, Loss: 0.9991116523742676\n",
      "Iteration: 8599, Loss: 0.9993464946746826\n",
      "Iteration: 8600, Loss: 0.9995183348655701\n",
      "Iteration: 8601, Loss: 0.999588668346405\n",
      "Iteration: 8602, Loss: 0.9994254112243652\n",
      "Iteration: 8603, Loss: 0.9989203810691833\n",
      "Iteration: 8604, Loss: 0.9941313862800598\n",
      "Iteration: 8605, Loss: 0.8296868801116943\n",
      "Iteration: 8606, Loss: 1.00102961063385\n",
      "Iteration: 8607, Loss: 1.2787660360336304\n",
      "Iteration: 8608, Loss: 1.25169837474823\n",
      "Iteration: 8609, Loss: 1.2520551681518555\n",
      "Iteration: 8610, Loss: 1.2526555061340332\n",
      "Iteration: 8611, Loss: 1.2914083003997803\n",
      "Iteration: 8612, Loss: 1.2528917789459229\n",
      "Iteration: 8613, Loss: 1.2531321048736572\n",
      "Iteration: 8614, Loss: 1.2533973455429077\n",
      "Iteration: 8615, Loss: 1.2536754608154297\n",
      "Iteration: 8616, Loss: 1.2539544105529785\n",
      "Iteration: 8617, Loss: 1.2542262077331543\n",
      "Iteration: 8618, Loss: 1.2544859647750854\n",
      "Iteration: 8619, Loss: 1.2547308206558228\n",
      "Iteration: 8620, Loss: 1.2549593448638916\n",
      "Iteration: 8621, Loss: 1.2551711797714233\n",
      "Iteration: 8622, Loss: 1.2550774812698364\n",
      "Iteration: 8623, Loss: 1.0055378675460815\n",
      "Iteration: 8624, Loss: 1.0056947469711304\n",
      "Iteration: 8625, Loss: 1.0058377981185913\n",
      "Iteration: 8626, Loss: 1.005967617034912\n",
      "Iteration: 8627, Loss: 1.0060853958129883\n",
      "Iteration: 8628, Loss: 1.0061918497085571\n",
      "Iteration: 8629, Loss: 1.006287932395935\n",
      "Iteration: 8630, Loss: 1.0063745975494385\n",
      "Iteration: 8631, Loss: 1.0064524412155151\n",
      "Iteration: 8632, Loss: 1.006522297859192\n",
      "Iteration: 8633, Loss: 1.0065852403640747\n",
      "Iteration: 8634, Loss: 1.0066466331481934\n",
      "Iteration: 8635, Loss: 1.006771445274353\n",
      "Iteration: 8636, Loss: 1.006929874420166\n",
      "Iteration: 8637, Loss: 1.0067861080169678\n",
      "Iteration: 8638, Loss: 1.0068110227584839\n",
      "Iteration: 8639, Loss: 1.0068415403366089\n",
      "Iteration: 8640, Loss: 1.0068689584732056\n",
      "Iteration: 8641, Loss: 1.0068930387496948\n",
      "Iteration: 8642, Loss: 1.0069142580032349\n",
      "Iteration: 8643, Loss: 1.0069327354431152\n",
      "Iteration: 8644, Loss: 1.0069488286972046\n",
      "Iteration: 8645, Loss: 1.0069626569747925\n",
      "Iteration: 8646, Loss: 1.0069745779037476\n",
      "Iteration: 8647, Loss: 1.0069845914840698\n",
      "Iteration: 8648, Loss: 1.0069931745529175\n",
      "Iteration: 8649, Loss: 1.007000207901001\n",
      "Iteration: 8650, Loss: 1.0070059299468994\n",
      "Iteration: 8651, Loss: 1.0070104598999023\n",
      "Iteration: 8652, Loss: 1.0070139169692993\n",
      "Iteration: 8653, Loss: 1.0070165395736694\n",
      "Iteration: 8654, Loss: 1.0070182085037231\n",
      "Iteration: 8655, Loss: 1.00701904296875\n",
      "Iteration: 8656, Loss: 1.007019281387329\n",
      "Iteration: 8657, Loss: 1.0070189237594604\n",
      "Iteration: 8658, Loss: 1.0070178508758545\n",
      "Iteration: 8659, Loss: 1.0070164203643799\n",
      "Iteration: 8660, Loss: 1.0070143938064575\n",
      "Iteration: 8661, Loss: 1.007012128829956\n",
      "Iteration: 8662, Loss: 1.0070093870162964\n",
      "Iteration: 8663, Loss: 1.007006287574768\n",
      "Iteration: 8664, Loss: 1.0070029497146606\n",
      "Iteration: 8665, Loss: 1.0069992542266846\n",
      "Iteration: 8666, Loss: 1.006995439529419\n",
      "Iteration: 8667, Loss: 1.0069913864135742\n",
      "Iteration: 8668, Loss: 1.0069870948791504\n",
      "Iteration: 8669, Loss: 1.0069825649261475\n",
      "Iteration: 8670, Loss: 1.0069780349731445\n",
      "Iteration: 8671, Loss: 1.0069732666015625\n",
      "Iteration: 8672, Loss: 1.006968379020691\n",
      "Iteration: 8673, Loss: 1.0069633722305298\n",
      "Iteration: 8674, Loss: 1.006958246231079\n",
      "Iteration: 8675, Loss: 1.0069531202316284\n",
      "Iteration: 8676, Loss: 1.0069478750228882\n",
      "Iteration: 8677, Loss: 1.0069425106048584\n",
      "Iteration: 8678, Loss: 1.0069371461868286\n",
      "Iteration: 8679, Loss: 1.0069316625595093\n",
      "Iteration: 8680, Loss: 1.00692617893219\n",
      "Iteration: 8681, Loss: 1.006920576095581\n",
      "Iteration: 8682, Loss: 1.0069149732589722\n",
      "Iteration: 8683, Loss: 1.0069093704223633\n",
      "Iteration: 8684, Loss: 1.0069037675857544\n",
      "Iteration: 8685, Loss: 1.006898045539856\n",
      "Iteration: 8686, Loss: 1.0068923234939575\n",
      "Iteration: 8687, Loss: 1.006886601448059\n",
      "Iteration: 8688, Loss: 1.006880760192871\n",
      "Iteration: 8689, Loss: 1.0068750381469727\n",
      "Iteration: 8690, Loss: 1.0068691968917847\n",
      "Iteration: 8691, Loss: 1.0068634748458862\n",
      "Iteration: 8692, Loss: 1.0068576335906982\n",
      "Iteration: 8693, Loss: 1.0068517923355103\n",
      "Iteration: 8694, Loss: 1.0068459510803223\n",
      "Iteration: 8695, Loss: 1.0068401098251343\n",
      "Iteration: 8696, Loss: 1.0068342685699463\n",
      "Iteration: 8697, Loss: 1.0068284273147583\n",
      "Iteration: 8698, Loss: 1.0068225860595703\n",
      "Iteration: 8699, Loss: 1.0068167448043823\n",
      "Iteration: 8700, Loss: 1.0068109035491943\n",
      "Iteration: 8701, Loss: 1.0068050622940063\n",
      "Iteration: 8702, Loss: 1.0067992210388184\n",
      "Iteration: 8703, Loss: 1.0067933797836304\n",
      "Iteration: 8704, Loss: 1.0067875385284424\n",
      "Iteration: 8705, Loss: 1.0067815780639648\n",
      "Iteration: 8706, Loss: 1.0067757368087769\n",
      "Iteration: 8707, Loss: 1.0067698955535889\n",
      "Iteration: 8708, Loss: 1.0067640542984009\n",
      "Iteration: 8709, Loss: 1.006758213043213\n",
      "Iteration: 8710, Loss: 1.006752371788025\n",
      "Iteration: 8711, Loss: 1.006746530532837\n",
      "Iteration: 8712, Loss: 1.006740689277649\n",
      "Iteration: 8713, Loss: 1.006734848022461\n",
      "Iteration: 8714, Loss: 1.006729006767273\n",
      "Iteration: 8715, Loss: 1.006723165512085\n",
      "Iteration: 8716, Loss: 1.006717324256897\n",
      "Iteration: 8717, Loss: 1.006711483001709\n",
      "Iteration: 8718, Loss: 1.0067057609558105\n",
      "Iteration: 8719, Loss: 1.0066999197006226\n",
      "Iteration: 8720, Loss: 1.0066940784454346\n",
      "Iteration: 8721, Loss: 1.0066882371902466\n",
      "Iteration: 8722, Loss: 1.0066823959350586\n",
      "Iteration: 8723, Loss: 1.0066766738891602\n",
      "Iteration: 8724, Loss: 1.0066708326339722\n",
      "Iteration: 8725, Loss: 1.0066649913787842\n",
      "Iteration: 8726, Loss: 1.0066592693328857\n",
      "Iteration: 8727, Loss: 1.0066534280776978\n",
      "Iteration: 8728, Loss: 1.0066477060317993\n",
      "Iteration: 8729, Loss: 1.0066418647766113\n",
      "Iteration: 8730, Loss: 1.006636142730713\n",
      "Iteration: 8731, Loss: 1.006630301475525\n",
      "Iteration: 8732, Loss: 1.0066245794296265\n",
      "Iteration: 8733, Loss: 1.006618857383728\n",
      "Iteration: 8734, Loss: 1.00661301612854\n",
      "Iteration: 8735, Loss: 1.0066072940826416\n",
      "Iteration: 8736, Loss: 1.0066015720367432\n",
      "Iteration: 8737, Loss: 1.0065957307815552\n",
      "Iteration: 8738, Loss: 1.0065900087356567\n",
      "Iteration: 8739, Loss: 1.0065842866897583\n",
      "Iteration: 8740, Loss: 1.0065785646438599\n",
      "Iteration: 8741, Loss: 1.0065728425979614\n",
      "Iteration: 8742, Loss: 1.006567120552063\n",
      "Iteration: 8743, Loss: 1.0065613985061646\n",
      "Iteration: 8744, Loss: 1.0065556764602661\n",
      "Iteration: 8745, Loss: 1.0065499544143677\n",
      "Iteration: 8746, Loss: 1.0065442323684692\n",
      "Iteration: 8747, Loss: 1.0065385103225708\n",
      "Iteration: 8748, Loss: 1.0065327882766724\n",
      "Iteration: 8749, Loss: 1.006527066230774\n",
      "Iteration: 8750, Loss: 1.006521463394165\n",
      "Iteration: 8751, Loss: 1.0065157413482666\n",
      "Iteration: 8752, Loss: 1.0065100193023682\n",
      "Iteration: 8753, Loss: 1.0065042972564697\n",
      "Iteration: 8754, Loss: 1.0064986944198608\n",
      "Iteration: 8755, Loss: 1.0064929723739624\n",
      "Iteration: 8756, Loss: 1.006487250328064\n",
      "Iteration: 8757, Loss: 1.006481647491455\n",
      "Iteration: 8758, Loss: 1.0064759254455566\n",
      "Iteration: 8759, Loss: 1.0064703226089478\n",
      "Iteration: 8760, Loss: 1.0064646005630493\n",
      "Iteration: 8761, Loss: 1.0064589977264404\n",
      "Iteration: 8762, Loss: 1.006453275680542\n",
      "Iteration: 8763, Loss: 1.006447672843933\n",
      "Iteration: 8764, Loss: 1.0064420700073242\n",
      "Iteration: 8765, Loss: 1.0064363479614258\n",
      "Iteration: 8766, Loss: 1.006430745124817\n",
      "Iteration: 8767, Loss: 1.006425142288208\n",
      "Iteration: 8768, Loss: 1.0064195394515991\n",
      "Iteration: 8769, Loss: 1.0064138174057007\n",
      "Iteration: 8770, Loss: 1.0064082145690918\n",
      "Iteration: 8771, Loss: 1.006402611732483\n",
      "Iteration: 8772, Loss: 1.006397008895874\n",
      "Iteration: 8773, Loss: 1.0063914060592651\n",
      "Iteration: 8774, Loss: 1.0063858032226562\n",
      "Iteration: 8775, Loss: 1.0063802003860474\n",
      "Iteration: 8776, Loss: 1.0063745975494385\n",
      "Iteration: 8777, Loss: 1.0063689947128296\n",
      "Iteration: 8778, Loss: 1.0063633918762207\n",
      "Iteration: 8779, Loss: 1.0063577890396118\n",
      "Iteration: 8780, Loss: 1.006352186203003\n",
      "Iteration: 8781, Loss: 1.0063467025756836\n",
      "Iteration: 8782, Loss: 1.0063410997390747\n",
      "Iteration: 8783, Loss: 1.0063354969024658\n",
      "Iteration: 8784, Loss: 1.006329894065857\n",
      "Iteration: 8785, Loss: 1.0063244104385376\n",
      "Iteration: 8786, Loss: 1.0063188076019287\n",
      "Iteration: 8787, Loss: 1.0063132047653198\n",
      "Iteration: 8788, Loss: 1.0063077211380005\n",
      "Iteration: 8789, Loss: 1.0063021183013916\n",
      "Iteration: 8790, Loss: 1.0062965154647827\n",
      "Iteration: 8791, Loss: 1.0062910318374634\n",
      "Iteration: 8792, Loss: 1.0062854290008545\n",
      "Iteration: 8793, Loss: 1.0062799453735352\n",
      "Iteration: 8794, Loss: 1.0062743425369263\n",
      "Iteration: 8795, Loss: 1.006268858909607\n",
      "Iteration: 8796, Loss: 1.0062633752822876\n",
      "Iteration: 8797, Loss: 1.0062577724456787\n",
      "Iteration: 8798, Loss: 1.0062522888183594\n",
      "Iteration: 8799, Loss: 1.00624680519104\n",
      "Iteration: 8800, Loss: 1.0062412023544312\n",
      "Iteration: 8801, Loss: 1.0062357187271118\n",
      "Iteration: 8802, Loss: 1.0062302350997925\n",
      "Iteration: 8803, Loss: 1.0062247514724731\n",
      "Iteration: 8804, Loss: 1.0062192678451538\n",
      "Iteration: 8805, Loss: 1.006213665008545\n",
      "Iteration: 8806, Loss: 1.0062081813812256\n",
      "Iteration: 8807, Loss: 1.0062026977539062\n",
      "Iteration: 8808, Loss: 1.006197214126587\n",
      "Iteration: 8809, Loss: 1.0061917304992676\n",
      "Iteration: 8810, Loss: 1.0061862468719482\n",
      "Iteration: 8811, Loss: 1.006180763244629\n",
      "Iteration: 8812, Loss: 1.0061752796173096\n",
      "Iteration: 8813, Loss: 1.0061697959899902\n",
      "Iteration: 8814, Loss: 1.006164312362671\n",
      "Iteration: 8815, Loss: 1.0061588287353516\n",
      "Iteration: 8816, Loss: 1.0061534643173218\n",
      "Iteration: 8817, Loss: 1.0061479806900024\n",
      "Iteration: 8818, Loss: 1.006142497062683\n",
      "Iteration: 8819, Loss: 1.0061370134353638\n",
      "Iteration: 8820, Loss: 1.006131649017334\n",
      "Iteration: 8821, Loss: 1.0061261653900146\n",
      "Iteration: 8822, Loss: 1.0061206817626953\n",
      "Iteration: 8823, Loss: 1.006115198135376\n",
      "Iteration: 8824, Loss: 1.0061098337173462\n",
      "Iteration: 8825, Loss: 1.0061043500900269\n",
      "Iteration: 8826, Loss: 1.006098985671997\n",
      "Iteration: 8827, Loss: 1.0060935020446777\n",
      "Iteration: 8828, Loss: 1.006088137626648\n",
      "Iteration: 8829, Loss: 1.0060826539993286\n",
      "Iteration: 8830, Loss: 1.0060772895812988\n",
      "Iteration: 8831, Loss: 1.0060718059539795\n",
      "Iteration: 8832, Loss: 1.0060664415359497\n",
      "Iteration: 8833, Loss: 1.0060609579086304\n",
      "Iteration: 8834, Loss: 1.0060555934906006\n",
      "Iteration: 8835, Loss: 1.0060502290725708\n",
      "Iteration: 8836, Loss: 1.0060447454452515\n",
      "Iteration: 8837, Loss: 1.0060393810272217\n",
      "Iteration: 8838, Loss: 1.006034016609192\n",
      "Iteration: 8839, Loss: 1.0060285329818726\n",
      "Iteration: 8840, Loss: 1.0060231685638428\n",
      "Iteration: 8841, Loss: 1.006017804145813\n",
      "Iteration: 8842, Loss: 1.0060124397277832\n",
      "Iteration: 8843, Loss: 1.0060070753097534\n",
      "Iteration: 8844, Loss: 1.0060017108917236\n",
      "Iteration: 8845, Loss: 1.0059962272644043\n",
      "Iteration: 8846, Loss: 1.0059908628463745\n",
      "Iteration: 8847, Loss: 1.0059854984283447\n",
      "Iteration: 8848, Loss: 1.005980134010315\n",
      "Iteration: 8849, Loss: 1.0059747695922852\n",
      "Iteration: 8850, Loss: 1.0059694051742554\n",
      "Iteration: 8851, Loss: 1.0059640407562256\n",
      "Iteration: 8852, Loss: 1.0059586763381958\n",
      "Iteration: 8853, Loss: 1.0059534311294556\n",
      "Iteration: 8854, Loss: 1.0059480667114258\n",
      "Iteration: 8855, Loss: 1.005942702293396\n",
      "Iteration: 8856, Loss: 1.0059373378753662\n",
      "Iteration: 8857, Loss: 1.0059319734573364\n",
      "Iteration: 8858, Loss: 1.0059266090393066\n",
      "Iteration: 8859, Loss: 1.0059213638305664\n",
      "Iteration: 8860, Loss: 1.0059159994125366\n",
      "Iteration: 8861, Loss: 1.0059106349945068\n",
      "Iteration: 8862, Loss: 1.0059053897857666\n",
      "Iteration: 8863, Loss: 1.0059000253677368\n",
      "Iteration: 8864, Loss: 1.005894660949707\n",
      "Iteration: 8865, Loss: 1.0058894157409668\n",
      "Iteration: 8866, Loss: 1.005884051322937\n",
      "Iteration: 8867, Loss: 1.0058786869049072\n",
      "Iteration: 8868, Loss: 1.005873441696167\n",
      "Iteration: 8869, Loss: 1.0058680772781372\n",
      "Iteration: 8870, Loss: 1.005862832069397\n",
      "Iteration: 8871, Loss: 1.0058574676513672\n",
      "Iteration: 8872, Loss: 1.005852222442627\n",
      "Iteration: 8873, Loss: 1.0058469772338867\n",
      "Iteration: 8874, Loss: 1.005841612815857\n",
      "Iteration: 8875, Loss: 1.0058363676071167\n",
      "Iteration: 8876, Loss: 1.005831003189087\n",
      "Iteration: 8877, Loss: 1.0058257579803467\n",
      "Iteration: 8878, Loss: 1.0058205127716064\n",
      "Iteration: 8879, Loss: 1.0058151483535767\n",
      "Iteration: 8880, Loss: 1.0058099031448364\n",
      "Iteration: 8881, Loss: 1.0058046579360962\n",
      "Iteration: 8882, Loss: 1.005799412727356\n",
      "Iteration: 8883, Loss: 1.0057940483093262\n",
      "Iteration: 8884, Loss: 1.005788803100586\n",
      "Iteration: 8885, Loss: 1.0057835578918457\n",
      "Iteration: 8886, Loss: 1.0057783126831055\n",
      "Iteration: 8887, Loss: 1.0057730674743652\n",
      "Iteration: 8888, Loss: 1.005767822265625\n",
      "Iteration: 8889, Loss: 1.0057625770568848\n",
      "Iteration: 8890, Loss: 1.0057573318481445\n",
      "Iteration: 8891, Loss: 1.0057520866394043\n",
      "Iteration: 8892, Loss: 1.005746841430664\n",
      "Iteration: 8893, Loss: 1.0057415962219238\n",
      "Iteration: 8894, Loss: 1.0057363510131836\n",
      "Iteration: 8895, Loss: 1.0057311058044434\n",
      "Iteration: 8896, Loss: 1.0057258605957031\n",
      "Iteration: 8897, Loss: 1.005720615386963\n",
      "Iteration: 8898, Loss: 1.0057153701782227\n",
      "Iteration: 8899, Loss: 1.0057101249694824\n",
      "Iteration: 8900, Loss: 1.0057048797607422\n",
      "Iteration: 8901, Loss: 1.0056997537612915\n",
      "Iteration: 8902, Loss: 1.0056945085525513\n",
      "Iteration: 8903, Loss: 1.005689263343811\n",
      "Iteration: 8904, Loss: 1.0056840181350708\n",
      "Iteration: 8905, Loss: 1.0056788921356201\n",
      "Iteration: 8906, Loss: 1.0056736469268799\n",
      "Iteration: 8907, Loss: 1.0056684017181396\n",
      "Iteration: 8908, Loss: 1.005663275718689\n",
      "Iteration: 8909, Loss: 1.0056580305099487\n",
      "Iteration: 8910, Loss: 1.0056527853012085\n",
      "Iteration: 8911, Loss: 1.0056476593017578\n",
      "Iteration: 8912, Loss: 1.0056424140930176\n",
      "Iteration: 8913, Loss: 1.005637288093567\n",
      "Iteration: 8914, Loss: 1.0056320428848267\n",
      "Iteration: 8915, Loss: 1.005626916885376\n",
      "Iteration: 8916, Loss: 1.0056216716766357\n",
      "Iteration: 8917, Loss: 1.005616545677185\n",
      "Iteration: 8918, Loss: 1.0056113004684448\n",
      "Iteration: 8919, Loss: 1.0056061744689941\n",
      "Iteration: 8920, Loss: 1.005600929260254\n",
      "Iteration: 8921, Loss: 1.0055958032608032\n",
      "Iteration: 8922, Loss: 1.0055906772613525\n",
      "Iteration: 8923, Loss: 1.0055854320526123\n",
      "Iteration: 8924, Loss: 1.0055803060531616\n",
      "Iteration: 8925, Loss: 1.005575180053711\n",
      "Iteration: 8926, Loss: 1.0055700540542603\n",
      "Iteration: 8927, Loss: 1.00556480884552\n",
      "Iteration: 8928, Loss: 1.0055596828460693\n",
      "Iteration: 8929, Loss: 1.0055545568466187\n",
      "Iteration: 8930, Loss: 1.005549430847168\n",
      "Iteration: 8931, Loss: 1.0055441856384277\n",
      "Iteration: 8932, Loss: 1.005539059638977\n",
      "Iteration: 8933, Loss: 1.0055339336395264\n",
      "Iteration: 8934, Loss: 1.0055288076400757\n",
      "Iteration: 8935, Loss: 1.005523681640625\n",
      "Iteration: 8936, Loss: 1.0055185556411743\n",
      "Iteration: 8937, Loss: 1.0055134296417236\n",
      "Iteration: 8938, Loss: 1.005508303642273\n",
      "Iteration: 8939, Loss: 1.0055031776428223\n",
      "Iteration: 8940, Loss: 1.0054980516433716\n",
      "Iteration: 8941, Loss: 1.005492925643921\n",
      "Iteration: 8942, Loss: 1.0054877996444702\n",
      "Iteration: 8943, Loss: 1.0054826736450195\n",
      "Iteration: 8944, Loss: 1.0054775476455688\n",
      "Iteration: 8945, Loss: 1.0054724216461182\n",
      "Iteration: 8946, Loss: 1.0054672956466675\n",
      "Iteration: 8947, Loss: 1.0054622888565063\n",
      "Iteration: 8948, Loss: 1.0054571628570557\n",
      "Iteration: 8949, Loss: 1.005452036857605\n",
      "Iteration: 8950, Loss: 1.0054469108581543\n",
      "Iteration: 8951, Loss: 1.0054417848587036\n",
      "Iteration: 8952, Loss: 1.0054367780685425\n",
      "Iteration: 8953, Loss: 1.0054316520690918\n",
      "Iteration: 8954, Loss: 1.0054265260696411\n",
      "Iteration: 8955, Loss: 1.0054214000701904\n",
      "Iteration: 8956, Loss: 1.0054163932800293\n",
      "Iteration: 8957, Loss: 1.0054112672805786\n",
      "Iteration: 8958, Loss: 1.0054062604904175\n",
      "Iteration: 8959, Loss: 1.0054011344909668\n",
      "Iteration: 8960, Loss: 1.0053960084915161\n",
      "Iteration: 8961, Loss: 1.005391001701355\n",
      "Iteration: 8962, Loss: 1.0053858757019043\n",
      "Iteration: 8963, Loss: 1.0053808689117432\n",
      "Iteration: 8964, Loss: 1.0053757429122925\n",
      "Iteration: 8965, Loss: 1.0053707361221313\n",
      "Iteration: 8966, Loss: 1.0053656101226807\n",
      "Iteration: 8967, Loss: 1.0053606033325195\n",
      "Iteration: 8968, Loss: 1.0053554773330688\n",
      "Iteration: 8969, Loss: 1.0053504705429077\n",
      "Iteration: 8970, Loss: 1.0053454637527466\n",
      "Iteration: 8971, Loss: 1.005340337753296\n",
      "Iteration: 8972, Loss: 1.0053353309631348\n",
      "Iteration: 8973, Loss: 1.0053303241729736\n",
      "Iteration: 8974, Loss: 1.005325198173523\n",
      "Iteration: 8975, Loss: 1.0053201913833618\n",
      "Iteration: 8976, Loss: 1.0053151845932007\n",
      "Iteration: 8977, Loss: 1.00531005859375\n",
      "Iteration: 8978, Loss: 1.0053050518035889\n",
      "Iteration: 8979, Loss: 1.0053000450134277\n",
      "Iteration: 8980, Loss: 1.0052950382232666\n",
      "Iteration: 8981, Loss: 1.0052900314331055\n",
      "Iteration: 8982, Loss: 1.0052849054336548\n",
      "Iteration: 8983, Loss: 1.0052798986434937\n",
      "Iteration: 8984, Loss: 1.0052748918533325\n",
      "Iteration: 8985, Loss: 1.0052698850631714\n",
      "Iteration: 8986, Loss: 1.0052648782730103\n",
      "Iteration: 8987, Loss: 1.0052598714828491\n",
      "Iteration: 8988, Loss: 1.005254864692688\n",
      "Iteration: 8989, Loss: 1.0052498579025269\n",
      "Iteration: 8990, Loss: 1.0052448511123657\n",
      "Iteration: 8991, Loss: 1.0052398443222046\n",
      "Iteration: 8992, Loss: 1.0052348375320435\n",
      "Iteration: 8993, Loss: 1.0052298307418823\n",
      "Iteration: 8994, Loss: 1.0052248239517212\n",
      "Iteration: 8995, Loss: 1.00521981716156\n",
      "Iteration: 8996, Loss: 1.005214810371399\n",
      "Iteration: 8997, Loss: 1.0052098035812378\n",
      "Iteration: 8998, Loss: 1.0052047967910767\n",
      "Iteration: 8999, Loss: 1.005199909210205\n",
      "Iteration: 9000, Loss: 1.005194902420044\n",
      "Iteration: 9001, Loss: 1.0051898956298828\n",
      "Iteration: 9002, Loss: 1.0051848888397217\n",
      "Iteration: 9003, Loss: 1.0051798820495605\n",
      "Iteration: 9004, Loss: 1.005174994468689\n",
      "Iteration: 9005, Loss: 1.0051699876785278\n",
      "Iteration: 9006, Loss: 1.0051649808883667\n",
      "Iteration: 9007, Loss: 1.0051599740982056\n",
      "Iteration: 9008, Loss: 1.005155086517334\n",
      "Iteration: 9009, Loss: 1.0051500797271729\n",
      "Iteration: 9010, Loss: 1.0051450729370117\n",
      "Iteration: 9011, Loss: 1.0051401853561401\n",
      "Iteration: 9012, Loss: 1.005135178565979\n",
      "Iteration: 9013, Loss: 1.0051302909851074\n",
      "Iteration: 9014, Loss: 1.0051252841949463\n",
      "Iteration: 9015, Loss: 1.0051203966140747\n",
      "Iteration: 9016, Loss: 1.0051153898239136\n",
      "Iteration: 9017, Loss: 1.0051103830337524\n",
      "Iteration: 9018, Loss: 1.0051054954528809\n",
      "Iteration: 9019, Loss: 1.0051006078720093\n",
      "Iteration: 9020, Loss: 1.0050956010818481\n",
      "Iteration: 9021, Loss: 1.0050907135009766\n",
      "Iteration: 9022, Loss: 1.0050857067108154\n",
      "Iteration: 9023, Loss: 1.0050808191299438\n",
      "Iteration: 9024, Loss: 1.0050758123397827\n",
      "Iteration: 9025, Loss: 1.0050709247589111\n",
      "Iteration: 9026, Loss: 1.0050660371780396\n",
      "Iteration: 9027, Loss: 1.0050610303878784\n",
      "Iteration: 9028, Loss: 1.0050561428070068\n",
      "Iteration: 9029, Loss: 1.0050512552261353\n",
      "Iteration: 9030, Loss: 1.0050462484359741\n",
      "Iteration: 9031, Loss: 1.0050413608551025\n",
      "Iteration: 9032, Loss: 1.005036473274231\n",
      "Iteration: 9033, Loss: 1.0050315856933594\n",
      "Iteration: 9034, Loss: 1.0050266981124878\n",
      "Iteration: 9035, Loss: 1.0050216913223267\n",
      "Iteration: 9036, Loss: 1.005016803741455\n",
      "Iteration: 9037, Loss: 1.0050119161605835\n",
      "Iteration: 9038, Loss: 1.005007028579712\n",
      "Iteration: 9039, Loss: 1.0050021409988403\n",
      "Iteration: 9040, Loss: 1.0049972534179688\n",
      "Iteration: 9041, Loss: 1.0049923658370972\n",
      "Iteration: 9042, Loss: 1.004987359046936\n",
      "Iteration: 9043, Loss: 1.0049824714660645\n",
      "Iteration: 9044, Loss: 1.0049775838851929\n",
      "Iteration: 9045, Loss: 1.0049726963043213\n",
      "Iteration: 9046, Loss: 1.0049678087234497\n",
      "Iteration: 9047, Loss: 1.0049629211425781\n",
      "Iteration: 9048, Loss: 1.0049580335617065\n",
      "Iteration: 9049, Loss: 1.0049532651901245\n",
      "Iteration: 9050, Loss: 1.004948377609253\n",
      "Iteration: 9051, Loss: 1.0049434900283813\n",
      "Iteration: 9052, Loss: 1.0049386024475098\n",
      "Iteration: 9053, Loss: 1.0049337148666382\n",
      "Iteration: 9054, Loss: 1.0049288272857666\n",
      "Iteration: 9055, Loss: 1.004923939704895\n",
      "Iteration: 9056, Loss: 1.0049190521240234\n",
      "Iteration: 9057, Loss: 1.0049142837524414\n",
      "Iteration: 9058, Loss: 1.0049093961715698\n",
      "Iteration: 9059, Loss: 1.0049045085906982\n",
      "Iteration: 9060, Loss: 1.0048996210098267\n",
      "Iteration: 9061, Loss: 1.0048948526382446\n",
      "Iteration: 9062, Loss: 1.004889965057373\n",
      "Iteration: 9063, Loss: 1.0048850774765015\n",
      "Iteration: 9064, Loss: 1.0048801898956299\n",
      "Iteration: 9065, Loss: 1.0048754215240479\n",
      "Iteration: 9066, Loss: 1.0048705339431763\n",
      "Iteration: 9067, Loss: 1.0048656463623047\n",
      "Iteration: 9068, Loss: 1.0048608779907227\n",
      "Iteration: 9069, Loss: 1.004855990409851\n",
      "Iteration: 9070, Loss: 1.004851222038269\n",
      "Iteration: 9071, Loss: 1.0048463344573975\n",
      "Iteration: 9072, Loss: 1.0048415660858154\n",
      "Iteration: 9073, Loss: 1.0048366785049438\n",
      "Iteration: 9074, Loss: 1.0048319101333618\n",
      "Iteration: 9075, Loss: 1.0048270225524902\n",
      "Iteration: 9076, Loss: 1.0048222541809082\n",
      "Iteration: 9077, Loss: 1.0048173666000366\n",
      "Iteration: 9078, Loss: 1.0048125982284546\n",
      "Iteration: 9079, Loss: 1.004807710647583\n",
      "Iteration: 9080, Loss: 1.004802942276001\n",
      "Iteration: 9081, Loss: 1.0047980546951294\n",
      "Iteration: 9082, Loss: 1.0047932863235474\n",
      "Iteration: 9083, Loss: 1.0047885179519653\n",
      "Iteration: 9084, Loss: 1.0047836303710938\n",
      "Iteration: 9085, Loss: 1.0047788619995117\n",
      "Iteration: 9086, Loss: 1.0047740936279297\n",
      "Iteration: 9087, Loss: 1.004769206047058\n",
      "Iteration: 9088, Loss: 1.004764437675476\n",
      "Iteration: 9089, Loss: 1.004759669303894\n",
      "Iteration: 9090, Loss: 1.004754900932312\n",
      "Iteration: 9091, Loss: 1.0047500133514404\n",
      "Iteration: 9092, Loss: 1.0047452449798584\n",
      "Iteration: 9093, Loss: 1.0047404766082764\n",
      "Iteration: 9094, Loss: 1.0047357082366943\n",
      "Iteration: 9095, Loss: 1.0047309398651123\n",
      "Iteration: 9096, Loss: 1.0047261714935303\n",
      "Iteration: 9097, Loss: 1.0047212839126587\n",
      "Iteration: 9098, Loss: 1.0047165155410767\n",
      "Iteration: 9099, Loss: 1.0047117471694946\n",
      "Iteration: 9100, Loss: 1.0047069787979126\n",
      "Iteration: 9101, Loss: 1.0047022104263306\n",
      "Iteration: 9102, Loss: 1.0046974420547485\n",
      "Iteration: 9103, Loss: 1.0046926736831665\n",
      "Iteration: 9104, Loss: 1.0046879053115845\n",
      "Iteration: 9105, Loss: 1.0046831369400024\n",
      "Iteration: 9106, Loss: 1.0046783685684204\n",
      "Iteration: 9107, Loss: 1.0046736001968384\n",
      "Iteration: 9108, Loss: 1.0046688318252563\n",
      "Iteration: 9109, Loss: 1.0046640634536743\n",
      "Iteration: 9110, Loss: 1.0046592950820923\n",
      "Iteration: 9111, Loss: 1.0046545267105103\n",
      "Iteration: 9112, Loss: 1.0046498775482178\n",
      "Iteration: 9113, Loss: 1.0046451091766357\n",
      "Iteration: 9114, Loss: 1.0046403408050537\n",
      "Iteration: 9115, Loss: 1.0046355724334717\n",
      "Iteration: 9116, Loss: 1.0046308040618896\n",
      "Iteration: 9117, Loss: 1.0046260356903076\n",
      "Iteration: 9118, Loss: 1.0046213865280151\n",
      "Iteration: 9119, Loss: 1.004616618156433\n",
      "Iteration: 9120, Loss: 1.004611849784851\n",
      "Iteration: 9121, Loss: 1.004607081413269\n",
      "Iteration: 9122, Loss: 1.0046024322509766\n",
      "Iteration: 9123, Loss: 1.0045976638793945\n",
      "Iteration: 9124, Loss: 1.0045928955078125\n",
      "Iteration: 9125, Loss: 1.00458824634552\n",
      "Iteration: 9126, Loss: 1.004583477973938\n",
      "Iteration: 9127, Loss: 1.004578709602356\n",
      "Iteration: 9128, Loss: 1.0045740604400635\n",
      "Iteration: 9129, Loss: 1.0045692920684814\n",
      "Iteration: 9130, Loss: 1.004564642906189\n",
      "Iteration: 9131, Loss: 1.004559874534607\n",
      "Iteration: 9132, Loss: 1.004555106163025\n",
      "Iteration: 9133, Loss: 1.0045504570007324\n",
      "Iteration: 9134, Loss: 1.0045456886291504\n",
      "Iteration: 9135, Loss: 1.004541039466858\n",
      "Iteration: 9136, Loss: 1.0045362710952759\n",
      "Iteration: 9137, Loss: 1.0045316219329834\n",
      "Iteration: 9138, Loss: 1.0045268535614014\n",
      "Iteration: 9139, Loss: 1.0045222043991089\n",
      "Iteration: 9140, Loss: 1.0045175552368164\n",
      "Iteration: 9141, Loss: 1.0045127868652344\n",
      "Iteration: 9142, Loss: 1.004508137702942\n",
      "Iteration: 9143, Loss: 1.0045033693313599\n",
      "Iteration: 9144, Loss: 1.0044987201690674\n",
      "Iteration: 9145, Loss: 1.004494071006775\n",
      "Iteration: 9146, Loss: 1.0044893026351929\n",
      "Iteration: 9147, Loss: 1.0044846534729004\n",
      "Iteration: 9148, Loss: 1.004480004310608\n",
      "Iteration: 9149, Loss: 1.0044753551483154\n",
      "Iteration: 9150, Loss: 1.0044705867767334\n",
      "Iteration: 9151, Loss: 1.004465937614441\n",
      "Iteration: 9152, Loss: 1.0044612884521484\n",
      "Iteration: 9153, Loss: 1.004456639289856\n",
      "Iteration: 9154, Loss: 1.004451870918274\n",
      "Iteration: 9155, Loss: 1.0044472217559814\n",
      "Iteration: 9156, Loss: 1.004442572593689\n",
      "Iteration: 9157, Loss: 1.0044379234313965\n",
      "Iteration: 9158, Loss: 1.004433274269104\n",
      "Iteration: 9159, Loss: 1.0044286251068115\n",
      "Iteration: 9160, Loss: 1.004423975944519\n",
      "Iteration: 9161, Loss: 1.004419207572937\n",
      "Iteration: 9162, Loss: 1.0044145584106445\n",
      "Iteration: 9163, Loss: 1.004409909248352\n",
      "Iteration: 9164, Loss: 1.0044052600860596\n",
      "Iteration: 9165, Loss: 1.004400610923767\n",
      "Iteration: 9166, Loss: 1.0043959617614746\n",
      "Iteration: 9167, Loss: 1.0043913125991821\n",
      "Iteration: 9168, Loss: 1.0043866634368896\n",
      "Iteration: 9169, Loss: 1.0043820142745972\n",
      "Iteration: 9170, Loss: 1.0043773651123047\n",
      "Iteration: 9171, Loss: 1.0043727159500122\n",
      "Iteration: 9172, Loss: 1.0043681859970093\n",
      "Iteration: 9173, Loss: 1.0043635368347168\n",
      "Iteration: 9174, Loss: 1.0043588876724243\n",
      "Iteration: 9175, Loss: 1.0043542385101318\n",
      "Iteration: 9176, Loss: 1.0043495893478394\n",
      "Iteration: 9177, Loss: 1.0043449401855469\n",
      "Iteration: 9178, Loss: 1.0043402910232544\n",
      "Iteration: 9179, Loss: 1.0043357610702515\n",
      "Iteration: 9180, Loss: 1.004331111907959\n",
      "Iteration: 9181, Loss: 1.0043264627456665\n",
      "Iteration: 9182, Loss: 1.004321813583374\n",
      "Iteration: 9183, Loss: 1.0043171644210815\n",
      "Iteration: 9184, Loss: 1.0043126344680786\n",
      "Iteration: 9185, Loss: 1.0043079853057861\n",
      "Iteration: 9186, Loss: 1.0043033361434937\n",
      "Iteration: 9187, Loss: 1.0042988061904907\n",
      "Iteration: 9188, Loss: 1.0042941570281982\n",
      "Iteration: 9189, Loss: 1.0042895078659058\n",
      "Iteration: 9190, Loss: 1.0042849779129028\n",
      "Iteration: 9191, Loss: 1.0042803287506104\n",
      "Iteration: 9192, Loss: 1.0042756795883179\n",
      "Iteration: 9193, Loss: 1.004271149635315\n",
      "Iteration: 9194, Loss: 1.0042665004730225\n",
      "Iteration: 9195, Loss: 1.0042619705200195\n",
      "Iteration: 9196, Loss: 1.004257321357727\n",
      "Iteration: 9197, Loss: 1.0042527914047241\n",
      "Iteration: 9198, Loss: 1.0042481422424316\n",
      "Iteration: 9199, Loss: 1.0042436122894287\n",
      "Iteration: 9200, Loss: 1.0042389631271362\n",
      "Iteration: 9201, Loss: 1.0042344331741333\n",
      "Iteration: 9202, Loss: 1.0042297840118408\n",
      "Iteration: 9203, Loss: 1.004225254058838\n",
      "Iteration: 9204, Loss: 1.0042206048965454\n",
      "Iteration: 9205, Loss: 1.0042160749435425\n",
      "Iteration: 9206, Loss: 1.0042115449905396\n",
      "Iteration: 9207, Loss: 1.004206895828247\n",
      "Iteration: 9208, Loss: 1.0042023658752441\n",
      "Iteration: 9209, Loss: 1.0041978359222412\n",
      "Iteration: 9210, Loss: 1.0041931867599487\n",
      "Iteration: 9211, Loss: 1.0041886568069458\n",
      "Iteration: 9212, Loss: 1.0041841268539429\n",
      "Iteration: 9213, Loss: 1.0041794776916504\n",
      "Iteration: 9214, Loss: 1.0041749477386475\n",
      "Iteration: 9215, Loss: 1.0041704177856445\n",
      "Iteration: 9216, Loss: 1.0041658878326416\n",
      "Iteration: 9217, Loss: 1.0041612386703491\n",
      "Iteration: 9218, Loss: 1.0041567087173462\n",
      "Iteration: 9219, Loss: 1.0041521787643433\n",
      "Iteration: 9220, Loss: 1.0041476488113403\n",
      "Iteration: 9221, Loss: 1.0041431188583374\n",
      "Iteration: 9222, Loss: 1.004138469696045\n",
      "Iteration: 9223, Loss: 1.004133939743042\n",
      "Iteration: 9224, Loss: 1.004129409790039\n",
      "Iteration: 9225, Loss: 1.0041248798370361\n",
      "Iteration: 9226, Loss: 1.0041203498840332\n",
      "Iteration: 9227, Loss: 1.0041158199310303\n",
      "Iteration: 9228, Loss: 1.0041112899780273\n",
      "Iteration: 9229, Loss: 1.0041067600250244\n",
      "Iteration: 9230, Loss: 1.0041022300720215\n",
      "Iteration: 9231, Loss: 1.0040977001190186\n",
      "Iteration: 9232, Loss: 1.0040931701660156\n",
      "Iteration: 9233, Loss: 1.0040886402130127\n",
      "Iteration: 9234, Loss: 1.0040841102600098\n",
      "Iteration: 9235, Loss: 1.0040795803070068\n",
      "Iteration: 9236, Loss: 1.004075050354004\n",
      "Iteration: 9237, Loss: 1.004070520401001\n",
      "Iteration: 9238, Loss: 1.004065990447998\n",
      "Iteration: 9239, Loss: 1.0040614604949951\n",
      "Iteration: 9240, Loss: 1.0040569305419922\n",
      "Iteration: 9241, Loss: 1.0040525197982788\n",
      "Iteration: 9242, Loss: 1.0040479898452759\n",
      "Iteration: 9243, Loss: 1.004043459892273\n",
      "Iteration: 9244, Loss: 1.00403892993927\n",
      "Iteration: 9245, Loss: 1.004034399986267\n",
      "Iteration: 9246, Loss: 1.0040298700332642\n",
      "Iteration: 9247, Loss: 1.0040254592895508\n",
      "Iteration: 9248, Loss: 1.0040209293365479\n",
      "Iteration: 9249, Loss: 1.004016399383545\n",
      "Iteration: 9250, Loss: 1.004011869430542\n",
      "Iteration: 9251, Loss: 1.0040074586868286\n",
      "Iteration: 9252, Loss: 1.0040029287338257\n",
      "Iteration: 9253, Loss: 1.0039983987808228\n",
      "Iteration: 9254, Loss: 1.0039939880371094\n",
      "Iteration: 9255, Loss: 1.0039894580841064\n",
      "Iteration: 9256, Loss: 1.0039849281311035\n",
      "Iteration: 9257, Loss: 1.0039805173873901\n",
      "Iteration: 9258, Loss: 1.0039759874343872\n",
      "Iteration: 9259, Loss: 1.0039715766906738\n",
      "Iteration: 9260, Loss: 1.003967046737671\n",
      "Iteration: 9261, Loss: 1.003962516784668\n",
      "Iteration: 9262, Loss: 1.0039581060409546\n",
      "Iteration: 9263, Loss: 1.0039535760879517\n",
      "Iteration: 9264, Loss: 1.0039491653442383\n",
      "Iteration: 9265, Loss: 1.0039446353912354\n",
      "Iteration: 9266, Loss: 1.003940224647522\n",
      "Iteration: 9267, Loss: 1.003935694694519\n",
      "Iteration: 9268, Loss: 1.0039312839508057\n",
      "Iteration: 9269, Loss: 1.0039267539978027\n",
      "Iteration: 9270, Loss: 1.0039223432540894\n",
      "Iteration: 9271, Loss: 1.003917932510376\n",
      "Iteration: 9272, Loss: 1.003913402557373\n",
      "Iteration: 9273, Loss: 1.0039089918136597\n",
      "Iteration: 9274, Loss: 1.0039044618606567\n",
      "Iteration: 9275, Loss: 1.0039000511169434\n",
      "Iteration: 9276, Loss: 1.00389564037323\n",
      "Iteration: 9277, Loss: 1.003891110420227\n",
      "Iteration: 9278, Loss: 1.0038866996765137\n",
      "Iteration: 9279, Loss: 1.0038822889328003\n",
      "Iteration: 9280, Loss: 1.0038777589797974\n",
      "Iteration: 9281, Loss: 1.003873348236084\n",
      "Iteration: 9282, Loss: 1.0038689374923706\n",
      "Iteration: 9283, Loss: 1.0038645267486572\n",
      "Iteration: 9284, Loss: 1.0038599967956543\n",
      "Iteration: 9285, Loss: 1.003855586051941\n",
      "Iteration: 9286, Loss: 1.0038511753082275\n",
      "Iteration: 9287, Loss: 1.0038467645645142\n",
      "Iteration: 9288, Loss: 1.0038423538208008\n",
      "Iteration: 9289, Loss: 1.0038379430770874\n",
      "Iteration: 9290, Loss: 1.0038334131240845\n",
      "Iteration: 9291, Loss: 1.003829002380371\n",
      "Iteration: 9292, Loss: 1.0038245916366577\n",
      "Iteration: 9293, Loss: 1.0038201808929443\n",
      "Iteration: 9294, Loss: 1.003815770149231\n",
      "Iteration: 9295, Loss: 1.0038113594055176\n",
      "Iteration: 9296, Loss: 1.0038069486618042\n",
      "Iteration: 9297, Loss: 1.0038025379180908\n",
      "Iteration: 9298, Loss: 1.0037981271743774\n",
      "Iteration: 9299, Loss: 1.003793716430664\n",
      "Iteration: 9300, Loss: 1.0037893056869507\n",
      "Iteration: 9301, Loss: 1.0037848949432373\n",
      "Iteration: 9302, Loss: 1.003780484199524\n",
      "Iteration: 9303, Loss: 1.0037760734558105\n",
      "Iteration: 9304, Loss: 1.0037716627120972\n",
      "Iteration: 9305, Loss: 1.0037672519683838\n",
      "Iteration: 9306, Loss: 1.0037628412246704\n",
      "Iteration: 9307, Loss: 1.003758430480957\n",
      "Iteration: 9308, Loss: 1.0037540197372437\n",
      "Iteration: 9309, Loss: 1.0037496089935303\n",
      "Iteration: 9310, Loss: 1.0037453174591064\n",
      "Iteration: 9311, Loss: 1.003740906715393\n",
      "Iteration: 9312, Loss: 1.0037364959716797\n",
      "Iteration: 9313, Loss: 1.0037320852279663\n",
      "Iteration: 9314, Loss: 1.003727674484253\n",
      "Iteration: 9315, Loss: 1.003723382949829\n",
      "Iteration: 9316, Loss: 1.0037189722061157\n",
      "Iteration: 9317, Loss: 1.0037145614624023\n",
      "Iteration: 9318, Loss: 1.003710150718689\n",
      "Iteration: 9319, Loss: 1.0037058591842651\n",
      "Iteration: 9320, Loss: 1.0037014484405518\n",
      "Iteration: 9321, Loss: 1.0036970376968384\n",
      "Iteration: 9322, Loss: 1.003692626953125\n",
      "Iteration: 9323, Loss: 1.0036883354187012\n",
      "Iteration: 9324, Loss: 1.0036839246749878\n",
      "Iteration: 9325, Loss: 1.003679633140564\n",
      "Iteration: 9326, Loss: 1.0036752223968506\n",
      "Iteration: 9327, Loss: 1.0036708116531372\n",
      "Iteration: 9328, Loss: 1.0036665201187134\n",
      "Iteration: 9329, Loss: 1.003662109375\n",
      "Iteration: 9330, Loss: 1.0036578178405762\n",
      "Iteration: 9331, Loss: 1.0036534070968628\n",
      "Iteration: 9332, Loss: 1.0036489963531494\n",
      "Iteration: 9333, Loss: 1.0036447048187256\n",
      "Iteration: 9334, Loss: 1.0036402940750122\n",
      "Iteration: 9335, Loss: 1.0036360025405884\n",
      "Iteration: 9336, Loss: 1.003631591796875\n",
      "Iteration: 9337, Loss: 1.0036273002624512\n",
      "Iteration: 9338, Loss: 1.0036228895187378\n",
      "Iteration: 9339, Loss: 1.003618597984314\n",
      "Iteration: 9340, Loss: 1.0036143064498901\n",
      "Iteration: 9341, Loss: 1.0036098957061768\n",
      "Iteration: 9342, Loss: 1.003605604171753\n",
      "Iteration: 9343, Loss: 1.0036011934280396\n",
      "Iteration: 9344, Loss: 1.0035969018936157\n",
      "Iteration: 9345, Loss: 1.003592610359192\n",
      "Iteration: 9346, Loss: 1.0035881996154785\n",
      "Iteration: 9347, Loss: 1.0035839080810547\n",
      "Iteration: 9348, Loss: 1.0035796165466309\n",
      "Iteration: 9349, Loss: 1.0035752058029175\n",
      "Iteration: 9350, Loss: 1.0035709142684937\n",
      "Iteration: 9351, Loss: 1.0035666227340698\n",
      "Iteration: 9352, Loss: 1.003562331199646\n",
      "Iteration: 9353, Loss: 1.0035579204559326\n",
      "Iteration: 9354, Loss: 1.0035536289215088\n",
      "Iteration: 9355, Loss: 1.003549337387085\n",
      "Iteration: 9356, Loss: 1.0035450458526611\n",
      "Iteration: 9357, Loss: 1.0035406351089478\n",
      "Iteration: 9358, Loss: 1.003536343574524\n",
      "Iteration: 9359, Loss: 1.0035320520401\n",
      "Iteration: 9360, Loss: 1.0035277605056763\n",
      "Iteration: 9361, Loss: 1.0035234689712524\n",
      "Iteration: 9362, Loss: 1.0035191774368286\n",
      "Iteration: 9363, Loss: 1.0035148859024048\n",
      "Iteration: 9364, Loss: 1.003510594367981\n",
      "Iteration: 9365, Loss: 1.0035061836242676\n",
      "Iteration: 9366, Loss: 1.0035018920898438\n",
      "Iteration: 9367, Loss: 1.00349760055542\n",
      "Iteration: 9368, Loss: 1.003493309020996\n",
      "Iteration: 9369, Loss: 1.0034890174865723\n",
      "Iteration: 9370, Loss: 1.0034847259521484\n",
      "Iteration: 9371, Loss: 1.0034804344177246\n",
      "Iteration: 9372, Loss: 1.0034761428833008\n",
      "Iteration: 9373, Loss: 1.003471851348877\n",
      "Iteration: 9374, Loss: 1.0034675598144531\n",
      "Iteration: 9375, Loss: 1.0034632682800293\n",
      "Iteration: 9376, Loss: 1.0034589767456055\n",
      "Iteration: 9377, Loss: 1.0034548044204712\n",
      "Iteration: 9378, Loss: 1.0034505128860474\n",
      "Iteration: 9379, Loss: 1.0034462213516235\n",
      "Iteration: 9380, Loss: 1.0034419298171997\n",
      "Iteration: 9381, Loss: 1.0034376382827759\n",
      "Iteration: 9382, Loss: 1.003433346748352\n",
      "Iteration: 9383, Loss: 1.0034290552139282\n",
      "Iteration: 9384, Loss: 1.003424882888794\n",
      "Iteration: 9385, Loss: 1.0034205913543701\n",
      "Iteration: 9386, Loss: 1.0034162998199463\n",
      "Iteration: 9387, Loss: 1.0034120082855225\n",
      "Iteration: 9388, Loss: 1.0034077167510986\n",
      "Iteration: 9389, Loss: 1.0034035444259644\n",
      "Iteration: 9390, Loss: 1.0033992528915405\n",
      "Iteration: 9391, Loss: 1.0033949613571167\n",
      "Iteration: 9392, Loss: 1.0033907890319824\n",
      "Iteration: 9393, Loss: 1.0033864974975586\n",
      "Iteration: 9394, Loss: 1.0033822059631348\n",
      "Iteration: 9395, Loss: 1.003377914428711\n",
      "Iteration: 9396, Loss: 1.0033737421035767\n",
      "Iteration: 9397, Loss: 1.0033694505691528\n",
      "Iteration: 9398, Loss: 1.0033652782440186\n",
      "Iteration: 9399, Loss: 1.0033609867095947\n",
      "Iteration: 9400, Loss: 1.003356695175171\n",
      "Iteration: 9401, Loss: 1.0033525228500366\n",
      "Iteration: 9402, Loss: 1.0033482313156128\n",
      "Iteration: 9403, Loss: 1.0033440589904785\n",
      "Iteration: 9404, Loss: 1.0033397674560547\n",
      "Iteration: 9405, Loss: 1.0033355951309204\n",
      "Iteration: 9406, Loss: 1.0033313035964966\n",
      "Iteration: 9407, Loss: 1.0033271312713623\n",
      "Iteration: 9408, Loss: 1.0033228397369385\n",
      "Iteration: 9409, Loss: 1.0033186674118042\n",
      "Iteration: 9410, Loss: 1.0033143758773804\n",
      "Iteration: 9411, Loss: 1.003310203552246\n",
      "Iteration: 9412, Loss: 1.0033059120178223\n",
      "Iteration: 9413, Loss: 1.003301739692688\n",
      "Iteration: 9414, Loss: 1.0032974481582642\n",
      "Iteration: 9415, Loss: 1.0032932758331299\n",
      "Iteration: 9416, Loss: 1.0032891035079956\n",
      "Iteration: 9417, Loss: 1.0032848119735718\n",
      "Iteration: 9418, Loss: 1.0032806396484375\n",
      "Iteration: 9419, Loss: 1.0032764673233032\n",
      "Iteration: 9420, Loss: 1.0032721757888794\n",
      "Iteration: 9421, Loss: 1.0032680034637451\n",
      "Iteration: 9422, Loss: 1.0032638311386108\n",
      "Iteration: 9423, Loss: 1.003259539604187\n",
      "Iteration: 9424, Loss: 1.0032553672790527\n",
      "Iteration: 9425, Loss: 1.0032511949539185\n",
      "Iteration: 9426, Loss: 1.0032470226287842\n",
      "Iteration: 9427, Loss: 1.0032427310943604\n",
      "Iteration: 9428, Loss: 1.003238558769226\n",
      "Iteration: 9429, Loss: 1.0032343864440918\n",
      "Iteration: 9430, Loss: 1.0032302141189575\n",
      "Iteration: 9431, Loss: 1.0032260417938232\n",
      "Iteration: 9432, Loss: 1.0032217502593994\n",
      "Iteration: 9433, Loss: 1.0032175779342651\n",
      "Iteration: 9434, Loss: 1.0032134056091309\n",
      "Iteration: 9435, Loss: 1.0032092332839966\n",
      "Iteration: 9436, Loss: 1.0032050609588623\n",
      "Iteration: 9437, Loss: 1.003200888633728\n",
      "Iteration: 9438, Loss: 1.0031967163085938\n",
      "Iteration: 9439, Loss: 1.0031925439834595\n",
      "Iteration: 9440, Loss: 1.0031883716583252\n",
      "Iteration: 9441, Loss: 1.0031840801239014\n",
      "Iteration: 9442, Loss: 1.003179907798767\n",
      "Iteration: 9443, Loss: 1.0031757354736328\n",
      "Iteration: 9444, Loss: 1.0031715631484985\n",
      "Iteration: 9445, Loss: 1.0031673908233643\n",
      "Iteration: 9446, Loss: 1.00316321849823\n",
      "Iteration: 9447, Loss: 1.0031590461730957\n",
      "Iteration: 9448, Loss: 1.003154993057251\n",
      "Iteration: 9449, Loss: 1.0031508207321167\n",
      "Iteration: 9450, Loss: 1.0031466484069824\n",
      "Iteration: 9451, Loss: 1.0031424760818481\n",
      "Iteration: 9452, Loss: 1.0031383037567139\n",
      "Iteration: 9453, Loss: 1.0031341314315796\n",
      "Iteration: 9454, Loss: 1.0031299591064453\n",
      "Iteration: 9455, Loss: 1.003125786781311\n",
      "Iteration: 9456, Loss: 1.0031216144561768\n",
      "Iteration: 9457, Loss: 1.003117561340332\n",
      "Iteration: 9458, Loss: 1.0031133890151978\n",
      "Iteration: 9459, Loss: 1.0031092166900635\n",
      "Iteration: 9460, Loss: 1.0031050443649292\n",
      "Iteration: 9461, Loss: 1.003100872039795\n",
      "Iteration: 9462, Loss: 1.0030968189239502\n",
      "Iteration: 9463, Loss: 1.003092646598816\n",
      "Iteration: 9464, Loss: 1.0030884742736816\n",
      "Iteration: 9465, Loss: 1.0030843019485474\n",
      "Iteration: 9466, Loss: 1.0030802488327026\n",
      "Iteration: 9467, Loss: 1.0030760765075684\n",
      "Iteration: 9468, Loss: 1.003071904182434\n",
      "Iteration: 9469, Loss: 1.0030678510665894\n",
      "Iteration: 9470, Loss: 1.003063678741455\n",
      "Iteration: 9471, Loss: 1.0030595064163208\n",
      "Iteration: 9472, Loss: 1.003055453300476\n",
      "Iteration: 9473, Loss: 1.0030512809753418\n",
      "Iteration: 9474, Loss: 1.0030471086502075\n",
      "Iteration: 9475, Loss: 1.0030430555343628\n",
      "Iteration: 9476, Loss: 1.0030388832092285\n",
      "Iteration: 9477, Loss: 1.0030348300933838\n",
      "Iteration: 9478, Loss: 1.0030306577682495\n",
      "Iteration: 9479, Loss: 1.0030266046524048\n",
      "Iteration: 9480, Loss: 1.0030224323272705\n",
      "Iteration: 9481, Loss: 1.0030182600021362\n",
      "Iteration: 9482, Loss: 1.0030142068862915\n",
      "Iteration: 9483, Loss: 1.0030100345611572\n",
      "Iteration: 9484, Loss: 1.0030059814453125\n",
      "Iteration: 9485, Loss: 1.0030019283294678\n",
      "Iteration: 9486, Loss: 1.0029977560043335\n",
      "Iteration: 9487, Loss: 1.0029937028884888\n",
      "Iteration: 9488, Loss: 1.0029895305633545\n",
      "Iteration: 9489, Loss: 1.0029854774475098\n",
      "Iteration: 9490, Loss: 1.0029813051223755\n",
      "Iteration: 9491, Loss: 1.0029772520065308\n",
      "Iteration: 9492, Loss: 1.002973198890686\n",
      "Iteration: 9493, Loss: 1.0029690265655518\n",
      "Iteration: 9494, Loss: 1.002964973449707\n",
      "Iteration: 9495, Loss: 1.0029609203338623\n",
      "Iteration: 9496, Loss: 1.002956748008728\n",
      "Iteration: 9497, Loss: 1.0029526948928833\n",
      "Iteration: 9498, Loss: 1.0029486417770386\n",
      "Iteration: 9499, Loss: 1.0029444694519043\n",
      "Iteration: 9500, Loss: 1.0029404163360596\n",
      "Iteration: 9501, Loss: 1.0029363632202148\n",
      "Iteration: 9502, Loss: 1.0029323101043701\n",
      "Iteration: 9503, Loss: 1.0029281377792358\n",
      "Iteration: 9504, Loss: 1.0029240846633911\n",
      "Iteration: 9505, Loss: 1.0029200315475464\n",
      "Iteration: 9506, Loss: 1.0029159784317017\n",
      "Iteration: 9507, Loss: 1.002911925315857\n",
      "Iteration: 9508, Loss: 1.0029077529907227\n",
      "Iteration: 9509, Loss: 1.002903699874878\n",
      "Iteration: 9510, Loss: 1.0028996467590332\n",
      "Iteration: 9511, Loss: 1.0028955936431885\n",
      "Iteration: 9512, Loss: 1.0028915405273438\n",
      "Iteration: 9513, Loss: 1.002887487411499\n",
      "Iteration: 9514, Loss: 1.0028834342956543\n",
      "Iteration: 9515, Loss: 1.0028793811798096\n",
      "Iteration: 9516, Loss: 1.0028753280639648\n",
      "Iteration: 9517, Loss: 1.0028711557388306\n",
      "Iteration: 9518, Loss: 1.0028671026229858\n",
      "Iteration: 9519, Loss: 1.0028630495071411\n",
      "Iteration: 9520, Loss: 1.0028589963912964\n",
      "Iteration: 9521, Loss: 1.0028549432754517\n",
      "Iteration: 9522, Loss: 1.002850890159607\n",
      "Iteration: 9523, Loss: 1.0028468370437622\n",
      "Iteration: 9524, Loss: 1.0028427839279175\n",
      "Iteration: 9525, Loss: 1.0028388500213623\n",
      "Iteration: 9526, Loss: 1.0028347969055176\n",
      "Iteration: 9527, Loss: 1.0028307437896729\n",
      "Iteration: 9528, Loss: 1.0028266906738281\n",
      "Iteration: 9529, Loss: 1.0028226375579834\n",
      "Iteration: 9530, Loss: 1.0028185844421387\n",
      "Iteration: 9531, Loss: 1.002814531326294\n",
      "Iteration: 9532, Loss: 1.0028104782104492\n",
      "Iteration: 9533, Loss: 1.0028064250946045\n",
      "Iteration: 9534, Loss: 1.0028024911880493\n",
      "Iteration: 9535, Loss: 1.0027984380722046\n",
      "Iteration: 9536, Loss: 1.0027943849563599\n",
      "Iteration: 9537, Loss: 1.0027903318405151\n",
      "Iteration: 9538, Loss: 1.0027862787246704\n",
      "Iteration: 9539, Loss: 1.0027823448181152\n",
      "Iteration: 9540, Loss: 1.0027782917022705\n",
      "Iteration: 9541, Loss: 1.0027742385864258\n",
      "Iteration: 9542, Loss: 1.002770185470581\n",
      "Iteration: 9543, Loss: 1.0027662515640259\n",
      "Iteration: 9544, Loss: 1.0027621984481812\n",
      "Iteration: 9545, Loss: 1.0027581453323364\n",
      "Iteration: 9546, Loss: 1.0027542114257812\n",
      "Iteration: 9547, Loss: 1.0027501583099365\n",
      "Iteration: 9548, Loss: 1.0027461051940918\n",
      "Iteration: 9549, Loss: 1.0027421712875366\n",
      "Iteration: 9550, Loss: 1.002738118171692\n",
      "Iteration: 9551, Loss: 1.0027340650558472\n",
      "Iteration: 9552, Loss: 1.002730131149292\n",
      "Iteration: 9553, Loss: 1.0027260780334473\n",
      "Iteration: 9554, Loss: 1.002722144126892\n",
      "Iteration: 9555, Loss: 1.0027180910110474\n",
      "Iteration: 9556, Loss: 1.0027140378952026\n",
      "Iteration: 9557, Loss: 1.0027101039886475\n",
      "Iteration: 9558, Loss: 1.0027060508728027\n",
      "Iteration: 9559, Loss: 1.0027021169662476\n",
      "Iteration: 9560, Loss: 1.0026980638504028\n",
      "Iteration: 9561, Loss: 1.0026941299438477\n",
      "Iteration: 9562, Loss: 1.002690076828003\n",
      "Iteration: 9563, Loss: 1.0026861429214478\n",
      "Iteration: 9564, Loss: 1.002682089805603\n",
      "Iteration: 9565, Loss: 1.0026781558990479\n",
      "Iteration: 9566, Loss: 1.0026742219924927\n",
      "Iteration: 9567, Loss: 1.002670168876648\n",
      "Iteration: 9568, Loss: 1.0026662349700928\n",
      "Iteration: 9569, Loss: 1.002662181854248\n",
      "Iteration: 9570, Loss: 1.0026582479476929\n",
      "Iteration: 9571, Loss: 1.0026543140411377\n",
      "Iteration: 9572, Loss: 1.002650260925293\n",
      "Iteration: 9573, Loss: 1.0026463270187378\n",
      "Iteration: 9574, Loss: 1.0026423931121826\n",
      "Iteration: 9575, Loss: 1.002638339996338\n",
      "Iteration: 9576, Loss: 1.0026344060897827\n",
      "Iteration: 9577, Loss: 1.0026304721832275\n",
      "Iteration: 9578, Loss: 1.0026264190673828\n",
      "Iteration: 9579, Loss: 1.0026224851608276\n",
      "Iteration: 9580, Loss: 1.0026185512542725\n",
      "Iteration: 9581, Loss: 1.0026146173477173\n",
      "Iteration: 9582, Loss: 1.0026105642318726\n",
      "Iteration: 9583, Loss: 1.0026066303253174\n",
      "Iteration: 9584, Loss: 1.0026026964187622\n",
      "Iteration: 9585, Loss: 1.002598762512207\n",
      "Iteration: 9586, Loss: 1.0025948286056519\n",
      "Iteration: 9587, Loss: 1.0025907754898071\n",
      "Iteration: 9588, Loss: 1.002586841583252\n",
      "Iteration: 9589, Loss: 1.0025829076766968\n",
      "Iteration: 9590, Loss: 1.0025789737701416\n",
      "Iteration: 9591, Loss: 1.0025750398635864\n",
      "Iteration: 9592, Loss: 1.0025711059570312\n",
      "Iteration: 9593, Loss: 1.002567172050476\n",
      "Iteration: 9594, Loss: 1.002563238143921\n",
      "Iteration: 9595, Loss: 1.0025593042373657\n",
      "Iteration: 9596, Loss: 1.0025553703308105\n",
      "Iteration: 9597, Loss: 1.0025514364242554\n",
      "Iteration: 9598, Loss: 1.0025473833084106\n",
      "Iteration: 9599, Loss: 1.0025434494018555\n",
      "Iteration: 9600, Loss: 1.0025395154953003\n",
      "Iteration: 9601, Loss: 1.0025355815887451\n",
      "Iteration: 9602, Loss: 1.00253164768219\n",
      "Iteration: 9603, Loss: 1.0025278329849243\n",
      "Iteration: 9604, Loss: 1.0025238990783691\n",
      "Iteration: 9605, Loss: 1.002519965171814\n",
      "Iteration: 9606, Loss: 1.0025160312652588\n",
      "Iteration: 9607, Loss: 1.0025120973587036\n",
      "Iteration: 9608, Loss: 1.0025081634521484\n",
      "Iteration: 9609, Loss: 1.0025042295455933\n",
      "Iteration: 9610, Loss: 1.002500295639038\n",
      "Iteration: 9611, Loss: 1.002496361732483\n",
      "Iteration: 9612, Loss: 1.0024924278259277\n",
      "Iteration: 9613, Loss: 1.002488613128662\n",
      "Iteration: 9614, Loss: 1.002484679222107\n",
      "Iteration: 9615, Loss: 1.0024807453155518\n",
      "Iteration: 9616, Loss: 1.0024768114089966\n",
      "Iteration: 9617, Loss: 1.0024728775024414\n",
      "Iteration: 9618, Loss: 1.0024689435958862\n",
      "Iteration: 9619, Loss: 1.0024651288986206\n",
      "Iteration: 9620, Loss: 1.0024611949920654\n",
      "Iteration: 9621, Loss: 1.0024572610855103\n",
      "Iteration: 9622, Loss: 1.002453327178955\n",
      "Iteration: 9623, Loss: 1.0024495124816895\n",
      "Iteration: 9624, Loss: 1.0024455785751343\n",
      "Iteration: 9625, Loss: 1.002441644668579\n",
      "Iteration: 9626, Loss: 1.0024378299713135\n",
      "Iteration: 9627, Loss: 1.0024338960647583\n",
      "Iteration: 9628, Loss: 1.0024299621582031\n",
      "Iteration: 9629, Loss: 1.0024261474609375\n",
      "Iteration: 9630, Loss: 1.0024222135543823\n",
      "Iteration: 9631, Loss: 1.0024182796478271\n",
      "Iteration: 9632, Loss: 1.0024144649505615\n",
      "Iteration: 9633, Loss: 1.0024105310440063\n",
      "Iteration: 9634, Loss: 1.0024067163467407\n",
      "Iteration: 9635, Loss: 1.0024027824401855\n",
      "Iteration: 9636, Loss: 1.0023988485336304\n",
      "Iteration: 9637, Loss: 1.0023950338363647\n",
      "Iteration: 9638, Loss: 1.0023910999298096\n",
      "Iteration: 9639, Loss: 1.002387285232544\n",
      "Iteration: 9640, Loss: 1.0023833513259888\n",
      "Iteration: 9641, Loss: 1.0023795366287231\n",
      "Iteration: 9642, Loss: 1.002375602722168\n",
      "Iteration: 9643, Loss: 1.0023717880249023\n",
      "Iteration: 9644, Loss: 1.0023678541183472\n",
      "Iteration: 9645, Loss: 1.0023640394210815\n",
      "Iteration: 9646, Loss: 1.002360224723816\n",
      "Iteration: 9647, Loss: 1.0023562908172607\n",
      "Iteration: 9648, Loss: 1.0023524761199951\n",
      "Iteration: 9649, Loss: 1.00234854221344\n",
      "Iteration: 9650, Loss: 1.0023447275161743\n",
      "Iteration: 9651, Loss: 1.0023409128189087\n",
      "Iteration: 9652, Loss: 1.0023369789123535\n",
      "Iteration: 9653, Loss: 1.002333164215088\n",
      "Iteration: 9654, Loss: 1.0023292303085327\n",
      "Iteration: 9655, Loss: 1.002325415611267\n",
      "Iteration: 9656, Loss: 1.0023216009140015\n",
      "Iteration: 9657, Loss: 1.0023176670074463\n",
      "Iteration: 9658, Loss: 1.0023138523101807\n",
      "Iteration: 9659, Loss: 1.002310037612915\n",
      "Iteration: 9660, Loss: 1.0023062229156494\n",
      "Iteration: 9661, Loss: 1.0023022890090942\n",
      "Iteration: 9662, Loss: 1.0022984743118286\n",
      "Iteration: 9663, Loss: 1.002294659614563\n",
      "Iteration: 9664, Loss: 1.0022908449172974\n",
      "Iteration: 9665, Loss: 1.0022869110107422\n",
      "Iteration: 9666, Loss: 1.0022830963134766\n",
      "Iteration: 9667, Loss: 1.002279281616211\n",
      "Iteration: 9668, Loss: 1.0022754669189453\n",
      "Iteration: 9669, Loss: 1.0022716522216797\n",
      "Iteration: 9670, Loss: 1.002267837524414\n",
      "Iteration: 9671, Loss: 1.0022639036178589\n",
      "Iteration: 9672, Loss: 1.0022600889205933\n",
      "Iteration: 9673, Loss: 1.0022562742233276\n",
      "Iteration: 9674, Loss: 1.002252459526062\n",
      "Iteration: 9675, Loss: 1.0022486448287964\n",
      "Iteration: 9676, Loss: 1.0022448301315308\n",
      "Iteration: 9677, Loss: 1.0022410154342651\n",
      "Iteration: 9678, Loss: 1.0022372007369995\n",
      "Iteration: 9679, Loss: 1.0022333860397339\n",
      "Iteration: 9680, Loss: 1.0022295713424683\n",
      "Iteration: 9681, Loss: 1.0022257566452026\n",
      "Iteration: 9682, Loss: 1.002221941947937\n",
      "Iteration: 9683, Loss: 1.0022181272506714\n",
      "Iteration: 9684, Loss: 1.0022143125534058\n",
      "Iteration: 9685, Loss: 1.0022104978561401\n",
      "Iteration: 9686, Loss: 1.0022066831588745\n",
      "Iteration: 9687, Loss: 1.0022028684616089\n",
      "Iteration: 9688, Loss: 1.0021990537643433\n",
      "Iteration: 9689, Loss: 1.0021952390670776\n",
      "Iteration: 9690, Loss: 1.002191424369812\n",
      "Iteration: 9691, Loss: 1.0021876096725464\n",
      "Iteration: 9692, Loss: 1.0021837949752808\n",
      "Iteration: 9693, Loss: 1.0021799802780151\n",
      "Iteration: 9694, Loss: 1.002176284790039\n",
      "Iteration: 9695, Loss: 1.0021724700927734\n",
      "Iteration: 9696, Loss: 1.0021686553955078\n",
      "Iteration: 9697, Loss: 1.0021648406982422\n",
      "Iteration: 9698, Loss: 1.0021610260009766\n",
      "Iteration: 9699, Loss: 1.002157211303711\n",
      "Iteration: 9700, Loss: 1.0021535158157349\n",
      "Iteration: 9701, Loss: 1.0021497011184692\n",
      "Iteration: 9702, Loss: 1.0021458864212036\n",
      "Iteration: 9703, Loss: 1.002142071723938\n",
      "Iteration: 9704, Loss: 1.002138376235962\n",
      "Iteration: 9705, Loss: 1.0021345615386963\n",
      "Iteration: 9706, Loss: 1.0021307468414307\n",
      "Iteration: 9707, Loss: 1.002126932144165\n",
      "Iteration: 9708, Loss: 1.002123236656189\n",
      "Iteration: 9709, Loss: 1.0021194219589233\n",
      "Iteration: 9710, Loss: 1.0021156072616577\n",
      "Iteration: 9711, Loss: 1.0021119117736816\n",
      "Iteration: 9712, Loss: 1.002108097076416\n",
      "Iteration: 9713, Loss: 1.0021042823791504\n",
      "Iteration: 9714, Loss: 1.0021005868911743\n",
      "Iteration: 9715, Loss: 1.0020967721939087\n",
      "Iteration: 9716, Loss: 1.0020930767059326\n",
      "Iteration: 9717, Loss: 1.002089262008667\n",
      "Iteration: 9718, Loss: 1.0020854473114014\n",
      "Iteration: 9719, Loss: 1.0020817518234253\n",
      "Iteration: 9720, Loss: 1.0020779371261597\n",
      "Iteration: 9721, Loss: 1.0020742416381836\n",
      "Iteration: 9722, Loss: 1.002070426940918\n",
      "Iteration: 9723, Loss: 1.002066731452942\n",
      "Iteration: 9724, Loss: 1.0020629167556763\n",
      "Iteration: 9725, Loss: 1.0020592212677002\n",
      "Iteration: 9726, Loss: 1.0020554065704346\n",
      "Iteration: 9727, Loss: 1.0020517110824585\n",
      "Iteration: 9728, Loss: 1.0020478963851929\n",
      "Iteration: 9729, Loss: 1.0020442008972168\n",
      "Iteration: 9730, Loss: 1.0020403861999512\n",
      "Iteration: 9731, Loss: 1.002036690711975\n",
      "Iteration: 9732, Loss: 1.002032995223999\n",
      "Iteration: 9733, Loss: 1.0020291805267334\n",
      "Iteration: 9734, Loss: 1.0020254850387573\n",
      "Iteration: 9735, Loss: 1.0020216703414917\n",
      "Iteration: 9736, Loss: 1.0020179748535156\n",
      "Iteration: 9737, Loss: 1.0020142793655396\n",
      "Iteration: 9738, Loss: 1.002010464668274\n",
      "Iteration: 9739, Loss: 1.0020067691802979\n",
      "Iteration: 9740, Loss: 1.0020030736923218\n",
      "Iteration: 9741, Loss: 1.0019992589950562\n",
      "Iteration: 9742, Loss: 1.00199556350708\n",
      "Iteration: 9743, Loss: 1.001991868019104\n",
      "Iteration: 9744, Loss: 1.001988172531128\n",
      "Iteration: 9745, Loss: 1.0019843578338623\n",
      "Iteration: 9746, Loss: 1.0019806623458862\n",
      "Iteration: 9747, Loss: 1.0019769668579102\n",
      "Iteration: 9748, Loss: 1.001973271369934\n",
      "Iteration: 9749, Loss: 1.0019694566726685\n",
      "Iteration: 9750, Loss: 1.0019657611846924\n",
      "Iteration: 9751, Loss: 1.0019620656967163\n",
      "Iteration: 9752, Loss: 1.0019583702087402\n",
      "Iteration: 9753, Loss: 1.0019546747207642\n",
      "Iteration: 9754, Loss: 1.0019508600234985\n",
      "Iteration: 9755, Loss: 1.0019471645355225\n",
      "Iteration: 9756, Loss: 1.0019434690475464\n",
      "Iteration: 9757, Loss: 1.0019397735595703\n",
      "Iteration: 9758, Loss: 1.0019360780715942\n",
      "Iteration: 9759, Loss: 1.0019323825836182\n",
      "Iteration: 9760, Loss: 1.001928687095642\n",
      "Iteration: 9761, Loss: 1.001924991607666\n",
      "Iteration: 9762, Loss: 1.0019211769104004\n",
      "Iteration: 9763, Loss: 1.0019174814224243\n",
      "Iteration: 9764, Loss: 1.0019137859344482\n",
      "Iteration: 9765, Loss: 1.0019100904464722\n",
      "Iteration: 9766, Loss: 1.001906394958496\n",
      "Iteration: 9767, Loss: 1.00190269947052\n",
      "Iteration: 9768, Loss: 1.001899003982544\n",
      "Iteration: 9769, Loss: 1.0018953084945679\n",
      "Iteration: 9770, Loss: 1.0018914937973022\n",
      "Iteration: 9771, Loss: 1.0018877983093262\n",
      "Iteration: 9772, Loss: 1.00188410282135\n",
      "Iteration: 9773, Loss: 1.001880407333374\n",
      "Iteration: 9774, Loss: 1.001876711845398\n",
      "Iteration: 9775, Loss: 1.0018731355667114\n",
      "Iteration: 9776, Loss: 1.0018694400787354\n",
      "Iteration: 9777, Loss: 1.0018657445907593\n",
      "Iteration: 9778, Loss: 1.0018620491027832\n",
      "Iteration: 9779, Loss: 1.0018583536148071\n",
      "Iteration: 9780, Loss: 1.001854658126831\n",
      "Iteration: 9781, Loss: 1.001850962638855\n",
      "Iteration: 9782, Loss: 1.001847267150879\n",
      "Iteration: 9783, Loss: 1.0018435716629028\n",
      "Iteration: 9784, Loss: 1.0018398761749268\n",
      "Iteration: 9785, Loss: 1.0018361806869507\n",
      "Iteration: 9786, Loss: 1.0018326044082642\n",
      "Iteration: 9787, Loss: 1.001828908920288\n",
      "Iteration: 9788, Loss: 1.001825213432312\n",
      "Iteration: 9789, Loss: 1.001821517944336\n",
      "Iteration: 9790, Loss: 1.0018179416656494\n",
      "Iteration: 9791, Loss: 1.0018142461776733\n",
      "Iteration: 9792, Loss: 1.0018105506896973\n",
      "Iteration: 9793, Loss: 1.0018069744110107\n",
      "Iteration: 9794, Loss: 1.0018032789230347\n",
      "Iteration: 9795, Loss: 1.0017995834350586\n",
      "Iteration: 9796, Loss: 1.0017958879470825\n",
      "Iteration: 9797, Loss: 1.001792311668396\n",
      "Iteration: 9798, Loss: 1.00178861618042\n",
      "Iteration: 9799, Loss: 1.0017849206924438\n",
      "Iteration: 9800, Loss: 1.0017812252044678\n",
      "Iteration: 9801, Loss: 1.0017776489257812\n",
      "Iteration: 9802, Loss: 1.0017739534378052\n",
      "Iteration: 9803, Loss: 1.001770257949829\n",
      "Iteration: 9804, Loss: 1.0017666816711426\n",
      "Iteration: 9805, Loss: 1.0017629861831665\n",
      "Iteration: 9806, Loss: 1.00175940990448\n",
      "Iteration: 9807, Loss: 1.001755714416504\n",
      "Iteration: 9808, Loss: 1.0017520189285278\n",
      "Iteration: 9809, Loss: 1.0017484426498413\n",
      "Iteration: 9810, Loss: 1.0017447471618652\n",
      "Iteration: 9811, Loss: 1.0017410516738892\n",
      "Iteration: 9812, Loss: 1.001737356185913\n",
      "Iteration: 9813, Loss: 1.0017338991165161\n",
      "Iteration: 9814, Loss: 1.00173020362854\n",
      "Iteration: 9815, Loss: 1.0017266273498535\n",
      "Iteration: 9816, Loss: 1.0017229318618774\n",
      "Iteration: 9817, Loss: 1.001719355583191\n",
      "Iteration: 9818, Loss: 1.0017156600952148\n",
      "Iteration: 9819, Loss: 1.0017120838165283\n",
      "Iteration: 9820, Loss: 1.0017082691192627\n",
      "Iteration: 9821, Loss: 1.0017046928405762\n",
      "Iteration: 9822, Loss: 1.0017009973526\n",
      "Iteration: 9823, Loss: 1.001697301864624\n",
      "Iteration: 9824, Loss: 1.001693606376648\n",
      "Iteration: 9825, Loss: 1.0016900300979614\n",
      "Iteration: 9826, Loss: 1.0016863346099854\n",
      "Iteration: 9827, Loss: 1.0016826391220093\n",
      "Iteration: 9828, Loss: 1.0016790628433228\n",
      "Iteration: 9829, Loss: 1.0016754865646362\n",
      "Iteration: 9830, Loss: 1.0016717910766602\n",
      "Iteration: 9831, Loss: 1.0016682147979736\n",
      "Iteration: 9832, Loss: 1.0016645193099976\n",
      "Iteration: 9833, Loss: 1.001660943031311\n",
      "Iteration: 9834, Loss: 1.0016573667526245\n",
      "Iteration: 9835, Loss: 1.0016535520553589\n",
      "Iteration: 9836, Loss: 1.0016499757766724\n",
      "Iteration: 9837, Loss: 1.0016463994979858\n",
      "Iteration: 9838, Loss: 1.0016427040100098\n",
      "Iteration: 9839, Loss: 1.0016390085220337\n",
      "Iteration: 9840, Loss: 1.0016353130340576\n",
      "Iteration: 9841, Loss: 1.001631736755371\n",
      "Iteration: 9842, Loss: 1.001628041267395\n",
      "Iteration: 9843, Loss: 1.001624345779419\n",
      "Iteration: 9844, Loss: 1.0016207695007324\n",
      "Iteration: 9845, Loss: 1.001617193222046\n",
      "Iteration: 9846, Loss: 1.0016134977340698\n",
      "Iteration: 9847, Loss: 1.0016099214553833\n",
      "Iteration: 9848, Loss: 1.0016061067581177\n",
      "Iteration: 9849, Loss: 1.0016025304794312\n",
      "Iteration: 9850, Loss: 1.001598834991455\n",
      "Iteration: 9851, Loss: 1.001595139503479\n",
      "Iteration: 9852, Loss: 1.0015915632247925\n",
      "Iteration: 9853, Loss: 1.0015877485275269\n",
      "Iteration: 9854, Loss: 1.0015841722488403\n",
      "Iteration: 9855, Loss: 1.0015804767608643\n",
      "Iteration: 9856, Loss: 1.0015769004821777\n",
      "Iteration: 9857, Loss: 1.0015732049942017\n",
      "Iteration: 9858, Loss: 1.0015696287155151\n",
      "Iteration: 9859, Loss: 1.0015658140182495\n",
      "Iteration: 9860, Loss: 1.0015621185302734\n",
      "Iteration: 9861, Loss: 1.0015584230422974\n",
      "Iteration: 9862, Loss: 1.0015547275543213\n",
      "Iteration: 9863, Loss: 1.0015510320663452\n",
      "Iteration: 9864, Loss: 1.0015473365783691\n",
      "Iteration: 9865, Loss: 1.001543641090393\n",
      "Iteration: 9866, Loss: 1.001539945602417\n",
      "Iteration: 9867, Loss: 1.001536250114441\n",
      "Iteration: 9868, Loss: 1.0015326738357544\n",
      "Iteration: 9869, Loss: 1.0015287399291992\n",
      "Iteration: 9870, Loss: 1.0015250444412231\n",
      "Iteration: 9871, Loss: 1.001521348953247\n",
      "Iteration: 9872, Loss: 1.001517653465271\n",
      "Iteration: 9873, Loss: 1.0015138387680054\n",
      "Iteration: 9874, Loss: 1.0015100240707397\n",
      "Iteration: 9875, Loss: 1.0015063285827637\n",
      "Iteration: 9876, Loss: 1.0015023946762085\n",
      "Iteration: 9877, Loss: 1.0014986991882324\n",
      "Iteration: 9878, Loss: 1.0014948844909668\n",
      "Iteration: 9879, Loss: 1.0014910697937012\n",
      "Iteration: 9880, Loss: 1.0014872550964355\n",
      "Iteration: 9881, Loss: 1.0014833211898804\n",
      "Iteration: 9882, Loss: 1.0014795064926147\n",
      "Iteration: 9883, Loss: 1.0014755725860596\n",
      "Iteration: 9884, Loss: 1.0014715194702148\n",
      "Iteration: 9885, Loss: 1.0014677047729492\n",
      "Iteration: 9886, Loss: 1.0014636516571045\n",
      "Iteration: 9887, Loss: 1.0014598369598389\n",
      "Iteration: 9888, Loss: 1.0014556646347046\n",
      "Iteration: 9889, Loss: 1.0014516115188599\n",
      "Iteration: 9890, Loss: 1.0014474391937256\n",
      "Iteration: 9891, Loss: 1.0014432668685913\n",
      "Iteration: 9892, Loss: 1.001439094543457\n",
      "Iteration: 9893, Loss: 1.0014348030090332\n",
      "Iteration: 9894, Loss: 1.0014303922653198\n",
      "Iteration: 9895, Loss: 1.0014259815216064\n",
      "Iteration: 9896, Loss: 1.0014216899871826\n",
      "Iteration: 9897, Loss: 1.0014171600341797\n",
      "Iteration: 9898, Loss: 1.0014123916625977\n",
      "Iteration: 9899, Loss: 1.0014076232910156\n",
      "Iteration: 9900, Loss: 1.001402497291565\n",
      "Iteration: 9901, Loss: 1.0013973712921143\n",
      "Iteration: 9902, Loss: 1.001391887664795\n",
      "Iteration: 9903, Loss: 1.001386284828186\n",
      "Iteration: 9904, Loss: 1.0013803243637085\n",
      "Iteration: 9905, Loss: 1.0013740062713623\n",
      "Iteration: 9906, Loss: 1.0013670921325684\n",
      "Iteration: 9907, Loss: 1.0013595819473267\n",
      "Iteration: 9908, Loss: 1.0013515949249268\n",
      "Iteration: 9909, Loss: 1.0013445615768433\n",
      "Iteration: 9910, Loss: 1.001334309577942\n",
      "Iteration: 9911, Loss: 1.0013225078582764\n",
      "Iteration: 9912, Loss: 1.0013084411621094\n",
      "Iteration: 9913, Loss: 1.0012925863265991\n",
      "Iteration: 9914, Loss: 1.0012706518173218\n",
      "Iteration: 9915, Loss: 1.0012413263320923\n",
      "Iteration: 9916, Loss: 1.0011996030807495\n",
      "Iteration: 9917, Loss: 1.0011358261108398\n",
      "Iteration: 9918, Loss: 1.0010274648666382\n",
      "Iteration: 9919, Loss: 1.0008255243301392\n",
      "Iteration: 9920, Loss: 1.0003504753112793\n",
      "Iteration: 9921, Loss: 0.9987576007843018\n",
      "Iteration: 9922, Loss: 0.9857770204544067\n",
      "Iteration: 9923, Loss: 0.756791353225708\n",
      "Iteration: 9924, Loss: 0.7513222098350525\n",
      "Iteration: 9925, Loss: 0.7514721155166626\n",
      "Iteration: 9926, Loss: 0.7522278428077698\n",
      "Iteration: 9927, Loss: 0.7543776631355286\n",
      "Iteration: 9928, Loss: 0.7551661729812622\n",
      "Iteration: 9929, Loss: 0.7531142234802246\n",
      "Iteration: 9930, Loss: 0.7519054412841797\n",
      "Iteration: 9931, Loss: 0.7515610456466675\n",
      "Iteration: 9932, Loss: 0.7514955997467041\n",
      "Iteration: 9933, Loss: 0.751498818397522\n",
      "Iteration: 9934, Loss: 0.7515178322792053\n",
      "Iteration: 9935, Loss: 0.7515401244163513\n",
      "Iteration: 9936, Loss: 0.7515619993209839\n",
      "Iteration: 9937, Loss: 0.7515824437141418\n",
      "Iteration: 9938, Loss: 0.751600980758667\n",
      "Iteration: 9939, Loss: 0.7516180276870728\n",
      "Iteration: 9940, Loss: 0.7516334652900696\n",
      "Iteration: 9941, Loss: 0.7516472339630127\n",
      "Iteration: 9942, Loss: 0.7516594529151917\n",
      "Iteration: 9943, Loss: 0.7516703009605408\n",
      "Iteration: 9944, Loss: 0.7516798377037048\n",
      "Iteration: 9945, Loss: 0.7516881823539734\n",
      "Iteration: 9946, Loss: 0.7516953945159912\n",
      "Iteration: 9947, Loss: 0.7517015337944031\n",
      "Iteration: 9948, Loss: 0.7517068982124329\n",
      "Iteration: 9949, Loss: 0.7517112493515015\n",
      "Iteration: 9950, Loss: 0.751714825630188\n",
      "Iteration: 9951, Loss: 0.7517176866531372\n",
      "Iteration: 9952, Loss: 0.7517199516296387\n",
      "Iteration: 9953, Loss: 0.7517216205596924\n",
      "Iteration: 9954, Loss: 0.7517226934432983\n",
      "Iteration: 9955, Loss: 0.7517232894897461\n",
      "Iteration: 9956, Loss: 0.7517234086990356\n",
      "Iteration: 9957, Loss: 0.7517231702804565\n",
      "Iteration: 9958, Loss: 0.7517225742340088\n",
      "Iteration: 9959, Loss: 0.7517216205596924\n",
      "Iteration: 9960, Loss: 0.7517203688621521\n",
      "Iteration: 9961, Loss: 0.7517189383506775\n",
      "Iteration: 9962, Loss: 0.751717209815979\n",
      "Iteration: 9963, Loss: 0.7517152428627014\n",
      "Iteration: 9964, Loss: 0.7517130374908447\n",
      "Iteration: 9965, Loss: 0.7517107129096985\n",
      "Iteration: 9966, Loss: 0.7517082095146179\n",
      "Iteration: 9967, Loss: 0.7517055869102478\n",
      "Iteration: 9968, Loss: 0.7517027854919434\n",
      "Iteration: 9969, Loss: 0.7516999244689941\n",
      "Iteration: 9970, Loss: 0.7516969442367554\n",
      "Iteration: 9971, Loss: 0.751693844795227\n",
      "Iteration: 9972, Loss: 0.751690685749054\n",
      "Iteration: 9973, Loss: 0.7516875863075256\n",
      "Iteration: 9974, Loss: 0.7516842484474182\n",
      "Iteration: 9975, Loss: 0.7516809105873108\n",
      "Iteration: 9976, Loss: 0.7516774535179138\n",
      "Iteration: 9977, Loss: 0.7516739964485168\n",
      "Iteration: 9978, Loss: 0.7516704797744751\n",
      "Iteration: 9979, Loss: 0.7516669034957886\n",
      "Iteration: 9980, Loss: 0.751663327217102\n",
      "Iteration: 9981, Loss: 0.7516597509384155\n",
      "Iteration: 9982, Loss: 0.7516561150550842\n",
      "Iteration: 9983, Loss: 0.7516524195671082\n",
      "Iteration: 9984, Loss: 0.7516487240791321\n",
      "Iteration: 9985, Loss: 0.751645028591156\n",
      "Iteration: 9986, Loss: 0.7516412734985352\n",
      "Iteration: 9987, Loss: 0.7516375780105591\n",
      "Iteration: 9988, Loss: 0.7516338229179382\n",
      "Iteration: 9989, Loss: 0.7516300678253174\n",
      "Iteration: 9990, Loss: 0.7516262531280518\n",
      "Iteration: 9991, Loss: 0.7516224980354309\n",
      "Iteration: 9992, Loss: 0.7516186833381653\n",
      "Iteration: 9993, Loss: 0.7516148686408997\n",
      "Iteration: 9994, Loss: 0.751611053943634\n",
      "Iteration: 9995, Loss: 0.7516072392463684\n",
      "Iteration: 9996, Loss: 0.7516034245491028\n",
      "Iteration: 9997, Loss: 0.7515995502471924\n",
      "Iteration: 9998, Loss: 0.7515957355499268\n",
      "Iteration: 9999, Loss: 0.7515919208526611\n",
      "Iteration: 10000, Loss: 0.7515881061553955\n",
      "Iteration: 10001, Loss: 0.7515842318534851\n",
      "Iteration: 10002, Loss: 0.7515804171562195\n",
      "Iteration: 10003, Loss: 0.7515765428543091\n",
      "Iteration: 10004, Loss: 0.7515727281570435\n",
      "Iteration: 10005, Loss: 0.7515689134597778\n",
      "Iteration: 10006, Loss: 0.7515650391578674\n",
      "Iteration: 10007, Loss: 0.7515612840652466\n",
      "Iteration: 10008, Loss: 0.7515574097633362\n",
      "Iteration: 10009, Loss: 0.7515535950660706\n",
      "Iteration: 10010, Loss: 0.7515497803688049\n",
      "Iteration: 10011, Loss: 0.7515459060668945\n",
      "Iteration: 10012, Loss: 0.7515421509742737\n",
      "Iteration: 10013, Loss: 0.7515383958816528\n",
      "Iteration: 10014, Loss: 0.7515345811843872\n",
      "Iteration: 10015, Loss: 0.7515307068824768\n",
      "Iteration: 10016, Loss: 0.7515268921852112\n",
      "Iteration: 10017, Loss: 0.7515230774879456\n",
      "Iteration: 10018, Loss: 0.7515192627906799\n",
      "Iteration: 10019, Loss: 0.7515154480934143\n",
      "Iteration: 10020, Loss: 0.7515116930007935\n",
      "Iteration: 10021, Loss: 0.7515078783035278\n",
      "Iteration: 10022, Loss: 0.7515040636062622\n",
      "Iteration: 10023, Loss: 0.7515002489089966\n",
      "Iteration: 10024, Loss: 0.751496434211731\n",
      "Iteration: 10025, Loss: 0.7514926195144653\n",
      "Iteration: 10026, Loss: 0.7514888048171997\n",
      "Iteration: 10027, Loss: 0.7514850497245789\n",
      "Iteration: 10028, Loss: 0.7514812350273132\n",
      "Iteration: 10029, Loss: 0.7514774203300476\n",
      "Iteration: 10030, Loss: 0.7514736652374268\n",
      "Iteration: 10031, Loss: 0.7514698505401611\n",
      "Iteration: 10032, Loss: 0.7514660358428955\n",
      "Iteration: 10033, Loss: 0.7514622807502747\n",
      "Iteration: 10034, Loss: 0.751458466053009\n",
      "Iteration: 10035, Loss: 0.7514547109603882\n",
      "Iteration: 10036, Loss: 0.7514509558677673\n",
      "Iteration: 10037, Loss: 0.7514471411705017\n",
      "Iteration: 10038, Loss: 0.7514433860778809\n",
      "Iteration: 10039, Loss: 0.75143963098526\n",
      "Iteration: 10040, Loss: 0.7514358162879944\n",
      "Iteration: 10041, Loss: 0.7514321804046631\n",
      "Iteration: 10042, Loss: 0.7514284253120422\n",
      "Iteration: 10043, Loss: 0.7514246702194214\n",
      "Iteration: 10044, Loss: 0.7514209151268005\n",
      "Iteration: 10045, Loss: 0.7514171600341797\n",
      "Iteration: 10046, Loss: 0.7514134049415588\n",
      "Iteration: 10047, Loss: 0.751409649848938\n",
      "Iteration: 10048, Loss: 0.7514059543609619\n",
      "Iteration: 10049, Loss: 0.7514021992683411\n",
      "Iteration: 10050, Loss: 0.7513984441757202\n",
      "Iteration: 10051, Loss: 0.7513947486877441\n",
      "Iteration: 10052, Loss: 0.7513909935951233\n",
      "Iteration: 10053, Loss: 0.7513872981071472\n",
      "Iteration: 10054, Loss: 0.7513836026191711\n",
      "Iteration: 10055, Loss: 0.7513798475265503\n",
      "Iteration: 10056, Loss: 0.7513761520385742\n",
      "Iteration: 10057, Loss: 0.7513723969459534\n",
      "Iteration: 10058, Loss: 0.7513687014579773\n",
      "Iteration: 10059, Loss: 0.7513649463653564\n",
      "Iteration: 10060, Loss: 0.7513612508773804\n",
      "Iteration: 10061, Loss: 0.7513575553894043\n",
      "Iteration: 10062, Loss: 0.7513538002967834\n",
      "Iteration: 10063, Loss: 0.7513501048088074\n",
      "Iteration: 10064, Loss: 0.7513464093208313\n",
      "Iteration: 10065, Loss: 0.7513427138328552\n",
      "Iteration: 10066, Loss: 0.7513390779495239\n",
      "Iteration: 10067, Loss: 0.7513353824615479\n",
      "Iteration: 10068, Loss: 0.7513316869735718\n",
      "Iteration: 10069, Loss: 0.7513280510902405\n",
      "Iteration: 10070, Loss: 0.7513243556022644\n",
      "Iteration: 10071, Loss: 0.7513206601142883\n",
      "Iteration: 10072, Loss: 0.7513169646263123\n",
      "Iteration: 10073, Loss: 0.7513132691383362\n",
      "Iteration: 10074, Loss: 0.7513096332550049\n",
      "Iteration: 10075, Loss: 0.7513059973716736\n",
      "Iteration: 10076, Loss: 0.7513023018836975\n",
      "Iteration: 10077, Loss: 0.7512986660003662\n",
      "Iteration: 10078, Loss: 0.7512950897216797\n",
      "Iteration: 10079, Loss: 0.7512913942337036\n",
      "Iteration: 10080, Loss: 0.7512877583503723\n",
      "Iteration: 10081, Loss: 0.751284122467041\n",
      "Iteration: 10082, Loss: 0.7512804269790649\n",
      "Iteration: 10083, Loss: 0.7512767910957336\n",
      "Iteration: 10084, Loss: 0.7512730956077576\n",
      "Iteration: 10085, Loss: 0.7512694597244263\n",
      "Iteration: 10086, Loss: 0.751265823841095\n",
      "Iteration: 10087, Loss: 0.7512621879577637\n",
      "Iteration: 10088, Loss: 0.7512585520744324\n",
      "Iteration: 10089, Loss: 0.7512549161911011\n",
      "Iteration: 10090, Loss: 0.751251220703125\n",
      "Iteration: 10091, Loss: 0.7512475848197937\n",
      "Iteration: 10092, Loss: 0.7512440085411072\n",
      "Iteration: 10093, Loss: 0.7512403726577759\n",
      "Iteration: 10094, Loss: 0.7512367367744446\n",
      "Iteration: 10095, Loss: 0.7512331008911133\n",
      "Iteration: 10096, Loss: 0.751229465007782\n",
      "Iteration: 10097, Loss: 0.7512258887290955\n",
      "Iteration: 10098, Loss: 0.7512223124504089\n",
      "Iteration: 10099, Loss: 0.7512186765670776\n",
      "Iteration: 10100, Loss: 0.7512150406837463\n",
      "Iteration: 10101, Loss: 0.7512115240097046\n",
      "Iteration: 10102, Loss: 0.7512079477310181\n",
      "Iteration: 10103, Loss: 0.7512043714523315\n",
      "Iteration: 10104, Loss: 0.7512007355690002\n",
      "Iteration: 10105, Loss: 0.7511971592903137\n",
      "Iteration: 10106, Loss: 0.7511935234069824\n",
      "Iteration: 10107, Loss: 0.7511899471282959\n",
      "Iteration: 10108, Loss: 0.7511863708496094\n",
      "Iteration: 10109, Loss: 0.7511829137802124\n",
      "Iteration: 10110, Loss: 0.7511792778968811\n",
      "Iteration: 10111, Loss: 0.7511757612228394\n",
      "Iteration: 10112, Loss: 0.7511721849441528\n",
      "Iteration: 10113, Loss: 0.7511686086654663\n",
      "Iteration: 10114, Loss: 0.7511650323867798\n",
      "Iteration: 10115, Loss: 0.7511614561080933\n",
      "Iteration: 10116, Loss: 0.7511578798294067\n",
      "Iteration: 10117, Loss: 0.751154363155365\n",
      "Iteration: 10118, Loss: 0.7511508464813232\n",
      "Iteration: 10119, Loss: 0.7511472702026367\n",
      "Iteration: 10120, Loss: 0.7511436939239502\n",
      "Iteration: 10121, Loss: 0.7511401176452637\n",
      "Iteration: 10122, Loss: 0.7511366009712219\n",
      "Iteration: 10123, Loss: 0.7511330246925354\n",
      "Iteration: 10124, Loss: 0.7511295080184937\n",
      "Iteration: 10125, Loss: 0.7511259913444519\n",
      "Iteration: 10126, Loss: 0.7511224150657654\n",
      "Iteration: 10127, Loss: 0.7511188983917236\n",
      "Iteration: 10128, Loss: 0.7511155009269714\n",
      "Iteration: 10129, Loss: 0.7511119246482849\n",
      "Iteration: 10130, Loss: 0.7511084079742432\n",
      "Iteration: 10131, Loss: 0.7511049509048462\n",
      "Iteration: 10132, Loss: 0.7511014938354492\n",
      "Iteration: 10133, Loss: 0.7510979175567627\n",
      "Iteration: 10134, Loss: 0.751094400882721\n",
      "Iteration: 10135, Loss: 0.7510908842086792\n",
      "Iteration: 10136, Loss: 0.7510874271392822\n",
      "Iteration: 10137, Loss: 0.7510839104652405\n",
      "Iteration: 10138, Loss: 0.7510803937911987\n",
      "Iteration: 10139, Loss: 0.7510769367218018\n",
      "Iteration: 10140, Loss: 0.7510734796524048\n",
      "Iteration: 10141, Loss: 0.7510700225830078\n",
      "Iteration: 10142, Loss: 0.7510665059089661\n",
      "Iteration: 10143, Loss: 0.7510630488395691\n",
      "Iteration: 10144, Loss: 0.7510597109794617\n",
      "Iteration: 10145, Loss: 0.7510561943054199\n",
      "Iteration: 10146, Loss: 0.751052737236023\n",
      "Iteration: 10147, Loss: 0.7510492205619812\n",
      "Iteration: 10148, Loss: 0.7510457634925842\n",
      "Iteration: 10149, Loss: 0.7510422468185425\n",
      "Iteration: 10150, Loss: 0.7510389089584351\n",
      "Iteration: 10151, Loss: 0.7510354518890381\n",
      "Iteration: 10152, Loss: 0.7510321140289307\n",
      "Iteration: 10153, Loss: 0.7510286569595337\n",
      "Iteration: 10154, Loss: 0.7510251998901367\n",
      "Iteration: 10155, Loss: 0.7510217428207397\n",
      "Iteration: 10156, Loss: 0.7510182857513428\n",
      "Iteration: 10157, Loss: 0.7510148882865906\n",
      "Iteration: 10158, Loss: 0.7510115504264832\n",
      "Iteration: 10159, Loss: 0.7510080933570862\n",
      "Iteration: 10160, Loss: 0.7510047554969788\n",
      "Iteration: 10161, Loss: 0.7510012984275818\n",
      "Iteration: 10162, Loss: 0.7509978413581848\n",
      "Iteration: 10163, Loss: 0.7509945034980774\n",
      "Iteration: 10164, Loss: 0.7509911060333252\n",
      "Iteration: 10165, Loss: 0.7509877681732178\n",
      "Iteration: 10166, Loss: 0.7509843707084656\n",
      "Iteration: 10167, Loss: 0.7509810328483582\n",
      "Iteration: 10168, Loss: 0.750977635383606\n",
      "Iteration: 10169, Loss: 0.7509742379188538\n",
      "Iteration: 10170, Loss: 0.7509709596633911\n",
      "Iteration: 10171, Loss: 0.7509676218032837\n",
      "Iteration: 10172, Loss: 0.7509642243385315\n",
      "Iteration: 10173, Loss: 0.7509609460830688\n",
      "Iteration: 10174, Loss: 0.7509576082229614\n",
      "Iteration: 10175, Loss: 0.750954270362854\n",
      "Iteration: 10176, Loss: 0.7509509921073914\n",
      "Iteration: 10177, Loss: 0.7509476542472839\n",
      "Iteration: 10178, Loss: 0.7509443163871765\n",
      "Iteration: 10179, Loss: 0.7509410977363586\n",
      "Iteration: 10180, Loss: 0.7509377002716064\n",
      "Iteration: 10181, Loss: 0.7509344816207886\n",
      "Iteration: 10182, Loss: 0.7509312033653259\n",
      "Iteration: 10183, Loss: 0.7509278655052185\n",
      "Iteration: 10184, Loss: 0.7509246468544006\n",
      "Iteration: 10185, Loss: 0.750921368598938\n",
      "Iteration: 10186, Loss: 0.7509180307388306\n",
      "Iteration: 10187, Loss: 0.7509148120880127\n",
      "Iteration: 10188, Loss: 0.7509115934371948\n",
      "Iteration: 10189, Loss: 0.750908374786377\n",
      "Iteration: 10190, Loss: 0.7509050369262695\n",
      "Iteration: 10191, Loss: 0.7509018778800964\n",
      "Iteration: 10192, Loss: 0.750898540019989\n",
      "Iteration: 10193, Loss: 0.7508954405784607\n",
      "Iteration: 10194, Loss: 0.750892162322998\n",
      "Iteration: 10195, Loss: 0.7508889436721802\n",
      "Iteration: 10196, Loss: 0.7508857250213623\n",
      "Iteration: 10197, Loss: 0.7508825063705444\n",
      "Iteration: 10198, Loss: 0.7508793473243713\n",
      "Iteration: 10199, Loss: 0.7508761286735535\n",
      "Iteration: 10200, Loss: 0.7508729100227356\n",
      "Iteration: 10201, Loss: 0.7508697509765625\n",
      "Iteration: 10202, Loss: 0.7508665323257446\n",
      "Iteration: 10203, Loss: 0.7508633732795715\n",
      "Iteration: 10204, Loss: 0.7508602142333984\n",
      "Iteration: 10205, Loss: 0.7508570551872253\n",
      "Iteration: 10206, Loss: 0.7508538365364075\n",
      "Iteration: 10207, Loss: 0.7508506774902344\n",
      "Iteration: 10208, Loss: 0.7508476376533508\n",
      "Iteration: 10209, Loss: 0.7508444786071777\n",
      "Iteration: 10210, Loss: 0.7508412599563599\n",
      "Iteration: 10211, Loss: 0.7508381605148315\n",
      "Iteration: 10212, Loss: 0.7508350610733032\n",
      "Iteration: 10213, Loss: 0.7508319616317749\n",
      "Iteration: 10214, Loss: 0.7508288025856018\n",
      "Iteration: 10215, Loss: 0.7508257031440735\n",
      "Iteration: 10216, Loss: 0.7508227229118347\n",
      "Iteration: 10217, Loss: 0.7508195042610168\n",
      "Iteration: 10218, Loss: 0.7508165240287781\n",
      "Iteration: 10219, Loss: 0.750813364982605\n",
      "Iteration: 10220, Loss: 0.7508102655410767\n",
      "Iteration: 10221, Loss: 0.7508072257041931\n",
      "Iteration: 10222, Loss: 0.7508041262626648\n",
      "Iteration: 10223, Loss: 0.7508012056350708\n",
      "Iteration: 10224, Loss: 0.7507980465888977\n",
      "Iteration: 10225, Loss: 0.7507950663566589\n",
      "Iteration: 10226, Loss: 0.7507919073104858\n",
      "Iteration: 10227, Loss: 0.7507889270782471\n",
      "Iteration: 10228, Loss: 0.7507858872413635\n",
      "Iteration: 10229, Loss: 0.7507827281951904\n",
      "Iteration: 10230, Loss: 0.7507798075675964\n",
      "Iteration: 10231, Loss: 0.7507767081260681\n",
      "Iteration: 10232, Loss: 0.7507737278938293\n",
      "Iteration: 10233, Loss: 0.7507706880569458\n",
      "Iteration: 10234, Loss: 0.7507675886154175\n",
      "Iteration: 10235, Loss: 0.7507647275924683\n",
      "Iteration: 10236, Loss: 0.7507616877555847\n",
      "Iteration: 10237, Loss: 0.750758707523346\n",
      "Iteration: 10238, Loss: 0.7507556676864624\n",
      "Iteration: 10239, Loss: 0.7507526874542236\n",
      "Iteration: 10240, Loss: 0.7507496476173401\n",
      "Iteration: 10241, Loss: 0.7507466077804565\n",
      "Iteration: 10242, Loss: 0.7507436871528625\n",
      "Iteration: 10243, Loss: 0.7507407069206238\n",
      "Iteration: 10244, Loss: 0.7507376074790955\n",
      "Iteration: 10245, Loss: 0.7507346868515015\n",
      "Iteration: 10246, Loss: 0.7507317066192627\n",
      "Iteration: 10247, Loss: 0.7507286667823792\n",
      "Iteration: 10248, Loss: 0.7507256865501404\n",
      "Iteration: 10249, Loss: 0.7507227659225464\n",
      "Iteration: 10250, Loss: 0.7507197856903076\n",
      "Iteration: 10251, Loss: 0.7507168054580688\n",
      "Iteration: 10252, Loss: 0.7507139444351196\n",
      "Iteration: 10253, Loss: 0.7507110238075256\n",
      "Iteration: 10254, Loss: 0.7507079839706421\n",
      "Iteration: 10255, Loss: 0.7507050037384033\n",
      "Iteration: 10256, Loss: 0.7507020831108093\n",
      "Iteration: 10257, Loss: 0.7506991028785706\n",
      "Iteration: 10258, Loss: 0.7506962418556213\n",
      "Iteration: 10259, Loss: 0.7506932020187378\n",
      "Iteration: 10260, Loss: 0.750690221786499\n",
      "Iteration: 10261, Loss: 0.7506874203681946\n",
      "Iteration: 10262, Loss: 0.7506844401359558\n",
      "Iteration: 10263, Loss: 0.750681459903717\n",
      "Iteration: 10264, Loss: 0.7506787180900574\n",
      "Iteration: 10265, Loss: 0.7506757378578186\n",
      "Iteration: 10266, Loss: 0.7506727576255798\n",
      "Iteration: 10267, Loss: 0.7506698369979858\n",
      "Iteration: 10268, Loss: 0.7506670355796814\n",
      "Iteration: 10269, Loss: 0.7506640553474426\n",
      "Iteration: 10270, Loss: 0.7506611943244934\n",
      "Iteration: 10271, Loss: 0.7506582140922546\n",
      "Iteration: 10272, Loss: 0.7506553530693054\n",
      "Iteration: 10273, Loss: 0.7506523728370667\n",
      "Iteration: 10274, Loss: 0.7506494522094727\n",
      "Iteration: 10275, Loss: 0.7506465911865234\n",
      "Iteration: 10276, Loss: 0.7506436109542847\n",
      "Iteration: 10277, Loss: 0.7506406903266907\n",
      "Iteration: 10278, Loss: 0.7506377696990967\n",
      "Iteration: 10279, Loss: 0.7506349086761475\n",
      "Iteration: 10280, Loss: 0.7506319880485535\n",
      "Iteration: 10281, Loss: 0.7506290674209595\n",
      "Iteration: 10282, Loss: 0.750626266002655\n",
      "Iteration: 10283, Loss: 0.750623345375061\n",
      "Iteration: 10284, Loss: 0.750620424747467\n",
      "Iteration: 10285, Loss: 0.750617504119873\n",
      "Iteration: 10286, Loss: 0.7506147027015686\n",
      "Iteration: 10287, Loss: 0.7506117820739746\n",
      "Iteration: 10288, Loss: 0.7506088614463806\n",
      "Iteration: 10289, Loss: 0.7506060004234314\n",
      "Iteration: 10290, Loss: 0.7506031394004822\n",
      "Iteration: 10291, Loss: 0.750600278377533\n",
      "Iteration: 10292, Loss: 0.750597357749939\n",
      "Iteration: 10293, Loss: 0.7505944967269897\n",
      "Iteration: 10294, Loss: 0.7505916357040405\n",
      "Iteration: 10295, Loss: 0.7505888342857361\n",
      "Iteration: 10296, Loss: 0.7505859136581421\n",
      "Iteration: 10297, Loss: 0.7505830526351929\n",
      "Iteration: 10298, Loss: 0.7505802512168884\n",
      "Iteration: 10299, Loss: 0.7505773901939392\n",
      "Iteration: 10300, Loss: 0.75057452917099\n",
      "Iteration: 10301, Loss: 0.7505716681480408\n",
      "Iteration: 10302, Loss: 0.7505688071250916\n",
      "Iteration: 10303, Loss: 0.7505659461021423\n",
      "Iteration: 10304, Loss: 0.7505630850791931\n",
      "Iteration: 10305, Loss: 0.7505601644515991\n",
      "Iteration: 10306, Loss: 0.7505573034286499\n",
      "Iteration: 10307, Loss: 0.7505545020103455\n",
      "Iteration: 10308, Loss: 0.7505516409873962\n",
      "Iteration: 10309, Loss: 0.750548779964447\n",
      "Iteration: 10310, Loss: 0.7505459785461426\n",
      "Iteration: 10311, Loss: 0.7505431175231934\n",
      "Iteration: 10312, Loss: 0.7505402565002441\n",
      "Iteration: 10313, Loss: 0.7505373954772949\n",
      "Iteration: 10314, Loss: 0.7505345344543457\n",
      "Iteration: 10315, Loss: 0.750531792640686\n",
      "Iteration: 10316, Loss: 0.7505289316177368\n",
      "Iteration: 10317, Loss: 0.7505261301994324\n",
      "Iteration: 10318, Loss: 0.7505232691764832\n",
      "Iteration: 10319, Loss: 0.7505204081535339\n",
      "Iteration: 10320, Loss: 0.7505176067352295\n",
      "Iteration: 10321, Loss: 0.750514805316925\n",
      "Iteration: 10322, Loss: 0.7505119442939758\n",
      "Iteration: 10323, Loss: 0.7505091428756714\n",
      "Iteration: 10324, Loss: 0.7505062818527222\n",
      "Iteration: 10325, Loss: 0.7505035996437073\n",
      "Iteration: 10326, Loss: 0.7505007386207581\n",
      "Iteration: 10327, Loss: 0.7504979372024536\n",
      "Iteration: 10328, Loss: 0.7504950761795044\n",
      "Iteration: 10329, Loss: 0.7504922747612\n",
      "Iteration: 10330, Loss: 0.7504895329475403\n",
      "Iteration: 10331, Loss: 0.7504867315292358\n",
      "Iteration: 10332, Loss: 0.7504839301109314\n",
      "Iteration: 10333, Loss: 0.750481128692627\n",
      "Iteration: 10334, Loss: 0.7504783272743225\n",
      "Iteration: 10335, Loss: 0.7504755258560181\n",
      "Iteration: 10336, Loss: 0.7504727244377136\n",
      "Iteration: 10337, Loss: 0.7504699230194092\n",
      "Iteration: 10338, Loss: 0.7504671216011047\n",
      "Iteration: 10339, Loss: 0.7504643797874451\n",
      "Iteration: 10340, Loss: 0.7504615783691406\n",
      "Iteration: 10341, Loss: 0.7504587769508362\n",
      "Iteration: 10342, Loss: 0.7504559755325317\n",
      "Iteration: 10343, Loss: 0.7504531741142273\n",
      "Iteration: 10344, Loss: 0.7504504323005676\n",
      "Iteration: 10345, Loss: 0.7504478693008423\n",
      "Iteration: 10346, Loss: 0.7504450678825378\n",
      "Iteration: 10347, Loss: 0.7504423260688782\n",
      "Iteration: 10348, Loss: 0.7504395246505737\n",
      "Iteration: 10349, Loss: 0.7504367232322693\n",
      "Iteration: 10350, Loss: 0.7504339814186096\n",
      "Iteration: 10351, Loss: 0.7504311800003052\n",
      "Iteration: 10352, Loss: 0.7504284977912903\n",
      "Iteration: 10353, Loss: 0.7504257559776306\n",
      "Iteration: 10354, Loss: 0.7504229545593262\n",
      "Iteration: 10355, Loss: 0.7504202127456665\n",
      "Iteration: 10356, Loss: 0.7504174709320068\n",
      "Iteration: 10357, Loss: 0.7504147887229919\n",
      "Iteration: 10358, Loss: 0.7504120469093323\n",
      "Iteration: 10359, Loss: 0.7504092454910278\n",
      "Iteration: 10360, Loss: 0.7504065036773682\n",
      "Iteration: 10361, Loss: 0.7504037618637085\n",
      "Iteration: 10362, Loss: 0.750400960445404\n",
      "Iteration: 10363, Loss: 0.7503982186317444\n",
      "Iteration: 10364, Loss: 0.7503954768180847\n",
      "Iteration: 10365, Loss: 0.750392735004425\n",
      "Iteration: 10366, Loss: 0.7503899931907654\n",
      "Iteration: 10367, Loss: 0.7503871917724609\n",
      "Iteration: 10368, Loss: 0.7503844499588013\n",
      "Iteration: 10369, Loss: 0.7503817081451416\n",
      "Iteration: 10370, Loss: 0.7503789663314819\n",
      "Iteration: 10371, Loss: 0.7503762245178223\n",
      "Iteration: 10372, Loss: 0.7503735423088074\n",
      "Iteration: 10373, Loss: 0.7503709197044373\n",
      "Iteration: 10374, Loss: 0.7503682971000671\n",
      "Iteration: 10375, Loss: 0.7503655552864075\n",
      "Iteration: 10376, Loss: 0.7503628134727478\n",
      "Iteration: 10377, Loss: 0.7503601312637329\n",
      "Iteration: 10378, Loss: 0.7503573894500732\n",
      "Iteration: 10379, Loss: 0.7503547668457031\n",
      "Iteration: 10380, Loss: 0.7503520846366882\n",
      "Iteration: 10381, Loss: 0.7503493428230286\n",
      "Iteration: 10382, Loss: 0.7503466010093689\n",
      "Iteration: 10383, Loss: 0.750343918800354\n",
      "Iteration: 10384, Loss: 0.7503411769866943\n",
      "Iteration: 10385, Loss: 0.7503384947776794\n",
      "Iteration: 10386, Loss: 0.7503357529640198\n",
      "Iteration: 10387, Loss: 0.7503331303596497\n",
      "Iteration: 10388, Loss: 0.75033038854599\n",
      "Iteration: 10389, Loss: 0.7503277063369751\n",
      "Iteration: 10390, Loss: 0.7503250241279602\n",
      "Iteration: 10391, Loss: 0.7503222823143005\n",
      "Iteration: 10392, Loss: 0.7503196597099304\n",
      "Iteration: 10393, Loss: 0.7503170967102051\n",
      "Iteration: 10394, Loss: 0.7503143548965454\n",
      "Iteration: 10395, Loss: 0.7503116726875305\n",
      "Iteration: 10396, Loss: 0.7503089904785156\n",
      "Iteration: 10397, Loss: 0.7503063678741455\n",
      "Iteration: 10398, Loss: 0.7503036856651306\n",
      "Iteration: 10399, Loss: 0.7503010034561157\n",
      "Iteration: 10400, Loss: 0.750298261642456\n",
      "Iteration: 10401, Loss: 0.7502955794334412\n",
      "Iteration: 10402, Loss: 0.7502928972244263\n",
      "Iteration: 10403, Loss: 0.7502902746200562\n",
      "Iteration: 10404, Loss: 0.7502875924110413\n",
      "Iteration: 10405, Loss: 0.7502849102020264\n",
      "Iteration: 10406, Loss: 0.7502822875976562\n",
      "Iteration: 10407, Loss: 0.7502796053886414\n",
      "Iteration: 10408, Loss: 0.7502769231796265\n",
      "Iteration: 10409, Loss: 0.7502742409706116\n",
      "Iteration: 10410, Loss: 0.7502715587615967\n",
      "Iteration: 10411, Loss: 0.7502689361572266\n",
      "Iteration: 10412, Loss: 0.7502663135528564\n",
      "Iteration: 10413, Loss: 0.7502636313438416\n",
      "Iteration: 10414, Loss: 0.7502609491348267\n",
      "Iteration: 10415, Loss: 0.7502583265304565\n",
      "Iteration: 10416, Loss: 0.7502556443214417\n",
      "Iteration: 10417, Loss: 0.7502529621124268\n",
      "Iteration: 10418, Loss: 0.7502503991127014\n",
      "Iteration: 10419, Loss: 0.7502477169036865\n",
      "Iteration: 10420, Loss: 0.7502450942993164\n",
      "Iteration: 10421, Loss: 0.7502424716949463\n",
      "Iteration: 10422, Loss: 0.7502398490905762\n",
      "Iteration: 10423, Loss: 0.7502371668815613\n",
      "Iteration: 10424, Loss: 0.7502345442771912\n",
      "Iteration: 10425, Loss: 0.7502319812774658\n",
      "Iteration: 10426, Loss: 0.7502292990684509\n",
      "Iteration: 10427, Loss: 0.7502266764640808\n",
      "Iteration: 10428, Loss: 0.7502241134643555\n",
      "Iteration: 10429, Loss: 0.7502214312553406\n",
      "Iteration: 10430, Loss: 0.7502188086509705\n",
      "Iteration: 10431, Loss: 0.7502162456512451\n",
      "Iteration: 10432, Loss: 0.750213623046875\n",
      "Iteration: 10433, Loss: 0.7502110004425049\n",
      "Iteration: 10434, Loss: 0.75020831823349\n",
      "Iteration: 10435, Loss: 0.7502056956291199\n",
      "Iteration: 10436, Loss: 0.7502031922340393\n",
      "Iteration: 10437, Loss: 0.7502005696296692\n",
      "Iteration: 10438, Loss: 0.7501979470252991\n",
      "Iteration: 10439, Loss: 0.750195324420929\n",
      "Iteration: 10440, Loss: 0.7501927018165588\n",
      "Iteration: 10441, Loss: 0.7501900792121887\n",
      "Iteration: 10442, Loss: 0.7501876354217529\n",
      "Iteration: 10443, Loss: 0.7501850724220276\n",
      "Iteration: 10444, Loss: 0.7501824498176575\n",
      "Iteration: 10445, Loss: 0.7501798272132874\n",
      "Iteration: 10446, Loss: 0.7501772046089172\n",
      "Iteration: 10447, Loss: 0.7501746416091919\n",
      "Iteration: 10448, Loss: 0.7501720786094666\n",
      "Iteration: 10449, Loss: 0.7501694560050964\n",
      "Iteration: 10450, Loss: 0.7501668930053711\n",
      "Iteration: 10451, Loss: 0.750164270401001\n",
      "Iteration: 10452, Loss: 0.7501617074012756\n",
      "Iteration: 10453, Loss: 0.7501590847969055\n",
      "Iteration: 10454, Loss: 0.7501565217971802\n",
      "Iteration: 10455, Loss: 0.7501538991928101\n",
      "Iteration: 10456, Loss: 0.7501513361930847\n",
      "Iteration: 10457, Loss: 0.7501487135887146\n",
      "Iteration: 10458, Loss: 0.7501461505889893\n",
      "Iteration: 10459, Loss: 0.7501437067985535\n",
      "Iteration: 10460, Loss: 0.7501411437988281\n",
      "Iteration: 10461, Loss: 0.7501386404037476\n",
      "Iteration: 10462, Loss: 0.7501360774040222\n",
      "Iteration: 10463, Loss: 0.7501335144042969\n",
      "Iteration: 10464, Loss: 0.7501309514045715\n",
      "Iteration: 10465, Loss: 0.750128448009491\n",
      "Iteration: 10466, Loss: 0.7501258850097656\n",
      "Iteration: 10467, Loss: 0.7501233220100403\n",
      "Iteration: 10468, Loss: 0.7501207590103149\n",
      "Iteration: 10469, Loss: 0.7501181960105896\n",
      "Iteration: 10470, Loss: 0.7501156330108643\n",
      "Iteration: 10471, Loss: 0.7501131296157837\n",
      "Iteration: 10472, Loss: 0.7501105666160583\n",
      "Iteration: 10473, Loss: 0.750108003616333\n",
      "Iteration: 10474, Loss: 0.7501054406166077\n",
      "Iteration: 10475, Loss: 0.7501028776168823\n",
      "Iteration: 10476, Loss: 0.750100314617157\n",
      "Iteration: 10477, Loss: 0.7500978112220764\n",
      "Iteration: 10478, Loss: 0.7500954866409302\n",
      "Iteration: 10479, Loss: 0.7500929236412048\n",
      "Iteration: 10480, Loss: 0.7500903606414795\n",
      "Iteration: 10481, Loss: 0.7500878572463989\n",
      "Iteration: 10482, Loss: 0.7500853538513184\n",
      "Iteration: 10483, Loss: 0.7500828504562378\n",
      "Iteration: 10484, Loss: 0.7500802874565125\n",
      "Iteration: 10485, Loss: 0.7500777840614319\n",
      "Iteration: 10486, Loss: 0.7500752806663513\n",
      "Iteration: 10487, Loss: 0.7500727772712708\n",
      "Iteration: 10488, Loss: 0.7500702142715454\n",
      "Iteration: 10489, Loss: 0.7500677108764648\n",
      "Iteration: 10490, Loss: 0.7500652074813843\n",
      "Iteration: 10491, Loss: 0.7500628232955933\n",
      "Iteration: 10492, Loss: 0.7500603795051575\n",
      "Iteration: 10493, Loss: 0.7500578761100769\n",
      "Iteration: 10494, Loss: 0.7500553131103516\n",
      "Iteration: 10495, Loss: 0.7500529289245605\n",
      "Iteration: 10496, Loss: 0.75005042552948\n",
      "Iteration: 10497, Loss: 0.7500479221343994\n",
      "Iteration: 10498, Loss: 0.7500454187393188\n",
      "Iteration: 10499, Loss: 0.7500429153442383\n",
      "Iteration: 10500, Loss: 0.7500404119491577\n",
      "Iteration: 10501, Loss: 0.7500379085540771\n",
      "Iteration: 10502, Loss: 0.7500354051589966\n",
      "Iteration: 10503, Loss: 0.750032901763916\n",
      "Iteration: 10504, Loss: 0.7500304579734802\n",
      "Iteration: 10505, Loss: 0.7500279545783997\n",
      "Iteration: 10506, Loss: 0.7500255107879639\n",
      "Iteration: 10507, Loss: 0.7500230073928833\n",
      "Iteration: 10508, Loss: 0.7500205039978027\n",
      "Iteration: 10509, Loss: 0.7500180006027222\n",
      "Iteration: 10510, Loss: 0.7500155568122864\n",
      "Iteration: 10511, Loss: 0.7500130534172058\n",
      "Iteration: 10512, Loss: 0.7500105500221252\n",
      "Iteration: 10513, Loss: 0.7500081658363342\n",
      "Iteration: 10514, Loss: 0.7500056624412537\n",
      "Iteration: 10515, Loss: 0.7500031590461731\n",
      "Iteration: 10516, Loss: 0.7500007152557373\n",
      "Iteration: 10517, Loss: 0.7499982118606567\n",
      "Iteration: 10518, Loss: 0.749995768070221\n",
      "Iteration: 10519, Loss: 0.7499932646751404\n",
      "Iteration: 10520, Loss: 0.7499909400939941\n",
      "Iteration: 10521, Loss: 0.7499884963035583\n",
      "Iteration: 10522, Loss: 0.7499859929084778\n",
      "Iteration: 10523, Loss: 0.749983549118042\n",
      "Iteration: 10524, Loss: 0.7499810457229614\n",
      "Iteration: 10525, Loss: 0.7499786019325256\n",
      "Iteration: 10526, Loss: 0.7499761581420898\n",
      "Iteration: 10527, Loss: 0.7499739527702332\n",
      "Iteration: 10528, Loss: 0.7499714493751526\n",
      "Iteration: 10529, Loss: 0.7499690055847168\n",
      "Iteration: 10530, Loss: 0.749966561794281\n",
      "Iteration: 10531, Loss: 0.7499641180038452\n",
      "Iteration: 10532, Loss: 0.7499616742134094\n",
      "Iteration: 10533, Loss: 0.7499592304229736\n",
      "Iteration: 10534, Loss: 0.7499567866325378\n",
      "Iteration: 10535, Loss: 0.749954342842102\n",
      "Iteration: 10536, Loss: 0.7499518990516663\n",
      "Iteration: 10537, Loss: 0.7499495148658752\n",
      "Iteration: 10538, Loss: 0.7499470710754395\n",
      "Iteration: 10539, Loss: 0.7499446272850037\n",
      "Iteration: 10540, Loss: 0.7499421834945679\n",
      "Iteration: 10541, Loss: 0.7499397993087769\n",
      "Iteration: 10542, Loss: 0.7499373555183411\n",
      "Iteration: 10543, Loss: 0.74993497133255\n",
      "Iteration: 10544, Loss: 0.7499325275421143\n",
      "Iteration: 10545, Loss: 0.7499301433563232\n",
      "Iteration: 10546, Loss: 0.7499276995658875\n",
      "Iteration: 10547, Loss: 0.7499253153800964\n",
      "Iteration: 10548, Loss: 0.7499228715896606\n",
      "Iteration: 10549, Loss: 0.7499204874038696\n",
      "Iteration: 10550, Loss: 0.7499180436134338\n",
      "Iteration: 10551, Loss: 0.7499157190322876\n",
      "Iteration: 10552, Loss: 0.7499133348464966\n",
      "Iteration: 10553, Loss: 0.7499108910560608\n",
      "Iteration: 10554, Loss: 0.7499085068702698\n",
      "Iteration: 10555, Loss: 0.7499063014984131\n",
      "Iteration: 10556, Loss: 0.7499039173126221\n",
      "Iteration: 10557, Loss: 0.7499016523361206\n",
      "Iteration: 10558, Loss: 0.7498992085456848\n",
      "Iteration: 10559, Loss: 0.7498968243598938\n",
      "Iteration: 10560, Loss: 0.7498944401741028\n",
      "Iteration: 10561, Loss: 0.7498920559883118\n",
      "Iteration: 10562, Loss: 0.7498896718025208\n",
      "Iteration: 10563, Loss: 0.749887228012085\n",
      "Iteration: 10564, Loss: 0.7498849034309387\n",
      "Iteration: 10565, Loss: 0.7498825192451477\n",
      "Iteration: 10566, Loss: 0.7498801350593567\n",
      "Iteration: 10567, Loss: 0.7498777508735657\n",
      "Iteration: 10568, Loss: 0.7498753666877747\n",
      "Iteration: 10569, Loss: 0.7498729825019836\n",
      "Iteration: 10570, Loss: 0.7498710751533508\n",
      "Iteration: 10571, Loss: 0.7498686909675598\n",
      "Iteration: 10572, Loss: 0.7498663067817688\n",
      "Iteration: 10573, Loss: 0.7498639822006226\n",
      "Iteration: 10574, Loss: 0.7498615980148315\n",
      "Iteration: 10575, Loss: 0.7498592138290405\n",
      "Iteration: 10576, Loss: 0.7498568296432495\n",
      "Iteration: 10577, Loss: 0.7498545050621033\n",
      "Iteration: 10578, Loss: 0.7498521208763123\n",
      "Iteration: 10579, Loss: 0.7498497366905212\n",
      "Iteration: 10580, Loss: 0.749847412109375\n",
      "Iteration: 10581, Loss: 0.749845027923584\n",
      "Iteration: 10582, Loss: 0.749842643737793\n",
      "Iteration: 10583, Loss: 0.7498403191566467\n",
      "Iteration: 10584, Loss: 0.7498379349708557\n",
      "Iteration: 10585, Loss: 0.7498356103897095\n",
      "Iteration: 10586, Loss: 0.7498332858085632\n",
      "Iteration: 10587, Loss: 0.7498309016227722\n",
      "Iteration: 10588, Loss: 0.7498286366462708\n",
      "Iteration: 10589, Loss: 0.7498263120651245\n",
      "Iteration: 10590, Loss: 0.7498239874839783\n",
      "Iteration: 10591, Loss: 0.7498216032981873\n",
      "Iteration: 10592, Loss: 0.7498193383216858\n",
      "Iteration: 10593, Loss: 0.7498170137405396\n",
      "Iteration: 10594, Loss: 0.7498146891593933\n",
      "Iteration: 10595, Loss: 0.7498123645782471\n",
      "Iteration: 10596, Loss: 0.7498100399971008\n",
      "Iteration: 10597, Loss: 0.7498076558113098\n",
      "Iteration: 10598, Loss: 0.7498053312301636\n",
      "Iteration: 10599, Loss: 0.7498030066490173\n",
      "Iteration: 10600, Loss: 0.7498006820678711\n",
      "Iteration: 10601, Loss: 0.7497983574867249\n",
      "Iteration: 10602, Loss: 0.7497960329055786\n",
      "Iteration: 10603, Loss: 0.7497937679290771\n",
      "Iteration: 10604, Loss: 0.7497914433479309\n",
      "Iteration: 10605, Loss: 0.7497891187667847\n",
      "Iteration: 10606, Loss: 0.7497868537902832\n",
      "Iteration: 10607, Loss: 0.749784529209137\n",
      "Iteration: 10608, Loss: 0.7497822046279907\n",
      "Iteration: 10609, Loss: 0.7497798800468445\n",
      "Iteration: 10610, Loss: 0.7497775554656982\n",
      "Iteration: 10611, Loss: 0.7497752904891968\n",
      "Iteration: 10612, Loss: 0.7497730255126953\n",
      "Iteration: 10613, Loss: 0.7497707009315491\n",
      "Iteration: 10614, Loss: 0.7497683763504028\n",
      "Iteration: 10615, Loss: 0.7497662305831909\n",
      "Iteration: 10616, Loss: 0.7497639656066895\n",
      "Iteration: 10617, Loss: 0.7497616410255432\n",
      "Iteration: 10618, Loss: 0.749759316444397\n",
      "Iteration: 10619, Loss: 0.7497570514678955\n",
      "Iteration: 10620, Loss: 0.7497547268867493\n",
      "Iteration: 10621, Loss: 0.7497525811195374\n",
      "Iteration: 10622, Loss: 0.7497502565383911\n",
      "Iteration: 10623, Loss: 0.7497479915618896\n",
      "Iteration: 10624, Loss: 0.7497456669807434\n",
      "Iteration: 10625, Loss: 0.7497434020042419\n",
      "Iteration: 10626, Loss: 0.7497411370277405\n",
      "Iteration: 10627, Loss: 0.749738872051239\n",
      "Iteration: 10628, Loss: 0.7497366070747375\n",
      "Iteration: 10629, Loss: 0.7497343420982361\n",
      "Iteration: 10630, Loss: 0.7497320771217346\n",
      "Iteration: 10631, Loss: 0.7497297525405884\n",
      "Iteration: 10632, Loss: 0.7497274875640869\n",
      "Iteration: 10633, Loss: 0.7497252225875854\n",
      "Iteration: 10634, Loss: 0.7497231364250183\n",
      "Iteration: 10635, Loss: 0.7497208714485168\n",
      "Iteration: 10636, Loss: 0.7497186064720154\n",
      "Iteration: 10637, Loss: 0.7497162818908691\n",
      "Iteration: 10638, Loss: 0.7497140169143677\n",
      "Iteration: 10639, Loss: 0.7497117519378662\n",
      "Iteration: 10640, Loss: 0.7497094869613647\n",
      "Iteration: 10641, Loss: 0.7497072815895081\n",
      "Iteration: 10642, Loss: 0.7497050166130066\n",
      "Iteration: 10643, Loss: 0.7497028112411499\n",
      "Iteration: 10644, Loss: 0.7497005462646484\n",
      "Iteration: 10645, Loss: 0.749698281288147\n",
      "Iteration: 10646, Loss: 0.7496960759162903\n",
      "Iteration: 10647, Loss: 0.7496938109397888\n",
      "Iteration: 10648, Loss: 0.7496915459632874\n",
      "Iteration: 10649, Loss: 0.7496893405914307\n",
      "Iteration: 10650, Loss: 0.7496870756149292\n",
      "Iteration: 10651, Loss: 0.7496848702430725\n",
      "Iteration: 10652, Loss: 0.749682605266571\n",
      "Iteration: 10653, Loss: 0.7496805191040039\n",
      "Iteration: 10654, Loss: 0.7496782541275024\n",
      "Iteration: 10655, Loss: 0.749675989151001\n",
      "Iteration: 10656, Loss: 0.7496737837791443\n",
      "Iteration: 10657, Loss: 0.7496715188026428\n",
      "Iteration: 10658, Loss: 0.7496693134307861\n",
      "Iteration: 10659, Loss: 0.7496670484542847\n",
      "Iteration: 10660, Loss: 0.7496649026870728\n",
      "Iteration: 10661, Loss: 0.7496626377105713\n",
      "Iteration: 10662, Loss: 0.7496604323387146\n",
      "Iteration: 10663, Loss: 0.7496582269668579\n",
      "Iteration: 10664, Loss: 0.7496560215950012\n",
      "Iteration: 10665, Loss: 0.7496538758277893\n",
      "Iteration: 10666, Loss: 0.7496516108512878\n",
      "Iteration: 10667, Loss: 0.7496494054794312\n",
      "Iteration: 10668, Loss: 0.7496472001075745\n",
      "Iteration: 10669, Loss: 0.7496449947357178\n",
      "Iteration: 10670, Loss: 0.7496427893638611\n",
      "Iteration: 10671, Loss: 0.7496405839920044\n",
      "Iteration: 10672, Loss: 0.7496383786201477\n",
      "Iteration: 10673, Loss: 0.7496362328529358\n",
      "Iteration: 10674, Loss: 0.7496340274810791\n",
      "Iteration: 10675, Loss: 0.7496318221092224\n",
      "Iteration: 10676, Loss: 0.7496296167373657\n",
      "Iteration: 10677, Loss: 0.749627411365509\n",
      "Iteration: 10678, Loss: 0.7496252655982971\n",
      "Iteration: 10679, Loss: 0.7496231198310852\n",
      "Iteration: 10680, Loss: 0.7496209144592285\n",
      "Iteration: 10681, Loss: 0.7496187090873718\n",
      "Iteration: 10682, Loss: 0.7496165037155151\n",
      "Iteration: 10683, Loss: 0.7496142983436584\n",
      "Iteration: 10684, Loss: 0.7496120929718018\n",
      "Iteration: 10685, Loss: 0.749610185623169\n",
      "Iteration: 10686, Loss: 0.7496080994606018\n",
      "Iteration: 10687, Loss: 0.7496058940887451\n",
      "Iteration: 10688, Loss: 0.7496036887168884\n",
      "Iteration: 10689, Loss: 0.7496015429496765\n",
      "Iteration: 10690, Loss: 0.7495993375778198\n",
      "Iteration: 10691, Loss: 0.7495973110198975\n",
      "Iteration: 10692, Loss: 0.7495951056480408\n",
      "Iteration: 10693, Loss: 0.7495930194854736\n",
      "Iteration: 10694, Loss: 0.7495908141136169\n",
      "Iteration: 10695, Loss: 0.749588668346405\n",
      "Iteration: 10696, Loss: 0.7495864629745483\n",
      "Iteration: 10697, Loss: 0.7495843172073364\n",
      "Iteration: 10698, Loss: 0.7495821714401245\n",
      "Iteration: 10699, Loss: 0.7495800256729126\n",
      "Iteration: 10700, Loss: 0.7495778799057007\n",
      "Iteration: 10701, Loss: 0.7495757341384888\n",
      "Iteration: 10702, Loss: 0.7495735883712769\n",
      "Iteration: 10703, Loss: 0.7495714426040649\n",
      "Iteration: 10704, Loss: 0.749569296836853\n",
      "Iteration: 10705, Loss: 0.7495671510696411\n",
      "Iteration: 10706, Loss: 0.749565064907074\n",
      "Iteration: 10707, Loss: 0.7495629191398621\n",
      "Iteration: 10708, Loss: 0.7495607733726501\n",
      "Iteration: 10709, Loss: 0.7495586276054382\n",
      "Iteration: 10710, Loss: 0.7495564818382263\n",
      "Iteration: 10711, Loss: 0.7495543360710144\n",
      "Iteration: 10712, Loss: 0.7495521903038025\n",
      "Iteration: 10713, Loss: 0.7495501637458801\n",
      "Iteration: 10714, Loss: 0.7495480179786682\n",
      "Iteration: 10715, Loss: 0.7495458722114563\n",
      "Iteration: 10716, Loss: 0.7495437264442444\n",
      "Iteration: 10717, Loss: 0.749541699886322\n",
      "Iteration: 10718, Loss: 0.7495395541191101\n",
      "Iteration: 10719, Loss: 0.7495375275611877\n",
      "Iteration: 10720, Loss: 0.7495353817939758\n",
      "Iteration: 10721, Loss: 0.7495332360267639\n",
      "Iteration: 10722, Loss: 0.749531090259552\n",
      "Iteration: 10723, Loss: 0.7495290637016296\n",
      "Iteration: 10724, Loss: 0.7495269179344177\n",
      "Iteration: 10725, Loss: 0.7495247721672058\n",
      "Iteration: 10726, Loss: 0.7495227456092834\n",
      "Iteration: 10727, Loss: 0.7495205998420715\n",
      "Iteration: 10728, Loss: 0.7495185136795044\n",
      "Iteration: 10729, Loss: 0.7495163679122925\n",
      "Iteration: 10730, Loss: 0.7495142817497253\n",
      "Iteration: 10731, Loss: 0.7495123744010925\n",
      "Iteration: 10732, Loss: 0.7495103478431702\n",
      "Iteration: 10733, Loss: 0.7495082020759583\n",
      "Iteration: 10734, Loss: 0.7495061755180359\n",
      "Iteration: 10735, Loss: 0.7495040893554688\n",
      "Iteration: 10736, Loss: 0.7495019435882568\n",
      "Iteration: 10737, Loss: 0.7494998574256897\n",
      "Iteration: 10738, Loss: 0.7494978904724121\n",
      "Iteration: 10739, Loss: 0.749495804309845\n",
      "Iteration: 10740, Loss: 0.7494938373565674\n",
      "Iteration: 10741, Loss: 0.7494916915893555\n",
      "Iteration: 10742, Loss: 0.7494896054267883\n",
      "Iteration: 10743, Loss: 0.7494875192642212\n",
      "Iteration: 10744, Loss: 0.7494854927062988\n",
      "Iteration: 10745, Loss: 0.7494834065437317\n",
      "Iteration: 10746, Loss: 0.7494814395904541\n",
      "Iteration: 10747, Loss: 0.749479353427887\n",
      "Iteration: 10748, Loss: 0.7494772672653198\n",
      "Iteration: 10749, Loss: 0.7494752407073975\n",
      "Iteration: 10750, Loss: 0.7494734525680542\n",
      "Iteration: 10751, Loss: 0.7494713664054871\n",
      "Iteration: 10752, Loss: 0.7494692802429199\n",
      "Iteration: 10753, Loss: 0.7494671940803528\n",
      "Iteration: 10754, Loss: 0.7494651675224304\n",
      "Iteration: 10755, Loss: 0.7494631409645081\n",
      "Iteration: 10756, Loss: 0.7494611740112305\n",
      "Iteration: 10757, Loss: 0.7494590878486633\n",
      "Iteration: 10758, Loss: 0.749457061290741\n",
      "Iteration: 10759, Loss: 0.7494549751281738\n",
      "Iteration: 10760, Loss: 0.749453067779541\n",
      "Iteration: 10761, Loss: 0.7494510412216187\n",
      "Iteration: 10762, Loss: 0.7494490146636963\n",
      "Iteration: 10763, Loss: 0.7494469285011292\n",
      "Iteration: 10764, Loss: 0.7494449019432068\n",
      "Iteration: 10765, Loss: 0.749442994594574\n",
      "Iteration: 10766, Loss: 0.7494409680366516\n",
      "Iteration: 10767, Loss: 0.7494388818740845\n",
      "Iteration: 10768, Loss: 0.7494368553161621\n",
      "Iteration: 10769, Loss: 0.7494348287582397\n",
      "Iteration: 10770, Loss: 0.7494328022003174\n",
      "Iteration: 10771, Loss: 0.749430775642395\n",
      "Iteration: 10772, Loss: 0.7494286894798279\n",
      "Iteration: 10773, Loss: 0.7494267225265503\n",
      "Iteration: 10774, Loss: 0.7494248151779175\n",
      "Iteration: 10775, Loss: 0.7494227886199951\n",
      "Iteration: 10776, Loss: 0.7494207620620728\n",
      "Iteration: 10777, Loss: 0.7494187355041504\n",
      "Iteration: 10778, Loss: 0.749416708946228\n",
      "Iteration: 10779, Loss: 0.7494147419929504\n",
      "Iteration: 10780, Loss: 0.7494127154350281\n",
      "Iteration: 10781, Loss: 0.7494106888771057\n",
      "Iteration: 10782, Loss: 0.7494086623191833\n",
      "Iteration: 10783, Loss: 0.7494069337844849\n",
      "Iteration: 10784, Loss: 0.7494049072265625\n",
      "Iteration: 10785, Loss: 0.7494028806686401\n",
      "Iteration: 10786, Loss: 0.7494010329246521\n",
      "Iteration: 10787, Loss: 0.7493990063667297\n",
      "Iteration: 10788, Loss: 0.7493970990180969\n",
      "Iteration: 10789, Loss: 0.7493950724601746\n",
      "Iteration: 10790, Loss: 0.7493931651115417\n",
      "Iteration: 10791, Loss: 0.7493911981582642\n",
      "Iteration: 10792, Loss: 0.7493891716003418\n",
      "Iteration: 10793, Loss: 0.749387264251709\n",
      "Iteration: 10794, Loss: 0.7493852376937866\n",
      "Iteration: 10795, Loss: 0.7493833303451538\n",
      "Iteration: 10796, Loss: 0.7493813037872314\n",
      "Iteration: 10797, Loss: 0.7493793964385986\n",
      "Iteration: 10798, Loss: 0.749377429485321\n",
      "Iteration: 10799, Loss: 0.7493754625320435\n",
      "Iteration: 10800, Loss: 0.7493735551834106\n",
      "Iteration: 10801, Loss: 0.7493715286254883\n",
      "Iteration: 10802, Loss: 0.7493695616722107\n",
      "Iteration: 10803, Loss: 0.7493675947189331\n",
      "Iteration: 10804, Loss: 0.7493656277656555\n",
      "Iteration: 10805, Loss: 0.7493636608123779\n",
      "Iteration: 10806, Loss: 0.7493616938591003\n",
      "Iteration: 10807, Loss: 0.7493597865104675\n",
      "Iteration: 10808, Loss: 0.7493578195571899\n",
      "Iteration: 10809, Loss: 0.7493559122085571\n",
      "Iteration: 10810, Loss: 0.7493539452552795\n",
      "Iteration: 10811, Loss: 0.7493520379066467\n",
      "Iteration: 10812, Loss: 0.7493501305580139\n",
      "Iteration: 10813, Loss: 0.7493482828140259\n",
      "Iteration: 10814, Loss: 0.7493463158607483\n",
      "Iteration: 10815, Loss: 0.7493444681167603\n",
      "Iteration: 10816, Loss: 0.7493425607681274\n",
      "Iteration: 10817, Loss: 0.7493406534194946\n",
      "Iteration: 10818, Loss: 0.749338686466217\n",
      "Iteration: 10819, Loss: 0.749336838722229\n",
      "Iteration: 10820, Loss: 0.7493349313735962\n",
      "Iteration: 10821, Loss: 0.7493329644203186\n",
      "Iteration: 10822, Loss: 0.749330997467041\n",
      "Iteration: 10823, Loss: 0.7493290901184082\n",
      "Iteration: 10824, Loss: 0.7493271231651306\n",
      "Iteration: 10825, Loss: 0.749325156211853\n",
      "Iteration: 10826, Loss: 0.749323308467865\n",
      "Iteration: 10827, Loss: 0.7493214011192322\n",
      "Iteration: 10828, Loss: 0.7493195533752441\n",
      "Iteration: 10829, Loss: 0.7493175864219666\n",
      "Iteration: 10830, Loss: 0.7493157386779785\n",
      "Iteration: 10831, Loss: 0.7493137717247009\n",
      "Iteration: 10832, Loss: 0.7493118643760681\n",
      "Iteration: 10833, Loss: 0.7493098974227905\n",
      "Iteration: 10834, Loss: 0.7493081092834473\n",
      "Iteration: 10835, Loss: 0.7493061423301697\n",
      "Iteration: 10836, Loss: 0.7493042349815369\n",
      "Iteration: 10837, Loss: 0.7493024468421936\n",
      "Iteration: 10838, Loss: 0.7493006587028503\n",
      "Iteration: 10839, Loss: 0.7492986917495728\n",
      "Iteration: 10840, Loss: 0.7492967844009399\n",
      "Iteration: 10841, Loss: 0.7492948770523071\n",
      "Iteration: 10842, Loss: 0.7492929697036743\n",
      "Iteration: 10843, Loss: 0.749291181564331\n",
      "Iteration: 10844, Loss: 0.7492892146110535\n",
      "Iteration: 10845, Loss: 0.7492873072624207\n",
      "Iteration: 10846, Loss: 0.7492854595184326\n",
      "Iteration: 10847, Loss: 0.7492836117744446\n",
      "Iteration: 10848, Loss: 0.7492817640304565\n",
      "Iteration: 10849, Loss: 0.7492798566818237\n",
      "Iteration: 10850, Loss: 0.7492779493331909\n",
      "Iteration: 10851, Loss: 0.7492760419845581\n",
      "Iteration: 10852, Loss: 0.7492742538452148\n",
      "Iteration: 10853, Loss: 0.749272346496582\n",
      "Iteration: 10854, Loss: 0.7492705583572388\n",
      "Iteration: 10855, Loss: 0.749268651008606\n",
      "Iteration: 10856, Loss: 0.7492668032646179\n",
      "Iteration: 10857, Loss: 0.7492648959159851\n",
      "Iteration: 10858, Loss: 0.7492631077766418\n",
      "Iteration: 10859, Loss: 0.7492612600326538\n",
      "Iteration: 10860, Loss: 0.749259352684021\n",
      "Iteration: 10861, Loss: 0.7492575645446777\n",
      "Iteration: 10862, Loss: 0.7492556571960449\n",
      "Iteration: 10863, Loss: 0.7492538690567017\n",
      "Iteration: 10864, Loss: 0.7492519617080688\n",
      "Iteration: 10865, Loss: 0.7492501735687256\n",
      "Iteration: 10866, Loss: 0.7492482662200928\n",
      "Iteration: 10867, Loss: 0.7492464780807495\n",
      "Iteration: 10868, Loss: 0.7492445707321167\n",
      "Iteration: 10869, Loss: 0.7492427229881287\n",
      "Iteration: 10870, Loss: 0.7492408752441406\n",
      "Iteration: 10871, Loss: 0.7492390275001526\n",
      "Iteration: 10872, Loss: 0.7492372393608093\n",
      "Iteration: 10873, Loss: 0.7492353320121765\n",
      "Iteration: 10874, Loss: 0.7492334842681885\n",
      "Iteration: 10875, Loss: 0.7492316961288452\n",
      "Iteration: 10876, Loss: 0.7492298483848572\n",
      "Iteration: 10877, Loss: 0.7492280006408691\n",
      "Iteration: 10878, Loss: 0.7492261528968811\n",
      "Iteration: 10879, Loss: 0.7492243647575378\n",
      "Iteration: 10880, Loss: 0.7492225170135498\n",
      "Iteration: 10881, Loss: 0.7492206692695618\n",
      "Iteration: 10882, Loss: 0.7492189407348633\n",
      "Iteration: 10883, Loss: 0.7492170929908752\n",
      "Iteration: 10884, Loss: 0.7492153644561768\n",
      "Iteration: 10885, Loss: 0.7492135167121887\n",
      "Iteration: 10886, Loss: 0.7492117285728455\n",
      "Iteration: 10887, Loss: 0.7492098808288574\n",
      "Iteration: 10888, Loss: 0.7492080926895142\n",
      "Iteration: 10889, Loss: 0.7492063045501709\n",
      "Iteration: 10890, Loss: 0.7492045164108276\n",
      "Iteration: 10891, Loss: 0.7492026686668396\n",
      "Iteration: 10892, Loss: 0.7492008805274963\n",
      "Iteration: 10893, Loss: 0.7491991519927979\n",
      "Iteration: 10894, Loss: 0.7491973638534546\n",
      "Iteration: 10895, Loss: 0.7491955757141113\n",
      "Iteration: 10896, Loss: 0.7491937279701233\n",
      "Iteration: 10897, Loss: 0.7491919994354248\n",
      "Iteration: 10898, Loss: 0.7491902709007263\n",
      "Iteration: 10899, Loss: 0.7491884827613831\n",
      "Iteration: 10900, Loss: 0.749186635017395\n",
      "Iteration: 10901, Loss: 0.7491848468780518\n",
      "Iteration: 10902, Loss: 0.7491829991340637\n",
      "Iteration: 10903, Loss: 0.7491812109947205\n",
      "Iteration: 10904, Loss: 0.7491794228553772\n",
      "Iteration: 10905, Loss: 0.7491776347160339\n",
      "Iteration: 10906, Loss: 0.7491758465766907\n",
      "Iteration: 10907, Loss: 0.7491740584373474\n",
      "Iteration: 10908, Loss: 0.7491722106933594\n",
      "Iteration: 10909, Loss: 0.7491704225540161\n",
      "Iteration: 10910, Loss: 0.7491686344146729\n",
      "Iteration: 10911, Loss: 0.7491668462753296\n",
      "Iteration: 10912, Loss: 0.7491651773452759\n",
      "Iteration: 10913, Loss: 0.7491633892059326\n",
      "Iteration: 10914, Loss: 0.7491616606712341\n",
      "Iteration: 10915, Loss: 0.7491598725318909\n",
      "Iteration: 10916, Loss: 0.7491580843925476\n",
      "Iteration: 10917, Loss: 0.7491562962532043\n",
      "Iteration: 10918, Loss: 0.7491546869277954\n",
      "Iteration: 10919, Loss: 0.7491528987884521\n",
      "Iteration: 10920, Loss: 0.7491511106491089\n",
      "Iteration: 10921, Loss: 0.7491493225097656\n",
      "Iteration: 10922, Loss: 0.7491475343704224\n",
      "Iteration: 10923, Loss: 0.7491457462310791\n",
      "Iteration: 10924, Loss: 0.7491440773010254\n",
      "Iteration: 10925, Loss: 0.7491423487663269\n",
      "Iteration: 10926, Loss: 0.7491405606269836\n",
      "Iteration: 10927, Loss: 0.7491388320922852\n",
      "Iteration: 10928, Loss: 0.7491370439529419\n",
      "Iteration: 10929, Loss: 0.7491352558135986\n",
      "Iteration: 10930, Loss: 0.7491335272789001\n",
      "Iteration: 10931, Loss: 0.7491318583488464\n",
      "Iteration: 10932, Loss: 0.749130129814148\n",
      "Iteration: 10933, Loss: 0.7491283416748047\n",
      "Iteration: 10934, Loss: 0.7491264939308167\n",
      "Iteration: 10935, Loss: 0.7491247057914734\n",
      "Iteration: 10936, Loss: 0.7491229772567749\n",
      "Iteration: 10937, Loss: 0.7491211891174316\n",
      "Iteration: 10938, Loss: 0.7491195797920227\n",
      "Iteration: 10939, Loss: 0.7491177916526794\n",
      "Iteration: 10940, Loss: 0.749116063117981\n",
      "Iteration: 10941, Loss: 0.7491143345832825\n",
      "Iteration: 10942, Loss: 0.749112606048584\n",
      "Iteration: 10943, Loss: 0.7491108179092407\n",
      "Iteration: 10944, Loss: 0.7491090893745422\n",
      "Iteration: 10945, Loss: 0.7491073608398438\n",
      "Iteration: 10946, Loss: 0.7491056323051453\n",
      "Iteration: 10947, Loss: 0.7491039037704468\n",
      "Iteration: 10948, Loss: 0.7491021752357483\n",
      "Iteration: 10949, Loss: 0.7491004467010498\n",
      "Iteration: 10950, Loss: 0.7490987181663513\n",
      "Iteration: 10951, Loss: 0.7490969896316528\n",
      "Iteration: 10952, Loss: 0.7490952610969543\n",
      "Iteration: 10953, Loss: 0.7491005063056946\n",
      "Iteration: 10954, Loss: 0.7490986585617065\n",
      "Iteration: 10955, Loss: 0.7490967512130737\n",
      "Iteration: 10956, Loss: 0.7490946650505066\n",
      "Iteration: 10957, Loss: 0.7490926384925842\n",
      "Iteration: 10958, Loss: 0.7490904927253723\n",
      "Iteration: 10959, Loss: 0.7490885257720947\n",
      "Iteration: 10960, Loss: 0.7490863800048828\n",
      "Iteration: 10961, Loss: 0.7490842342376709\n",
      "Iteration: 10962, Loss: 0.7490822672843933\n",
      "Iteration: 10963, Loss: 0.7490803003311157\n",
      "Iteration: 10964, Loss: 0.7490781545639038\n",
      "Iteration: 10965, Loss: 0.7490761876106262\n",
      "Iteration: 10966, Loss: 0.7490742802619934\n",
      "Iteration: 10967, Loss: 0.7490723729133606\n",
      "Iteration: 10968, Loss: 0.7490705251693726\n",
      "Iteration: 10969, Loss: 0.749068558216095\n",
      "Iteration: 10970, Loss: 0.7490667104721069\n",
      "Iteration: 10971, Loss: 0.7490650415420532\n",
      "Iteration: 10972, Loss: 0.7490631341934204\n",
      "Iteration: 10973, Loss: 0.7490612864494324\n",
      "Iteration: 10974, Loss: 0.7490595579147339\n",
      "Iteration: 10975, Loss: 0.7490577101707458\n",
      "Iteration: 10976, Loss: 0.7490559816360474\n",
      "Iteration: 10977, Loss: 0.7490541338920593\n",
      "Iteration: 10978, Loss: 0.7490524053573608\n",
      "Iteration: 10979, Loss: 0.7490507960319519\n",
      "Iteration: 10980, Loss: 0.7490489482879639\n",
      "Iteration: 10981, Loss: 0.7490472793579102\n",
      "Iteration: 10982, Loss: 0.7490456104278564\n",
      "Iteration: 10983, Loss: 0.749043881893158\n",
      "Iteration: 10984, Loss: 0.7490420341491699\n",
      "Iteration: 10985, Loss: 0.749040424823761\n",
      "Iteration: 10986, Loss: 0.7490387558937073\n",
      "Iteration: 10987, Loss: 0.7490370273590088\n",
      "Iteration: 10988, Loss: 0.7490353584289551\n",
      "Iteration: 10989, Loss: 0.7490336298942566\n",
      "Iteration: 10990, Loss: 0.7490319013595581\n",
      "Iteration: 10991, Loss: 0.7490302920341492\n",
      "Iteration: 10992, Loss: 0.7490286231040955\n",
      "Iteration: 10993, Loss: 0.749026894569397\n",
      "Iteration: 10994, Loss: 0.7490252256393433\n",
      "Iteration: 10995, Loss: 0.7490235567092896\n",
      "Iteration: 10996, Loss: 0.7490218877792358\n",
      "Iteration: 10997, Loss: 0.7490202188491821\n",
      "Iteration: 10998, Loss: 0.7490184903144836\n",
      "Iteration: 10999, Loss: 0.7490168213844299\n",
      "Iteration: 11000, Loss: 0.7490150928497314\n",
      "Iteration: 11001, Loss: 0.749013364315033\n",
      "Iteration: 11002, Loss: 0.7490116953849792\n",
      "Iteration: 11003, Loss: 0.7490100264549255\n",
      "Iteration: 11004, Loss: 0.7490083575248718\n",
      "Iteration: 11005, Loss: 0.7490066885948181\n",
      "Iteration: 11006, Loss: 0.7490050196647644\n",
      "Iteration: 11007, Loss: 0.7490034103393555\n",
      "Iteration: 11008, Loss: 0.749001681804657\n",
      "Iteration: 11009, Loss: 0.7489998936653137\n",
      "Iteration: 11010, Loss: 0.74899822473526\n",
      "Iteration: 11011, Loss: 0.7489965558052063\n",
      "Iteration: 11012, Loss: 0.7489948868751526\n",
      "Iteration: 11013, Loss: 0.7489933371543884\n",
      "Iteration: 11014, Loss: 0.7489917278289795\n",
      "Iteration: 11015, Loss: 0.7489900588989258\n",
      "Iteration: 11016, Loss: 0.7489883899688721\n",
      "Iteration: 11017, Loss: 0.7489866614341736\n",
      "Iteration: 11018, Loss: 0.7489849925041199\n",
      "Iteration: 11019, Loss: 0.7489833235740662\n",
      "Iteration: 11020, Loss: 0.7489816546440125\n",
      "Iteration: 11021, Loss: 0.7489799857139587\n",
      "Iteration: 11022, Loss: 0.7489782571792603\n",
      "Iteration: 11023, Loss: 0.7489765882492065\n",
      "Iteration: 11024, Loss: 0.7489750981330872\n",
      "Iteration: 11025, Loss: 0.7489734292030334\n",
      "Iteration: 11026, Loss: 0.7489717602729797\n",
      "Iteration: 11027, Loss: 0.748970091342926\n",
      "Iteration: 11028, Loss: 0.7489684224128723\n",
      "Iteration: 11029, Loss: 0.7489667534828186\n",
      "Iteration: 11030, Loss: 0.7489651441574097\n",
      "Iteration: 11031, Loss: 0.748963475227356\n",
      "Iteration: 11032, Loss: 0.748961865901947\n",
      "Iteration: 11033, Loss: 0.7489601373672485\n",
      "Iteration: 11034, Loss: 0.7489584684371948\n",
      "Iteration: 11035, Loss: 0.7489568591117859\n",
      "Iteration: 11036, Loss: 0.7489553093910217\n",
      "Iteration: 11037, Loss: 0.748953640460968\n",
      "Iteration: 11038, Loss: 0.7489520311355591\n",
      "Iteration: 11039, Loss: 0.7489503622055054\n",
      "Iteration: 11040, Loss: 0.7489486932754517\n",
      "Iteration: 11041, Loss: 0.7489470839500427\n",
      "Iteration: 11042, Loss: 0.748945415019989\n",
      "Iteration: 11043, Loss: 0.7489437460899353\n",
      "Iteration: 11044, Loss: 0.7489421367645264\n",
      "Iteration: 11045, Loss: 0.7489404678344727\n",
      "Iteration: 11046, Loss: 0.7489388585090637\n",
      "Iteration: 11047, Loss: 0.7489373087882996\n",
      "Iteration: 11048, Loss: 0.7489356994628906\n",
      "Iteration: 11049, Loss: 0.7489340305328369\n",
      "Iteration: 11050, Loss: 0.748932421207428\n",
      "Iteration: 11051, Loss: 0.748930811882019\n",
      "Iteration: 11052, Loss: 0.7489291429519653\n",
      "Iteration: 11053, Loss: 0.7489275336265564\n",
      "Iteration: 11054, Loss: 0.7489259243011475\n",
      "Iteration: 11055, Loss: 0.7489242553710938\n",
      "Iteration: 11056, Loss: 0.7489227056503296\n",
      "Iteration: 11057, Loss: 0.7489210963249207\n",
      "Iteration: 11058, Loss: 0.7489194869995117\n",
      "Iteration: 11059, Loss: 0.7489178776741028\n",
      "Iteration: 11060, Loss: 0.7489162683486938\n",
      "Iteration: 11061, Loss: 0.7489146590232849\n",
      "Iteration: 11062, Loss: 0.7489131093025208\n",
      "Iteration: 11063, Loss: 0.7489114999771118\n",
      "Iteration: 11064, Loss: 0.7489098906517029\n",
      "Iteration: 11065, Loss: 0.748908281326294\n",
      "Iteration: 11066, Loss: 0.748906672000885\n",
      "Iteration: 11067, Loss: 0.7489050626754761\n",
      "Iteration: 11068, Loss: 0.7489034533500671\n",
      "Iteration: 11069, Loss: 0.7489018440246582\n",
      "Iteration: 11070, Loss: 0.7489003539085388\n",
      "Iteration: 11071, Loss: 0.7488986849784851\n",
      "Iteration: 11072, Loss: 0.7488970756530762\n",
      "Iteration: 11073, Loss: 0.7488954663276672\n",
      "Iteration: 11074, Loss: 0.7488938570022583\n",
      "Iteration: 11075, Loss: 0.7488923072814941\n",
      "Iteration: 11076, Loss: 0.7488906383514404\n",
      "Iteration: 11077, Loss: 0.7488890290260315\n",
      "Iteration: 11078, Loss: 0.7488874793052673\n",
      "Iteration: 11079, Loss: 0.7488858699798584\n",
      "Iteration: 11080, Loss: 0.7488843202590942\n",
      "Iteration: 11081, Loss: 0.7488827109336853\n",
      "Iteration: 11082, Loss: 0.7488811016082764\n",
      "Iteration: 11083, Loss: 0.7488795518875122\n",
      "Iteration: 11084, Loss: 0.7488778829574585\n",
      "Iteration: 11085, Loss: 0.7488762736320496\n",
      "Iteration: 11086, Loss: 0.7488747239112854\n",
      "Iteration: 11087, Loss: 0.748873233795166\n",
      "Iteration: 11088, Loss: 0.7488716244697571\n",
      "Iteration: 11089, Loss: 0.7488700747489929\n",
      "Iteration: 11090, Loss: 0.748868465423584\n",
      "Iteration: 11091, Loss: 0.748866856098175\n",
      "Iteration: 11092, Loss: 0.7488653063774109\n",
      "Iteration: 11093, Loss: 0.7488636374473572\n",
      "Iteration: 11094, Loss: 0.748862087726593\n",
      "Iteration: 11095, Loss: 0.7488605380058289\n",
      "Iteration: 11096, Loss: 0.7488589882850647\n",
      "Iteration: 11097, Loss: 0.7488573789596558\n",
      "Iteration: 11098, Loss: 0.7488558888435364\n",
      "Iteration: 11099, Loss: 0.7488542199134827\n",
      "Iteration: 11100, Loss: 0.7488526105880737\n",
      "Iteration: 11101, Loss: 0.7488510608673096\n",
      "Iteration: 11102, Loss: 0.7488495111465454\n",
      "Iteration: 11103, Loss: 0.7488480806350708\n",
      "Iteration: 11104, Loss: 0.7488465309143066\n",
      "Iteration: 11105, Loss: 0.7488449215888977\n",
      "Iteration: 11106, Loss: 0.7488433718681335\n",
      "Iteration: 11107, Loss: 0.7488417625427246\n",
      "Iteration: 11108, Loss: 0.7488402724266052\n",
      "Iteration: 11109, Loss: 0.7488387227058411\n",
      "Iteration: 11110, Loss: 0.7488371729850769\n",
      "Iteration: 11111, Loss: 0.7488356232643127\n",
      "Iteration: 11112, Loss: 0.7488340735435486\n",
      "Iteration: 11113, Loss: 0.7488325238227844\n",
      "Iteration: 11114, Loss: 0.7488309741020203\n",
      "Iteration: 11115, Loss: 0.7488294243812561\n",
      "Iteration: 11116, Loss: 0.7488278150558472\n",
      "Iteration: 11117, Loss: 0.748826265335083\n",
      "Iteration: 11118, Loss: 0.7488248944282532\n",
      "Iteration: 11119, Loss: 0.7488231658935547\n",
      "Iteration: 11120, Loss: 0.7488216161727905\n",
      "Iteration: 11121, Loss: 0.7488200664520264\n",
      "Iteration: 11122, Loss: 0.7488185167312622\n",
      "Iteration: 11123, Loss: 0.748816967010498\n",
      "Iteration: 11124, Loss: 0.7488154172897339\n",
      "Iteration: 11125, Loss: 0.7488138675689697\n",
      "Iteration: 11126, Loss: 0.7488124370574951\n",
      "Iteration: 11127, Loss: 0.748810887336731\n",
      "Iteration: 11128, Loss: 0.748809278011322\n",
      "Iteration: 11129, Loss: 0.7488078474998474\n",
      "Iteration: 11130, Loss: 0.7488062977790833\n",
      "Iteration: 11131, Loss: 0.7488047480583191\n",
      "Iteration: 11132, Loss: 0.7488030791282654\n",
      "Iteration: 11133, Loss: 0.7488017082214355\n",
      "Iteration: 11134, Loss: 0.7488001585006714\n",
      "Iteration: 11135, Loss: 0.7487987279891968\n",
      "Iteration: 11136, Loss: 0.7487971782684326\n",
      "Iteration: 11137, Loss: 0.7487956881523132\n",
      "Iteration: 11138, Loss: 0.7487941384315491\n",
      "Iteration: 11139, Loss: 0.7487926483154297\n",
      "Iteration: 11140, Loss: 0.7487911581993103\n",
      "Iteration: 11141, Loss: 0.7487896084785461\n",
      "Iteration: 11142, Loss: 0.748788058757782\n",
      "Iteration: 11143, Loss: 0.7487865090370178\n",
      "Iteration: 11144, Loss: 0.7487850189208984\n",
      "Iteration: 11145, Loss: 0.7487834692001343\n",
      "Iteration: 11146, Loss: 0.7487820982933044\n",
      "Iteration: 11147, Loss: 0.7487805485725403\n",
      "Iteration: 11148, Loss: 0.7487790584564209\n",
      "Iteration: 11149, Loss: 0.7487776279449463\n",
      "Iteration: 11150, Loss: 0.7487761378288269\n",
      "Iteration: 11151, Loss: 0.7487745881080627\n",
      "Iteration: 11152, Loss: 0.7487730979919434\n",
      "Iteration: 11153, Loss: 0.748771607875824\n",
      "Iteration: 11154, Loss: 0.7487700581550598\n",
      "Iteration: 11155, Loss: 0.7487685680389404\n",
      "Iteration: 11156, Loss: 0.748767077922821\n",
      "Iteration: 11157, Loss: 0.7487654685974121\n",
      "Iteration: 11158, Loss: 0.7487639784812927\n",
      "Iteration: 11159, Loss: 0.7487624883651733\n",
      "Iteration: 11160, Loss: 0.7487610578536987\n",
      "Iteration: 11161, Loss: 0.7487594485282898\n",
      "Iteration: 11162, Loss: 0.7487579584121704\n",
      "Iteration: 11163, Loss: 0.748756468296051\n",
      "Iteration: 11164, Loss: 0.7487550973892212\n",
      "Iteration: 11165, Loss: 0.7487536072731018\n",
      "Iteration: 11166, Loss: 0.7487520575523376\n",
      "Iteration: 11167, Loss: 0.7487505674362183\n",
      "Iteration: 11168, Loss: 0.7487490773200989\n",
      "Iteration: 11169, Loss: 0.7487475872039795\n",
      "Iteration: 11170, Loss: 0.7487460374832153\n",
      "Iteration: 11171, Loss: 0.7487446069717407\n",
      "Iteration: 11172, Loss: 0.7487431168556213\n",
      "Iteration: 11173, Loss: 0.7487415671348572\n",
      "Iteration: 11174, Loss: 0.7487400770187378\n",
      "Iteration: 11175, Loss: 0.7487385869026184\n",
      "Iteration: 11176, Loss: 0.748737096786499\n",
      "Iteration: 11177, Loss: 0.7487356662750244\n",
      "Iteration: 11178, Loss: 0.748734176158905\n",
      "Iteration: 11179, Loss: 0.7487326860427856\n",
      "Iteration: 11180, Loss: 0.748731255531311\n",
      "Iteration: 11181, Loss: 0.7487297654151917\n",
      "Iteration: 11182, Loss: 0.7487282752990723\n",
      "Iteration: 11183, Loss: 0.7487267851829529\n",
      "Iteration: 11184, Loss: 0.7487252950668335\n",
      "Iteration: 11185, Loss: 0.7487238645553589\n",
      "Iteration: 11186, Loss: 0.7487223744392395\n",
      "Iteration: 11187, Loss: 0.7487208247184753\n",
      "Iteration: 11188, Loss: 0.7487193942070007\n",
      "Iteration: 11189, Loss: 0.7487179040908813\n",
      "Iteration: 11190, Loss: 0.748716413974762\n",
      "Iteration: 11191, Loss: 0.7487150430679321\n",
      "Iteration: 11192, Loss: 0.7487135529518127\n",
      "Iteration: 11193, Loss: 0.7487120628356934\n",
      "Iteration: 11194, Loss: 0.748710572719574\n",
      "Iteration: 11195, Loss: 0.7487090826034546\n",
      "Iteration: 11196, Loss: 0.7487075924873352\n",
      "Iteration: 11197, Loss: 0.7487061619758606\n",
      "Iteration: 11198, Loss: 0.7487046718597412\n",
      "Iteration: 11199, Loss: 0.7487031817436218\n",
      "Iteration: 11200, Loss: 0.748701810836792\n",
      "Iteration: 11201, Loss: 0.7487003207206726\n",
      "Iteration: 11202, Loss: 0.748698890209198\n",
      "Iteration: 11203, Loss: 0.7486974000930786\n",
      "Iteration: 11204, Loss: 0.748695969581604\n",
      "Iteration: 11205, Loss: 0.748694658279419\n",
      "Iteration: 11206, Loss: 0.7486931681632996\n",
      "Iteration: 11207, Loss: 0.7486919164657593\n",
      "Iteration: 11208, Loss: 0.7486904859542847\n",
      "Iteration: 11209, Loss: 0.7486891150474548\n",
      "Iteration: 11210, Loss: 0.7486876845359802\n",
      "Iteration: 11211, Loss: 0.7486861944198608\n",
      "Iteration: 11212, Loss: 0.7486847639083862\n",
      "Iteration: 11213, Loss: 0.7486832141876221\n",
      "Iteration: 11214, Loss: 0.7486817836761475\n",
      "Iteration: 11215, Loss: 0.7486802339553833\n",
      "Iteration: 11216, Loss: 0.7486788630485535\n",
      "Iteration: 11217, Loss: 0.7486773133277893\n",
      "Iteration: 11218, Loss: 0.7486758828163147\n",
      "Iteration: 11219, Loss: 0.7486744523048401\n",
      "Iteration: 11220, Loss: 0.7486730217933655\n",
      "Iteration: 11221, Loss: 0.7486715912818909\n",
      "Iteration: 11222, Loss: 0.7486701607704163\n",
      "Iteration: 11223, Loss: 0.7486687302589417\n",
      "Iteration: 11224, Loss: 0.7486674785614014\n",
      "Iteration: 11225, Loss: 0.748665988445282\n",
      "Iteration: 11226, Loss: 0.7486645579338074\n",
      "Iteration: 11227, Loss: 0.7486631274223328\n",
      "Iteration: 11228, Loss: 0.7486615777015686\n",
      "Iteration: 11229, Loss: 0.748660147190094\n",
      "Iteration: 11230, Loss: 0.7486587166786194\n",
      "Iteration: 11231, Loss: 0.7486572861671448\n",
      "Iteration: 11232, Loss: 0.7486559152603149\n",
      "Iteration: 11233, Loss: 0.7486544847488403\n",
      "Iteration: 11234, Loss: 0.7486530542373657\n",
      "Iteration: 11235, Loss: 0.7486518025398254\n",
      "Iteration: 11236, Loss: 0.7486503720283508\n",
      "Iteration: 11237, Loss: 0.7486489415168762\n",
      "Iteration: 11238, Loss: 0.7486475706100464\n",
      "Iteration: 11239, Loss: 0.7486461400985718\n",
      "Iteration: 11240, Loss: 0.7486447095870972\n",
      "Iteration: 11241, Loss: 0.7486432790756226\n",
      "Iteration: 11242, Loss: 0.7486417889595032\n",
      "Iteration: 11243, Loss: 0.7486404180526733\n",
      "Iteration: 11244, Loss: 0.7486390471458435\n",
      "Iteration: 11245, Loss: 0.7486376166343689\n",
      "Iteration: 11246, Loss: 0.7486361861228943\n",
      "Iteration: 11247, Loss: 0.7486347556114197\n",
      "Iteration: 11248, Loss: 0.7486333847045898\n",
      "Iteration: 11249, Loss: 0.7486319541931152\n",
      "Iteration: 11250, Loss: 0.7486305236816406\n",
      "Iteration: 11251, Loss: 0.7486292719841003\n",
      "Iteration: 11252, Loss: 0.7486279010772705\n",
      "Iteration: 11253, Loss: 0.7486264705657959\n",
      "Iteration: 11254, Loss: 0.7486250400543213\n",
      "Iteration: 11255, Loss: 0.7486236691474915\n",
      "Iteration: 11256, Loss: 0.7486222386360168\n",
      "Iteration: 11257, Loss: 0.7486206889152527\n",
      "Iteration: 11258, Loss: 0.7486193776130676\n",
      "Iteration: 11259, Loss: 0.7486180067062378\n",
      "Iteration: 11260, Loss: 0.7486165761947632\n",
      "Iteration: 11261, Loss: 0.7486152052879333\n",
      "Iteration: 11262, Loss: 0.7486137747764587\n",
      "Iteration: 11263, Loss: 0.7486124038696289\n",
      "Iteration: 11264, Loss: 0.7486110925674438\n",
      "Iteration: 11265, Loss: 0.7486096024513245\n",
      "Iteration: 11266, Loss: 0.7486081719398499\n",
      "Iteration: 11267, Loss: 0.7486069202423096\n",
      "Iteration: 11268, Loss: 0.748605489730835\n",
      "Iteration: 11269, Loss: 0.7486041784286499\n",
      "Iteration: 11270, Loss: 0.7486027479171753\n",
      "Iteration: 11271, Loss: 0.7486013770103455\n",
      "Iteration: 11272, Loss: 0.7485999464988708\n",
      "Iteration: 11273, Loss: 0.748598575592041\n",
      "Iteration: 11274, Loss: 0.7485972046852112\n",
      "Iteration: 11275, Loss: 0.7485958337783813\n",
      "Iteration: 11276, Loss: 0.748594343662262\n",
      "Iteration: 11277, Loss: 0.7485929727554321\n",
      "Iteration: 11278, Loss: 0.7485918402671814\n",
      "Iteration: 11279, Loss: 0.748590350151062\n",
      "Iteration: 11280, Loss: 0.7485889196395874\n",
      "Iteration: 11281, Loss: 0.7485875487327576\n",
      "Iteration: 11282, Loss: 0.748586118221283\n",
      "Iteration: 11283, Loss: 0.7485849261283875\n",
      "Iteration: 11284, Loss: 0.7485834956169128\n",
      "Iteration: 11285, Loss: 0.7485821843147278\n",
      "Iteration: 11286, Loss: 0.7485808730125427\n",
      "Iteration: 11287, Loss: 0.7485795021057129\n",
      "Iteration: 11288, Loss: 0.7485781908035278\n",
      "Iteration: 11289, Loss: 0.7485767006874084\n",
      "Iteration: 11290, Loss: 0.7485754489898682\n",
      "Iteration: 11291, Loss: 0.7485740184783936\n",
      "Iteration: 11292, Loss: 0.7485726475715637\n",
      "Iteration: 11293, Loss: 0.7485713362693787\n",
      "Iteration: 11294, Loss: 0.7485699653625488\n",
      "Iteration: 11295, Loss: 0.7485684752464294\n",
      "Iteration: 11296, Loss: 0.7485672235488892\n",
      "Iteration: 11297, Loss: 0.7485658526420593\n",
      "Iteration: 11298, Loss: 0.7485644817352295\n",
      "Iteration: 11299, Loss: 0.7485631704330444\n",
      "Iteration: 11300, Loss: 0.7485617399215698\n",
      "Iteration: 11301, Loss: 0.7485605478286743\n",
      "Iteration: 11302, Loss: 0.7485591769218445\n",
      "Iteration: 11303, Loss: 0.7485578060150146\n",
      "Iteration: 11304, Loss: 0.7485565543174744\n",
      "Iteration: 11305, Loss: 0.7485551834106445\n",
      "Iteration: 11306, Loss: 0.7485538125038147\n",
      "Iteration: 11307, Loss: 0.7485524415969849\n",
      "Iteration: 11308, Loss: 0.748551070690155\n",
      "Iteration: 11309, Loss: 0.74854975938797\n",
      "Iteration: 11310, Loss: 0.7485483884811401\n",
      "Iteration: 11311, Loss: 0.7485471963882446\n",
      "Iteration: 11312, Loss: 0.7485458254814148\n",
      "Iteration: 11313, Loss: 0.748544454574585\n",
      "Iteration: 11314, Loss: 0.7485432028770447\n",
      "Iteration: 11315, Loss: 0.7485418319702148\n",
      "Iteration: 11316, Loss: 0.7485405206680298\n",
      "Iteration: 11317, Loss: 0.7485390305519104\n",
      "Iteration: 11318, Loss: 0.7485378980636597\n",
      "Iteration: 11319, Loss: 0.7485364675521851\n",
      "Iteration: 11320, Loss: 0.7485350966453552\n",
      "Iteration: 11321, Loss: 0.7485337853431702\n",
      "Iteration: 11322, Loss: 0.7485324144363403\n",
      "Iteration: 11323, Loss: 0.7485311031341553\n",
      "Iteration: 11324, Loss: 0.7485297918319702\n",
      "Iteration: 11325, Loss: 0.7485284805297852\n",
      "Iteration: 11326, Loss: 0.7485271692276001\n",
      "Iteration: 11327, Loss: 0.7485257983207703\n",
      "Iteration: 11328, Loss: 0.7485244870185852\n",
      "Iteration: 11329, Loss: 0.7485231757164001\n",
      "Iteration: 11330, Loss: 0.7485218644142151\n",
      "Iteration: 11331, Loss: 0.74852055311203\n",
      "Iteration: 11332, Loss: 0.7485191822052002\n",
      "Iteration: 11333, Loss: 0.7485178709030151\n",
      "Iteration: 11334, Loss: 0.7485166192054749\n",
      "Iteration: 11335, Loss: 0.7485153675079346\n",
      "Iteration: 11336, Loss: 0.7485140562057495\n",
      "Iteration: 11337, Loss: 0.7485127449035645\n",
      "Iteration: 11338, Loss: 0.7485114336013794\n",
      "Iteration: 11339, Loss: 0.7485100626945496\n",
      "Iteration: 11340, Loss: 0.7485088109970093\n",
      "Iteration: 11341, Loss: 0.7485074996948242\n",
      "Iteration: 11342, Loss: 0.7485061287879944\n",
      "Iteration: 11343, Loss: 0.7485047578811646\n",
      "Iteration: 11344, Loss: 0.7485035061836243\n",
      "Iteration: 11345, Loss: 0.7485021948814392\n",
      "Iteration: 11346, Loss: 0.7485008835792542\n",
      "Iteration: 11347, Loss: 0.7484995722770691\n",
      "Iteration: 11348, Loss: 0.7484983205795288\n",
      "Iteration: 11349, Loss: 0.748496949672699\n",
      "Iteration: 11350, Loss: 0.7484955787658691\n",
      "Iteration: 11351, Loss: 0.7484942674636841\n",
      "Iteration: 11352, Loss: 0.7484930157661438\n",
      "Iteration: 11353, Loss: 0.7484918236732483\n",
      "Iteration: 11354, Loss: 0.7484905123710632\n",
      "Iteration: 11355, Loss: 0.7484892010688782\n",
      "Iteration: 11356, Loss: 0.7484878897666931\n",
      "Iteration: 11357, Loss: 0.7484866380691528\n",
      "Iteration: 11358, Loss: 0.7484853267669678\n",
      "Iteration: 11359, Loss: 0.7484839558601379\n",
      "Iteration: 11360, Loss: 0.7484827041625977\n",
      "Iteration: 11361, Loss: 0.7484814524650574\n",
      "Iteration: 11362, Loss: 0.7484801411628723\n",
      "Iteration: 11363, Loss: 0.7484787702560425\n",
      "Iteration: 11364, Loss: 0.7484774589538574\n",
      "Iteration: 11365, Loss: 0.7484762072563171\n",
      "Iteration: 11366, Loss: 0.7484748959541321\n",
      "Iteration: 11367, Loss: 0.7484736442565918\n",
      "Iteration: 11368, Loss: 0.7484723329544067\n",
      "Iteration: 11369, Loss: 0.7484710812568665\n",
      "Iteration: 11370, Loss: 0.7484697699546814\n",
      "Iteration: 11371, Loss: 0.7484685778617859\n",
      "Iteration: 11372, Loss: 0.7484672665596008\n",
      "Iteration: 11373, Loss: 0.7484660148620605\n",
      "Iteration: 11374, Loss: 0.7484647035598755\n",
      "Iteration: 11375, Loss: 0.7484633922576904\n",
      "Iteration: 11376, Loss: 0.7484620809555054\n",
      "Iteration: 11377, Loss: 0.7484608888626099\n",
      "Iteration: 11378, Loss: 0.7484596371650696\n",
      "Iteration: 11379, Loss: 0.7484583258628845\n",
      "Iteration: 11380, Loss: 0.7484570741653442\n",
      "Iteration: 11381, Loss: 0.7484557628631592\n",
      "Iteration: 11382, Loss: 0.7484544515609741\n",
      "Iteration: 11383, Loss: 0.7484531998634338\n",
      "Iteration: 11384, Loss: 0.748451828956604\n",
      "Iteration: 11385, Loss: 0.748450517654419\n",
      "Iteration: 11386, Loss: 0.7484493255615234\n",
      "Iteration: 11387, Loss: 0.7484480142593384\n",
      "Iteration: 11388, Loss: 0.7484468817710876\n",
      "Iteration: 11389, Loss: 0.7484456300735474\n",
      "Iteration: 11390, Loss: 0.7484442591667175\n",
      "Iteration: 11391, Loss: 0.748443067073822\n",
      "Iteration: 11392, Loss: 0.7484416961669922\n",
      "Iteration: 11393, Loss: 0.7484404444694519\n",
      "Iteration: 11394, Loss: 0.7484393119812012\n",
      "Iteration: 11395, Loss: 0.7484380602836609\n",
      "Iteration: 11396, Loss: 0.7484368085861206\n",
      "Iteration: 11397, Loss: 0.7484354376792908\n",
      "Iteration: 11398, Loss: 0.7484341859817505\n",
      "Iteration: 11399, Loss: 0.7484328746795654\n",
      "Iteration: 11400, Loss: 0.7484316229820251\n",
      "Iteration: 11401, Loss: 0.7484303116798401\n",
      "Iteration: 11402, Loss: 0.7484290599822998\n",
      "Iteration: 11403, Loss: 0.7484278082847595\n",
      "Iteration: 11404, Loss: 0.7484264969825745\n",
      "Iteration: 11405, Loss: 0.748425304889679\n",
      "Iteration: 11406, Loss: 0.7484239935874939\n",
      "Iteration: 11407, Loss: 0.7484228014945984\n",
      "Iteration: 11408, Loss: 0.7484215497970581\n",
      "Iteration: 11409, Loss: 0.7484202980995178\n",
      "Iteration: 11410, Loss: 0.7484190464019775\n",
      "Iteration: 11411, Loss: 0.748417854309082\n",
      "Iteration: 11412, Loss: 0.7484166026115417\n",
      "Iteration: 11413, Loss: 0.7484153509140015\n",
      "Iteration: 11414, Loss: 0.7484140992164612\n",
      "Iteration: 11415, Loss: 0.7484127879142761\n",
      "Iteration: 11416, Loss: 0.7484115362167358\n",
      "Iteration: 11417, Loss: 0.7484102845191956\n",
      "Iteration: 11418, Loss: 0.7484090328216553\n",
      "Iteration: 11419, Loss: 0.748407781124115\n",
      "Iteration: 11420, Loss: 0.7484065294265747\n",
      "Iteration: 11421, Loss: 0.7484052777290344\n",
      "Iteration: 11422, Loss: 0.7484040260314941\n",
      "Iteration: 11423, Loss: 0.7484027147293091\n",
      "Iteration: 11424, Loss: 0.7484014630317688\n",
      "Iteration: 11425, Loss: 0.7484002113342285\n",
      "Iteration: 11426, Loss: 0.7483991384506226\n",
      "Iteration: 11427, Loss: 0.7483978867530823\n",
      "Iteration: 11428, Loss: 0.7483966946601868\n",
      "Iteration: 11429, Loss: 0.7483953237533569\n",
      "Iteration: 11430, Loss: 0.7483940720558167\n",
      "Iteration: 11431, Loss: 0.7483929395675659\n",
      "Iteration: 11432, Loss: 0.7483916878700256\n",
      "Iteration: 11433, Loss: 0.7483904361724854\n",
      "Iteration: 11434, Loss: 0.7483891844749451\n",
      "Iteration: 11435, Loss: 0.7483879327774048\n",
      "Iteration: 11436, Loss: 0.7483866810798645\n",
      "Iteration: 11437, Loss: 0.7483854293823242\n",
      "Iteration: 11438, Loss: 0.7483841776847839\n",
      "Iteration: 11439, Loss: 0.7483829259872437\n",
      "Iteration: 11440, Loss: 0.7483816742897034\n",
      "Iteration: 11441, Loss: 0.7483805418014526\n",
      "Iteration: 11442, Loss: 0.7483792901039124\n",
      "Iteration: 11443, Loss: 0.7483780384063721\n",
      "Iteration: 11444, Loss: 0.7483767867088318\n",
      "Iteration: 11445, Loss: 0.7483755350112915\n",
      "Iteration: 11446, Loss: 0.7483742833137512\n",
      "Iteration: 11447, Loss: 0.7483730912208557\n",
      "Iteration: 11448, Loss: 0.7483718395233154\n",
      "Iteration: 11449, Loss: 0.7483705878257751\n",
      "Iteration: 11450, Loss: 0.7483693361282349\n",
      "Iteration: 11451, Loss: 0.7483683228492737\n",
      "Iteration: 11452, Loss: 0.7483670711517334\n",
      "Iteration: 11453, Loss: 0.7483659386634827\n",
      "Iteration: 11454, Loss: 0.7483646869659424\n",
      "Iteration: 11455, Loss: 0.7483634948730469\n",
      "Iteration: 11456, Loss: 0.7483622431755066\n",
      "Iteration: 11457, Loss: 0.7483609914779663\n",
      "Iteration: 11458, Loss: 0.7483597993850708\n",
      "Iteration: 11459, Loss: 0.7483585476875305\n",
      "Iteration: 11460, Loss: 0.7483572959899902\n",
      "Iteration: 11461, Loss: 0.7483561038970947\n",
      "Iteration: 11462, Loss: 0.7483549118041992\n",
      "Iteration: 11463, Loss: 0.7483537197113037\n",
      "Iteration: 11464, Loss: 0.7483524680137634\n",
      "Iteration: 11465, Loss: 0.7483512163162231\n",
      "Iteration: 11466, Loss: 0.7483499050140381\n",
      "Iteration: 11467, Loss: 0.748348593711853\n",
      "Iteration: 11468, Loss: 0.7483474612236023\n",
      "Iteration: 11469, Loss: 0.7483462691307068\n",
      "Iteration: 11470, Loss: 0.7483450770378113\n",
      "Iteration: 11471, Loss: 0.748343825340271\n",
      "Iteration: 11472, Loss: 0.748342752456665\n",
      "Iteration: 11473, Loss: 0.7483415603637695\n",
      "Iteration: 11474, Loss: 0.748340368270874\n",
      "Iteration: 11475, Loss: 0.7483391165733337\n",
      "Iteration: 11476, Loss: 0.7483379244804382\n",
      "Iteration: 11477, Loss: 0.7483367323875427\n",
      "Iteration: 11478, Loss: 0.7483355402946472\n",
      "Iteration: 11479, Loss: 0.7483343482017517\n",
      "Iteration: 11480, Loss: 0.7483330965042114\n",
      "Iteration: 11481, Loss: 0.7483319044113159\n",
      "Iteration: 11482, Loss: 0.7483307123184204\n",
      "Iteration: 11483, Loss: 0.7483294606208801\n",
      "Iteration: 11484, Loss: 0.7483282685279846\n",
      "Iteration: 11485, Loss: 0.7483270764350891\n",
      "Iteration: 11486, Loss: 0.7483258843421936\n",
      "Iteration: 11487, Loss: 0.7483246922492981\n",
      "Iteration: 11488, Loss: 0.7483235001564026\n",
      "Iteration: 11489, Loss: 0.7483222484588623\n",
      "Iteration: 11490, Loss: 0.7483210563659668\n",
      "Iteration: 11491, Loss: 0.7483198642730713\n",
      "Iteration: 11492, Loss: 0.7483187913894653\n",
      "Iteration: 11493, Loss: 0.748317539691925\n",
      "Iteration: 11494, Loss: 0.7483163475990295\n",
      "Iteration: 11495, Loss: 0.7483150959014893\n",
      "Iteration: 11496, Loss: 0.7483139038085938\n",
      "Iteration: 11497, Loss: 0.748312771320343\n",
      "Iteration: 11498, Loss: 0.7483115196228027\n",
      "Iteration: 11499, Loss: 0.7483103275299072\n",
      "Iteration: 11500, Loss: 0.7483091354370117\n",
      "Iteration: 11501, Loss: 0.7483079433441162\n",
      "Iteration: 11502, Loss: 0.7483067512512207\n",
      "Iteration: 11503, Loss: 0.7483057379722595\n",
      "Iteration: 11504, Loss: 0.7483044862747192\n",
      "Iteration: 11505, Loss: 0.748303234577179\n",
      "Iteration: 11506, Loss: 0.7483020424842834\n",
      "Iteration: 11507, Loss: 0.7483008503913879\n",
      "Iteration: 11508, Loss: 0.7482997179031372\n",
      "Iteration: 11509, Loss: 0.7482985258102417\n",
      "Iteration: 11510, Loss: 0.7482973337173462\n",
      "Iteration: 11511, Loss: 0.7482961416244507\n",
      "Iteration: 11512, Loss: 0.7482950687408447\n",
      "Iteration: 11513, Loss: 0.7482938766479492\n",
      "Iteration: 11514, Loss: 0.7482926845550537\n",
      "Iteration: 11515, Loss: 0.7482914924621582\n",
      "Iteration: 11516, Loss: 0.7482903003692627\n",
      "Iteration: 11517, Loss: 0.7482891082763672\n",
      "Iteration: 11518, Loss: 0.7482879161834717\n",
      "Iteration: 11519, Loss: 0.7482867240905762\n",
      "Iteration: 11520, Loss: 0.7482855319976807\n",
      "Iteration: 11521, Loss: 0.7482843399047852\n",
      "Iteration: 11522, Loss: 0.7482831478118896\n",
      "Iteration: 11523, Loss: 0.7482819557189941\n",
      "Iteration: 11524, Loss: 0.7482807636260986\n",
      "Iteration: 11525, Loss: 0.7482796907424927\n",
      "Iteration: 11526, Loss: 0.7482784986495972\n",
      "Iteration: 11527, Loss: 0.7482773065567017\n",
      "Iteration: 11528, Loss: 0.7482761144638062\n",
      "Iteration: 11529, Loss: 0.7482749819755554\n",
      "Iteration: 11530, Loss: 0.7482737898826599\n",
      "Iteration: 11531, Loss: 0.7482725977897644\n",
      "Iteration: 11532, Loss: 0.7482714653015137\n",
      "Iteration: 11533, Loss: 0.7482702732086182\n",
      "Iteration: 11534, Loss: 0.7482690811157227\n",
      "Iteration: 11535, Loss: 0.7482678890228271\n",
      "Iteration: 11536, Loss: 0.7482666969299316\n",
      "Iteration: 11537, Loss: 0.74826580286026\n",
      "Iteration: 11538, Loss: 0.7482646107673645\n",
      "Iteration: 11539, Loss: 0.748263418674469\n",
      "Iteration: 11540, Loss: 0.7482622265815735\n",
      "Iteration: 11541, Loss: 0.7482610940933228\n",
      "Iteration: 11542, Loss: 0.7482599020004272\n",
      "Iteration: 11543, Loss: 0.7482587695121765\n",
      "Iteration: 11544, Loss: 0.748257577419281\n",
      "Iteration: 11545, Loss: 0.7482564449310303\n",
      "Iteration: 11546, Loss: 0.7482552528381348\n",
      "Iteration: 11547, Loss: 0.7482541799545288\n",
      "Iteration: 11548, Loss: 0.7482531666755676\n",
      "Iteration: 11549, Loss: 0.7482519745826721\n",
      "Iteration: 11550, Loss: 0.7482508420944214\n",
      "Iteration: 11551, Loss: 0.7482496500015259\n",
      "Iteration: 11552, Loss: 0.7482485175132751\n",
      "Iteration: 11553, Loss: 0.7482473254203796\n",
      "Iteration: 11554, Loss: 0.7482461929321289\n",
      "Iteration: 11555, Loss: 0.7482450604438782\n",
      "Iteration: 11556, Loss: 0.7482438683509827\n",
      "Iteration: 11557, Loss: 0.7482427358627319\n",
      "Iteration: 11558, Loss: 0.7482416033744812\n",
      "Iteration: 11559, Loss: 0.7482404112815857\n",
      "Iteration: 11560, Loss: 0.748239278793335\n",
      "Iteration: 11561, Loss: 0.7482380867004395\n",
      "Iteration: 11562, Loss: 0.7482369542121887\n",
      "Iteration: 11563, Loss: 0.7482357621192932\n",
      "Iteration: 11564, Loss: 0.7482345700263977\n",
      "Iteration: 11565, Loss: 0.7482334971427917\n",
      "Iteration: 11566, Loss: 0.748232364654541\n",
      "Iteration: 11567, Loss: 0.7482311725616455\n",
      "Iteration: 11568, Loss: 0.7482300400733948\n",
      "Iteration: 11569, Loss: 0.7482288479804993\n",
      "Iteration: 11570, Loss: 0.7482277154922485\n",
      "Iteration: 11571, Loss: 0.7482265830039978\n",
      "Iteration: 11572, Loss: 0.7482252717018127\n",
      "Iteration: 11573, Loss: 0.748224139213562\n",
      "Iteration: 11574, Loss: 0.7482230067253113\n",
      "Iteration: 11575, Loss: 0.7482218742370605\n",
      "Iteration: 11576, Loss: 0.7482209801673889\n",
      "Iteration: 11577, Loss: 0.7482197284698486\n",
      "Iteration: 11578, Loss: 0.7482185959815979\n",
      "Iteration: 11579, Loss: 0.7482174038887024\n",
      "Iteration: 11580, Loss: 0.7482162714004517\n",
      "Iteration: 11581, Loss: 0.7482150793075562\n",
      "Iteration: 11582, Loss: 0.7482141256332397\n",
      "Iteration: 11583, Loss: 0.7482129335403442\n",
      "Iteration: 11584, Loss: 0.7482118010520935\n",
      "Iteration: 11585, Loss: 0.7482107281684875\n",
      "Iteration: 11586, Loss: 0.7482097148895264\n",
      "Iteration: 11587, Loss: 0.7482085227966309\n",
      "Iteration: 11588, Loss: 0.7482073903083801\n",
      "Iteration: 11589, Loss: 0.7482062578201294\n",
      "Iteration: 11590, Loss: 0.7482051253318787\n",
      "Iteration: 11591, Loss: 0.7482039928436279\n",
      "Iteration: 11592, Loss: 0.7482028603553772\n",
      "Iteration: 11593, Loss: 0.7482017278671265\n",
      "Iteration: 11594, Loss: 0.748200535774231\n",
      "Iteration: 11595, Loss: 0.7481994032859802\n",
      "Iteration: 11596, Loss: 0.7481982707977295\n",
      "Iteration: 11597, Loss: 0.7481971383094788\n",
      "Iteration: 11598, Loss: 0.748196005821228\n",
      "Iteration: 11599, Loss: 0.7481948733329773\n",
      "Iteration: 11600, Loss: 0.7481937408447266\n",
      "Iteration: 11601, Loss: 0.7481927275657654\n",
      "Iteration: 11602, Loss: 0.7481915950775146\n",
      "Iteration: 11603, Loss: 0.7481904625892639\n",
      "Iteration: 11604, Loss: 0.7481893301010132\n",
      "Iteration: 11605, Loss: 0.7481881380081177\n",
      "Iteration: 11606, Loss: 0.7481870055198669\n",
      "Iteration: 11607, Loss: 0.748185932636261\n",
      "Iteration: 11608, Loss: 0.7481848001480103\n",
      "Iteration: 11609, Loss: 0.7481836676597595\n",
      "Iteration: 11610, Loss: 0.748182475566864\n",
      "Iteration: 11611, Loss: 0.7481813430786133\n",
      "Iteration: 11612, Loss: 0.7481802105903625\n",
      "Iteration: 11613, Loss: 0.748179018497467\n",
      "Iteration: 11614, Loss: 0.7481780052185059\n",
      "Iteration: 11615, Loss: 0.7481768727302551\n",
      "Iteration: 11616, Loss: 0.7481757402420044\n",
      "Iteration: 11617, Loss: 0.7481746077537537\n",
      "Iteration: 11618, Loss: 0.7481734752655029\n",
      "Iteration: 11619, Loss: 0.7481723427772522\n",
      "Iteration: 11620, Loss: 0.7481712102890015\n",
      "Iteration: 11621, Loss: 0.7481700778007507\n",
      "Iteration: 11622, Loss: 0.7481691837310791\n",
      "Iteration: 11623, Loss: 0.7481680512428284\n",
      "Iteration: 11624, Loss: 0.7481669783592224\n",
      "Iteration: 11625, Loss: 0.7481658458709717\n",
      "Iteration: 11626, Loss: 0.748164713382721\n",
      "Iteration: 11627, Loss: 0.7481635808944702\n",
      "Iteration: 11628, Loss: 0.7481624484062195\n",
      "Iteration: 11629, Loss: 0.7481613755226135\n",
      "Iteration: 11630, Loss: 0.7481602430343628\n",
      "Iteration: 11631, Loss: 0.7481591105461121\n",
      "Iteration: 11632, Loss: 0.7481580972671509\n",
      "Iteration: 11633, Loss: 0.7481569647789001\n",
      "Iteration: 11634, Loss: 0.7481558322906494\n",
      "Iteration: 11635, Loss: 0.7481547594070435\n",
      "Iteration: 11636, Loss: 0.7481536269187927\n",
      "Iteration: 11637, Loss: 0.748152494430542\n",
      "Iteration: 11638, Loss: 0.7481513619422913\n",
      "Iteration: 11639, Loss: 0.7481502890586853\n",
      "Iteration: 11640, Loss: 0.7481492161750793\n",
      "Iteration: 11641, Loss: 0.7481480836868286\n",
      "Iteration: 11642, Loss: 0.7481470108032227\n",
      "Iteration: 11643, Loss: 0.7481459379196167\n",
      "Iteration: 11644, Loss: 0.748144805431366\n",
      "Iteration: 11645, Loss: 0.7481436729431152\n",
      "Iteration: 11646, Loss: 0.7481426000595093\n",
      "Iteration: 11647, Loss: 0.7481414675712585\n",
      "Iteration: 11648, Loss: 0.7481403350830078\n",
      "Iteration: 11649, Loss: 0.7481392621994019\n",
      "Iteration: 11650, Loss: 0.7481381297111511\n",
      "Iteration: 11651, Loss: 0.7481369972229004\n",
      "Iteration: 11652, Loss: 0.7481359243392944\n",
      "Iteration: 11653, Loss: 0.7481347918510437\n",
      "Iteration: 11654, Loss: 0.7481338381767273\n",
      "Iteration: 11655, Loss: 0.7481327056884766\n",
      "Iteration: 11656, Loss: 0.7481315732002258\n",
      "Iteration: 11657, Loss: 0.7481305599212646\n",
      "Iteration: 11658, Loss: 0.7481294274330139\n",
      "Iteration: 11659, Loss: 0.748128354549408\n",
      "Iteration: 11660, Loss: 0.7481272220611572\n",
      "Iteration: 11661, Loss: 0.7481261491775513\n",
      "Iteration: 11662, Loss: 0.7481250166893005\n",
      "Iteration: 11663, Loss: 0.7481239438056946\n",
      "Iteration: 11664, Loss: 0.7481228113174438\n",
      "Iteration: 11665, Loss: 0.7481216788291931\n",
      "Iteration: 11666, Loss: 0.7481206059455872\n",
      "Iteration: 11667, Loss: 0.7481195330619812\n",
      "Iteration: 11668, Loss: 0.7481184005737305\n",
      "Iteration: 11669, Loss: 0.7481175065040588\n",
      "Iteration: 11670, Loss: 0.7481163740158081\n",
      "Iteration: 11671, Loss: 0.7481153011322021\n",
      "Iteration: 11672, Loss: 0.7481142282485962\n",
      "Iteration: 11673, Loss: 0.7481130957603455\n",
      "Iteration: 11674, Loss: 0.7481120228767395\n",
      "Iteration: 11675, Loss: 0.7481109499931335\n",
      "Iteration: 11676, Loss: 0.7481098175048828\n",
      "Iteration: 11677, Loss: 0.7481087446212769\n",
      "Iteration: 11678, Loss: 0.7481076121330261\n",
      "Iteration: 11679, Loss: 0.7481065988540649\n",
      "Iteration: 11680, Loss: 0.748105525970459\n",
      "Iteration: 11681, Loss: 0.7481045722961426\n",
      "Iteration: 11682, Loss: 0.7481034398078918\n",
      "Iteration: 11683, Loss: 0.7481023669242859\n",
      "Iteration: 11684, Loss: 0.7481012940406799\n",
      "Iteration: 11685, Loss: 0.7481001615524292\n",
      "Iteration: 11686, Loss: 0.7480990290641785\n",
      "Iteration: 11687, Loss: 0.7480979561805725\n",
      "Iteration: 11688, Loss: 0.7480968832969666\n",
      "Iteration: 11689, Loss: 0.7480957508087158\n",
      "Iteration: 11690, Loss: 0.7480946779251099\n",
      "Iteration: 11691, Loss: 0.7480936050415039\n",
      "Iteration: 11692, Loss: 0.7480925917625427\n",
      "Iteration: 11693, Loss: 0.7480913996696472\n",
      "Iteration: 11694, Loss: 0.7480903267860413\n",
      "Iteration: 11695, Loss: 0.7480892539024353\n",
      "Iteration: 11696, Loss: 0.7480881810188293\n",
      "Iteration: 11697, Loss: 0.7480871081352234\n",
      "Iteration: 11698, Loss: 0.7480860948562622\n",
      "Iteration: 11699, Loss: 0.7480849623680115\n",
      "Iteration: 11700, Loss: 0.7480838894844055\n",
      "Iteration: 11701, Loss: 0.7480828166007996\n",
      "Iteration: 11702, Loss: 0.7480818033218384\n",
      "Iteration: 11703, Loss: 0.7480807900428772\n",
      "Iteration: 11704, Loss: 0.7480797171592712\n",
      "Iteration: 11705, Loss: 0.7480787038803101\n",
      "Iteration: 11706, Loss: 0.7480776309967041\n",
      "Iteration: 11707, Loss: 0.7480765581130981\n",
      "Iteration: 11708, Loss: 0.7480754852294922\n",
      "Iteration: 11709, Loss: 0.7480744123458862\n",
      "Iteration: 11710, Loss: 0.7480733394622803\n",
      "Iteration: 11711, Loss: 0.7480722665786743\n",
      "Iteration: 11712, Loss: 0.7480711936950684\n",
      "Iteration: 11713, Loss: 0.7480701804161072\n",
      "Iteration: 11714, Loss: 0.7480691075325012\n",
      "Iteration: 11715, Loss: 0.7480679750442505\n",
      "Iteration: 11716, Loss: 0.7480669617652893\n",
      "Iteration: 11717, Loss: 0.7480658888816833\n",
      "Iteration: 11718, Loss: 0.7480648159980774\n",
      "Iteration: 11719, Loss: 0.7480637431144714\n",
      "Iteration: 11720, Loss: 0.7480626702308655\n",
      "Iteration: 11721, Loss: 0.7480615973472595\n",
      "Iteration: 11722, Loss: 0.7480605244636536\n",
      "Iteration: 11723, Loss: 0.7480594515800476\n",
      "Iteration: 11724, Loss: 0.7480584979057312\n",
      "Iteration: 11725, Loss: 0.7480574250221252\n",
      "Iteration: 11726, Loss: 0.7480564117431641\n",
      "Iteration: 11727, Loss: 0.7480553388595581\n",
      "Iteration: 11728, Loss: 0.7480542659759521\n",
      "Iteration: 11729, Loss: 0.7480531930923462\n",
      "Iteration: 11730, Loss: 0.7480521202087402\n",
      "Iteration: 11731, Loss: 0.7480510473251343\n",
      "Iteration: 11732, Loss: 0.7480500340461731\n",
      "Iteration: 11733, Loss: 0.7480490803718567\n",
      "Iteration: 11734, Loss: 0.7480480074882507\n",
      "Iteration: 11735, Loss: 0.7480469346046448\n",
      "Iteration: 11736, Loss: 0.7480458617210388\n",
      "Iteration: 11737, Loss: 0.7480447888374329\n",
      "Iteration: 11738, Loss: 0.7480437755584717\n",
      "Iteration: 11739, Loss: 0.7480427026748657\n",
      "Iteration: 11740, Loss: 0.748041570186615\n",
      "Iteration: 11741, Loss: 0.748040497303009\n",
      "Iteration: 11742, Loss: 0.7480395436286926\n",
      "Iteration: 11743, Loss: 0.7480384707450867\n",
      "Iteration: 11744, Loss: 0.7480373978614807\n",
      "Iteration: 11745, Loss: 0.7480363249778748\n",
      "Iteration: 11746, Loss: 0.7480354905128479\n",
      "Iteration: 11747, Loss: 0.7480344176292419\n",
      "Iteration: 11748, Loss: 0.748033344745636\n",
      "Iteration: 11749, Loss: 0.74803227186203\n",
      "Iteration: 11750, Loss: 0.7480312585830688\n",
      "Iteration: 11751, Loss: 0.7480301856994629\n",
      "Iteration: 11752, Loss: 0.7480291724205017\n",
      "Iteration: 11753, Loss: 0.7480280995368958\n",
      "Iteration: 11754, Loss: 0.7480270266532898\n",
      "Iteration: 11755, Loss: 0.7480261325836182\n",
      "Iteration: 11756, Loss: 0.7480250597000122\n",
      "Iteration: 11757, Loss: 0.748024046421051\n",
      "Iteration: 11758, Loss: 0.7480229735374451\n",
      "Iteration: 11759, Loss: 0.7480219602584839\n",
      "Iteration: 11760, Loss: 0.7480208873748779\n",
      "Iteration: 11761, Loss: 0.7480198740959167\n",
      "Iteration: 11762, Loss: 0.7480188012123108\n",
      "Iteration: 11763, Loss: 0.7480179071426392\n",
      "Iteration: 11764, Loss: 0.7480168342590332\n",
      "Iteration: 11765, Loss: 0.748015820980072\n",
      "Iteration: 11766, Loss: 0.7480147480964661\n",
      "Iteration: 11767, Loss: 0.7480139136314392\n",
      "Iteration: 11768, Loss: 0.7480128407478333\n",
      "Iteration: 11769, Loss: 0.7480118274688721\n",
      "Iteration: 11770, Loss: 0.7480108737945557\n",
      "Iteration: 11771, Loss: 0.7480096220970154\n",
      "Iteration: 11772, Loss: 0.7480086088180542\n",
      "Iteration: 11773, Loss: 0.748007595539093\n",
      "Iteration: 11774, Loss: 0.7480065226554871\n",
      "Iteration: 11775, Loss: 0.7480054497718811\n",
      "Iteration: 11776, Loss: 0.7480044364929199\n",
      "Iteration: 11777, Loss: 0.748003363609314\n",
      "Iteration: 11778, Loss: 0.7480024099349976\n",
      "Iteration: 11779, Loss: 0.7480013966560364\n",
      "Iteration: 11780, Loss: 0.7480002641677856\n",
      "Iteration: 11781, Loss: 0.7479992508888245\n",
      "Iteration: 11782, Loss: 0.7479982376098633\n",
      "Iteration: 11783, Loss: 0.7479971647262573\n",
      "Iteration: 11784, Loss: 0.7479960322380066\n",
      "Iteration: 11785, Loss: 0.7479950189590454\n",
      "Iteration: 11786, Loss: 0.7479941248893738\n",
      "Iteration: 11787, Loss: 0.7479931116104126\n",
      "Iteration: 11788, Loss: 0.747992217540741\n",
      "Iteration: 11789, Loss: 0.747991144657135\n",
      "Iteration: 11790, Loss: 0.7479901313781738\n",
      "Iteration: 11791, Loss: 0.7479891180992126\n",
      "Iteration: 11792, Loss: 0.7479880452156067\n",
      "Iteration: 11793, Loss: 0.7479870319366455\n",
      "Iteration: 11794, Loss: 0.7479860782623291\n",
      "Iteration: 11795, Loss: 0.7479850053787231\n",
      "Iteration: 11796, Loss: 0.747983992099762\n",
      "Iteration: 11797, Loss: 0.7479829788208008\n",
      "Iteration: 11798, Loss: 0.7479819059371948\n",
      "Iteration: 11799, Loss: 0.7479808926582336\n",
      "Iteration: 11800, Loss: 0.7479798793792725\n",
      "Iteration: 11801, Loss: 0.7479788064956665\n",
      "Iteration: 11802, Loss: 0.7479777932167053\n",
      "Iteration: 11803, Loss: 0.7479767799377441\n",
      "Iteration: 11804, Loss: 0.747975766658783\n",
      "Iteration: 11805, Loss: 0.747974693775177\n",
      "Iteration: 11806, Loss: 0.7479736804962158\n",
      "Iteration: 11807, Loss: 0.7479726672172546\n",
      "Iteration: 11808, Loss: 0.7479715943336487\n",
      "Iteration: 11809, Loss: 0.747970700263977\n",
      "Iteration: 11810, Loss: 0.7479696869850159\n",
      "Iteration: 11811, Loss: 0.7479686737060547\n",
      "Iteration: 11812, Loss: 0.7479676008224487\n",
      "Iteration: 11813, Loss: 0.7479667067527771\n",
      "Iteration: 11814, Loss: 0.7479656934738159\n",
      "Iteration: 11815, Loss: 0.7479646801948547\n",
      "Iteration: 11816, Loss: 0.7479636669158936\n",
      "Iteration: 11817, Loss: 0.7479626536369324\n",
      "Iteration: 11818, Loss: 0.7479616403579712\n",
      "Iteration: 11819, Loss: 0.74796062707901\n",
      "Iteration: 11820, Loss: 0.7479596138000488\n",
      "Iteration: 11821, Loss: 0.7479586005210876\n",
      "Iteration: 11822, Loss: 0.7479575872421265\n",
      "Iteration: 11823, Loss: 0.7479565143585205\n",
      "Iteration: 11824, Loss: 0.7479555606842041\n",
      "Iteration: 11825, Loss: 0.7479545474052429\n",
      "Iteration: 11826, Loss: 0.7479535341262817\n",
      "Iteration: 11827, Loss: 0.7479525208473206\n",
      "Iteration: 11828, Loss: 0.7479515075683594\n",
      "Iteration: 11829, Loss: 0.7479504942893982\n",
      "Iteration: 11830, Loss: 0.747949481010437\n",
      "Iteration: 11831, Loss: 0.747948408126831\n",
      "Iteration: 11832, Loss: 0.7479473948478699\n",
      "Iteration: 11833, Loss: 0.7479463815689087\n",
      "Iteration: 11834, Loss: 0.7479453086853027\n",
      "Iteration: 11835, Loss: 0.7479442954063416\n",
      "Iteration: 11836, Loss: 0.7479432821273804\n",
      "Iteration: 11837, Loss: 0.7479423880577087\n",
      "Iteration: 11838, Loss: 0.7479413747787476\n",
      "Iteration: 11839, Loss: 0.7479404211044312\n",
      "Iteration: 11840, Loss: 0.74793940782547\n",
      "Iteration: 11841, Loss: 0.7479383945465088\n",
      "Iteration: 11842, Loss: 0.7479373812675476\n",
      "Iteration: 11843, Loss: 0.7479363679885864\n",
      "Iteration: 11844, Loss: 0.7479353547096252\n",
      "Iteration: 11845, Loss: 0.7479343414306641\n",
      "Iteration: 11846, Loss: 0.7479333281517029\n",
      "Iteration: 11847, Loss: 0.7479323744773865\n",
      "Iteration: 11848, Loss: 0.7479313611984253\n",
      "Iteration: 11849, Loss: 0.7479304671287537\n",
      "Iteration: 11850, Loss: 0.7479294538497925\n",
      "Iteration: 11851, Loss: 0.7479285001754761\n",
      "Iteration: 11852, Loss: 0.7479274868965149\n",
      "Iteration: 11853, Loss: 0.7479264736175537\n",
      "Iteration: 11854, Loss: 0.7479254603385925\n",
      "Iteration: 11855, Loss: 0.7479244470596313\n",
      "Iteration: 11856, Loss: 0.7479234337806702\n",
      "Iteration: 11857, Loss: 0.7479225397109985\n",
      "Iteration: 11858, Loss: 0.7479215264320374\n",
      "Iteration: 11859, Loss: 0.7479205131530762\n",
      "Iteration: 11860, Loss: 0.747919499874115\n",
      "Iteration: 11861, Loss: 0.7479184865951538\n",
      "Iteration: 11862, Loss: 0.7479174733161926\n",
      "Iteration: 11863, Loss: 0.7479165196418762\n",
      "Iteration: 11864, Loss: 0.747915506362915\n",
      "Iteration: 11865, Loss: 0.7479144930839539\n",
      "Iteration: 11866, Loss: 0.7479134798049927\n",
      "Iteration: 11867, Loss: 0.747912585735321\n",
      "Iteration: 11868, Loss: 0.7479116320610046\n",
      "Iteration: 11869, Loss: 0.7479106187820435\n",
      "Iteration: 11870, Loss: 0.747909665107727\n",
      "Iteration: 11871, Loss: 0.7479086518287659\n",
      "Iteration: 11872, Loss: 0.7479076981544495\n",
      "Iteration: 11873, Loss: 0.7479066848754883\n",
      "Iteration: 11874, Loss: 0.7479056715965271\n",
      "Iteration: 11875, Loss: 0.7479046583175659\n",
      "Iteration: 11876, Loss: 0.7479037046432495\n",
      "Iteration: 11877, Loss: 0.7479026913642883\n",
      "Iteration: 11878, Loss: 0.7479016780853271\n",
      "Iteration: 11879, Loss: 0.7479007840156555\n",
      "Iteration: 11880, Loss: 0.7478997707366943\n",
      "Iteration: 11881, Loss: 0.7478987574577332\n",
      "Iteration: 11882, Loss: 0.7478978037834167\n",
      "Iteration: 11883, Loss: 0.7478967905044556\n",
      "Iteration: 11884, Loss: 0.7478957772254944\n",
      "Iteration: 11885, Loss: 0.7478947639465332\n",
      "Iteration: 11886, Loss: 0.747893750667572\n",
      "Iteration: 11887, Loss: 0.7478927373886108\n",
      "Iteration: 11888, Loss: 0.7478917837142944\n",
      "Iteration: 11889, Loss: 0.747890830039978\n",
      "Iteration: 11890, Loss: 0.7478898763656616\n",
      "Iteration: 11891, Loss: 0.7478888630867004\n",
      "Iteration: 11892, Loss: 0.7478878498077393\n",
      "Iteration: 11893, Loss: 0.7478868961334229\n",
      "Iteration: 11894, Loss: 0.7478858828544617\n",
      "Iteration: 11895, Loss: 0.7478849291801453\n",
      "Iteration: 11896, Loss: 0.7478839159011841\n",
      "Iteration: 11897, Loss: 0.7478829622268677\n",
      "Iteration: 11898, Loss: 0.7478819489479065\n",
      "Iteration: 11899, Loss: 0.7478809952735901\n",
      "Iteration: 11900, Loss: 0.7478799819946289\n",
      "Iteration: 11901, Loss: 0.7478790283203125\n",
      "Iteration: 11902, Loss: 0.7478781342506409\n",
      "Iteration: 11903, Loss: 0.7478771805763245\n",
      "Iteration: 11904, Loss: 0.7478761672973633\n",
      "Iteration: 11905, Loss: 0.7478752732276917\n",
      "Iteration: 11906, Loss: 0.7478742599487305\n",
      "Iteration: 11907, Loss: 0.7478733062744141\n",
      "Iteration: 11908, Loss: 0.7478722929954529\n",
      "Iteration: 11909, Loss: 0.7478713393211365\n",
      "Iteration: 11910, Loss: 0.7478703856468201\n",
      "Iteration: 11911, Loss: 0.7478694319725037\n",
      "Iteration: 11912, Loss: 0.7478684186935425\n",
      "Iteration: 11913, Loss: 0.7478674650192261\n",
      "Iteration: 11914, Loss: 0.7478664517402649\n",
      "Iteration: 11915, Loss: 0.7478654980659485\n",
      "Iteration: 11916, Loss: 0.7478645443916321\n",
      "Iteration: 11917, Loss: 0.7478635311126709\n",
      "Iteration: 11918, Loss: 0.7478625774383545\n",
      "Iteration: 11919, Loss: 0.7478616833686829\n",
      "Iteration: 11920, Loss: 0.7478607296943665\n",
      "Iteration: 11921, Loss: 0.7478597164154053\n",
      "Iteration: 11922, Loss: 0.7478587627410889\n",
      "Iteration: 11923, Loss: 0.7478578090667725\n",
      "Iteration: 11924, Loss: 0.7478569149971008\n",
      "Iteration: 11925, Loss: 0.7478559613227844\n",
      "Iteration: 11926, Loss: 0.747855007648468\n",
      "Iteration: 11927, Loss: 0.7478540539741516\n",
      "Iteration: 11928, Loss: 0.7478531002998352\n",
      "Iteration: 11929, Loss: 0.747852087020874\n",
      "Iteration: 11930, Loss: 0.7478511929512024\n",
      "Iteration: 11931, Loss: 0.7478501200675964\n",
      "Iteration: 11932, Loss: 0.7478491067886353\n",
      "Iteration: 11933, Loss: 0.7478481531143188\n",
      "Iteration: 11934, Loss: 0.7478471398353577\n",
      "Iteration: 11935, Loss: 0.7478461861610413\n",
      "Iteration: 11936, Loss: 0.7478453516960144\n",
      "Iteration: 11937, Loss: 0.7478443384170532\n",
      "Iteration: 11938, Loss: 0.7478434443473816\n",
      "Iteration: 11939, Loss: 0.7478424906730652\n",
      "Iteration: 11940, Loss: 0.7478415369987488\n",
      "Iteration: 11941, Loss: 0.7478406429290771\n",
      "Iteration: 11942, Loss: 0.747839629650116\n",
      "Iteration: 11943, Loss: 0.7478386759757996\n",
      "Iteration: 11944, Loss: 0.7478377223014832\n",
      "Iteration: 11945, Loss: 0.747836709022522\n",
      "Iteration: 11946, Loss: 0.7478357553482056\n",
      "Iteration: 11947, Loss: 0.7478348016738892\n",
      "Iteration: 11948, Loss: 0.7478338479995728\n",
      "Iteration: 11949, Loss: 0.7478328943252563\n",
      "Iteration: 11950, Loss: 0.7478319406509399\n",
      "Iteration: 11951, Loss: 0.7478309869766235\n",
      "Iteration: 11952, Loss: 0.7478300929069519\n",
      "Iteration: 11953, Loss: 0.7478290796279907\n",
      "Iteration: 11954, Loss: 0.7478281259536743\n",
      "Iteration: 11955, Loss: 0.7478271722793579\n",
      "Iteration: 11956, Loss: 0.7478262186050415\n",
      "Iteration: 11957, Loss: 0.7478252649307251\n",
      "Iteration: 11958, Loss: 0.7478243112564087\n",
      "Iteration: 11959, Loss: 0.7478234171867371\n",
      "Iteration: 11960, Loss: 0.7478224635124207\n",
      "Iteration: 11961, Loss: 0.7478214502334595\n",
      "Iteration: 11962, Loss: 0.7478204965591431\n",
      "Iteration: 11963, Loss: 0.7478195428848267\n",
      "Iteration: 11964, Loss: 0.7478187084197998\n",
      "Iteration: 11965, Loss: 0.7478177547454834\n",
      "Iteration: 11966, Loss: 0.7478168606758118\n",
      "Iteration: 11967, Loss: 0.7478159070014954\n",
      "Iteration: 11968, Loss: 0.747814953327179\n",
      "Iteration: 11969, Loss: 0.7478139996528625\n",
      "Iteration: 11970, Loss: 0.7478130459785461\n",
      "Iteration: 11971, Loss: 0.7478120923042297\n",
      "Iteration: 11972, Loss: 0.7478111982345581\n",
      "Iteration: 11973, Loss: 0.7478102445602417\n",
      "Iteration: 11974, Loss: 0.7478092908859253\n",
      "Iteration: 11975, Loss: 0.7478083372116089\n",
      "Iteration: 11976, Loss: 0.7478073835372925\n",
      "Iteration: 11977, Loss: 0.7478064298629761\n",
      "Iteration: 11978, Loss: 0.7478054761886597\n",
      "Iteration: 11979, Loss: 0.7478045225143433\n",
      "Iteration: 11980, Loss: 0.7478035688400269\n",
      "Iteration: 11981, Loss: 0.7478026747703552\n",
      "Iteration: 11982, Loss: 0.7478017807006836\n",
      "Iteration: 11983, Loss: 0.7478008270263672\n",
      "Iteration: 11984, Loss: 0.7477998733520508\n",
      "Iteration: 11985, Loss: 0.7477989196777344\n",
      "Iteration: 11986, Loss: 0.747797966003418\n",
      "Iteration: 11987, Loss: 0.7477970123291016\n",
      "Iteration: 11988, Loss: 0.7477961778640747\n",
      "Iteration: 11989, Loss: 0.7477951645851135\n",
      "Iteration: 11990, Loss: 0.7477942109107971\n",
      "Iteration: 11991, Loss: 0.7477932572364807\n",
      "Iteration: 11992, Loss: 0.7477923035621643\n",
      "Iteration: 11993, Loss: 0.7477914690971375\n",
      "Iteration: 11994, Loss: 0.7477905750274658\n",
      "Iteration: 11995, Loss: 0.7477896213531494\n",
      "Iteration: 11996, Loss: 0.7477886080741882\n",
      "Iteration: 11997, Loss: 0.7477876543998718\n",
      "Iteration: 11998, Loss: 0.7477867603302002\n",
      "Iteration: 11999, Loss: 0.7477858066558838\n",
      "Iteration: 12000, Loss: 0.7477848529815674\n",
      "Iteration: 12001, Loss: 0.7477839589118958\n",
      "Iteration: 12002, Loss: 0.7477830648422241\n",
      "Iteration: 12003, Loss: 0.7477821111679077\n",
      "Iteration: 12004, Loss: 0.7477813363075256\n",
      "Iteration: 12005, Loss: 0.747780442237854\n",
      "Iteration: 12006, Loss: 0.7477794885635376\n",
      "Iteration: 12007, Loss: 0.747778594493866\n",
      "Iteration: 12008, Loss: 0.7477777004241943\n",
      "Iteration: 12009, Loss: 0.7477766275405884\n",
      "Iteration: 12010, Loss: 0.747775673866272\n",
      "Iteration: 12011, Loss: 0.7477747797966003\n",
      "Iteration: 12012, Loss: 0.7477739453315735\n",
      "Iteration: 12013, Loss: 0.7477730512619019\n",
      "Iteration: 12014, Loss: 0.7477720975875854\n",
      "Iteration: 12015, Loss: 0.7477710247039795\n",
      "Iteration: 12016, Loss: 0.7477701306343079\n",
      "Iteration: 12017, Loss: 0.7477691769599915\n",
      "Iteration: 12018, Loss: 0.7477682828903198\n",
      "Iteration: 12019, Loss: 0.7477673292160034\n",
      "Iteration: 12020, Loss: 0.747766375541687\n",
      "Iteration: 12021, Loss: 0.7477654814720154\n",
      "Iteration: 12022, Loss: 0.747764527797699\n",
      "Iteration: 12023, Loss: 0.7477636337280273\n",
      "Iteration: 12024, Loss: 0.7477626800537109\n",
      "Iteration: 12025, Loss: 0.7477619051933289\n",
      "Iteration: 12026, Loss: 0.7477609515190125\n",
      "Iteration: 12027, Loss: 0.747759997844696\n",
      "Iteration: 12028, Loss: 0.7477591037750244\n",
      "Iteration: 12029, Loss: 0.747758150100708\n",
      "Iteration: 12030, Loss: 0.7477572560310364\n",
      "Iteration: 12031, Loss: 0.74775630235672\n",
      "Iteration: 12032, Loss: 0.7477554082870483\n",
      "Iteration: 12033, Loss: 0.7477544546127319\n",
      "Iteration: 12034, Loss: 0.7477535605430603\n",
      "Iteration: 12035, Loss: 0.7477526068687439\n",
      "Iteration: 12036, Loss: 0.7477517127990723\n",
      "Iteration: 12037, Loss: 0.7477507591247559\n",
      "Iteration: 12038, Loss: 0.7477498650550842\n",
      "Iteration: 12039, Loss: 0.7477489113807678\n",
      "Iteration: 12040, Loss: 0.7477480173110962\n",
      "Iteration: 12041, Loss: 0.7477472424507141\n",
      "Iteration: 12042, Loss: 0.7477463483810425\n",
      "Iteration: 12043, Loss: 0.7477453947067261\n",
      "Iteration: 12044, Loss: 0.7477445006370544\n",
      "Iteration: 12045, Loss: 0.7477436065673828\n",
      "Iteration: 12046, Loss: 0.7477426528930664\n",
      "Iteration: 12047, Loss: 0.7477417588233948\n",
      "Iteration: 12048, Loss: 0.7477408647537231\n",
      "Iteration: 12049, Loss: 0.7477399706840515\n",
      "Iteration: 12050, Loss: 0.7477390766143799\n",
      "Iteration: 12051, Loss: 0.7477381229400635\n",
      "Iteration: 12052, Loss: 0.7477372288703918\n",
      "Iteration: 12053, Loss: 0.7477362751960754\n",
      "Iteration: 12054, Loss: 0.7477353811264038\n",
      "Iteration: 12055, Loss: 0.7477344870567322\n",
      "Iteration: 12056, Loss: 0.7477335333824158\n",
      "Iteration: 12057, Loss: 0.7477326393127441\n",
      "Iteration: 12058, Loss: 0.7477317452430725\n",
      "Iteration: 12059, Loss: 0.7477307915687561\n",
      "Iteration: 12060, Loss: 0.7477298974990845\n",
      "Iteration: 12061, Loss: 0.7477289438247681\n",
      "Iteration: 12062, Loss: 0.7477280497550964\n",
      "Iteration: 12063, Loss: 0.7477271556854248\n",
      "Iteration: 12064, Loss: 0.7477262020111084\n",
      "Iteration: 12065, Loss: 0.7477253079414368\n",
      "Iteration: 12066, Loss: 0.7477244138717651\n",
      "Iteration: 12067, Loss: 0.7477234601974487\n",
      "Iteration: 12068, Loss: 0.7477225661277771\n",
      "Iteration: 12069, Loss: 0.7477216720581055\n",
      "Iteration: 12070, Loss: 0.7477207183837891\n",
      "Iteration: 12071, Loss: 0.7477200031280518\n",
      "Iteration: 12072, Loss: 0.7477191090583801\n",
      "Iteration: 12073, Loss: 0.7477181553840637\n",
      "Iteration: 12074, Loss: 0.7477172613143921\n",
      "Iteration: 12075, Loss: 0.7477163672447205\n",
      "Iteration: 12076, Loss: 0.7477155327796936\n",
      "Iteration: 12077, Loss: 0.747714638710022\n",
      "Iteration: 12078, Loss: 0.7477137446403503\n",
      "Iteration: 12079, Loss: 0.7477128505706787\n",
      "Iteration: 12080, Loss: 0.7477118968963623\n",
      "Iteration: 12081, Loss: 0.7477110028266907\n",
      "Iteration: 12082, Loss: 0.747710108757019\n",
      "Iteration: 12083, Loss: 0.7477093935012817\n",
      "Iteration: 12084, Loss: 0.7477084398269653\n",
      "Iteration: 12085, Loss: 0.7477075457572937\n",
      "Iteration: 12086, Loss: 0.7477066516876221\n",
      "Iteration: 12087, Loss: 0.7477057576179504\n",
      "Iteration: 12088, Loss: 0.7477048635482788\n",
      "Iteration: 12089, Loss: 0.7477039098739624\n",
      "Iteration: 12090, Loss: 0.7477030158042908\n",
      "Iteration: 12091, Loss: 0.7477021217346191\n",
      "Iteration: 12092, Loss: 0.7477012276649475\n",
      "Iteration: 12093, Loss: 0.7477003335952759\n",
      "Iteration: 12094, Loss: 0.7476992607116699\n",
      "Iteration: 12095, Loss: 0.7476983666419983\n",
      "Iteration: 12096, Loss: 0.7476974725723267\n",
      "Iteration: 12097, Loss: 0.747696578502655\n",
      "Iteration: 12098, Loss: 0.7476957440376282\n",
      "Iteration: 12099, Loss: 0.7476948499679565\n",
      "Iteration: 12100, Loss: 0.7476939558982849\n",
      "Iteration: 12101, Loss: 0.7476930022239685\n",
      "Iteration: 12102, Loss: 0.7476921081542969\n",
      "Iteration: 12103, Loss: 0.7476912140846252\n",
      "Iteration: 12104, Loss: 0.7476903796195984\n",
      "Iteration: 12105, Loss: 0.7476894855499268\n",
      "Iteration: 12106, Loss: 0.7476885318756104\n",
      "Iteration: 12107, Loss: 0.7476876378059387\n",
      "Iteration: 12108, Loss: 0.7476867437362671\n",
      "Iteration: 12109, Loss: 0.7476858496665955\n",
      "Iteration: 12110, Loss: 0.7476849555969238\n",
      "Iteration: 12111, Loss: 0.747684121131897\n",
      "Iteration: 12112, Loss: 0.7476832270622253\n",
      "Iteration: 12113, Loss: 0.7476824522018433\n",
      "Iteration: 12114, Loss: 0.7476815581321716\n",
      "Iteration: 12115, Loss: 0.7476806640625\n",
      "Iteration: 12116, Loss: 0.7476797699928284\n",
      "Iteration: 12117, Loss: 0.7476788759231567\n",
      "Iteration: 12118, Loss: 0.7476779818534851\n",
      "Iteration: 12119, Loss: 0.7476771473884583\n",
      "Iteration: 12120, Loss: 0.7476762533187866\n",
      "Iteration: 12121, Loss: 0.747675359249115\n",
      "Iteration: 12122, Loss: 0.7476744651794434\n",
      "Iteration: 12123, Loss: 0.7476735711097717\n",
      "Iteration: 12124, Loss: 0.7476726770401001\n",
      "Iteration: 12125, Loss: 0.7476717829704285\n",
      "Iteration: 12126, Loss: 0.7476708889007568\n",
      "Iteration: 12127, Loss: 0.74767005443573\n",
      "Iteration: 12128, Loss: 0.7476691603660583\n",
      "Iteration: 12129, Loss: 0.7476682662963867\n",
      "Iteration: 12130, Loss: 0.7476674318313599\n",
      "Iteration: 12131, Loss: 0.7476665377616882\n",
      "Iteration: 12132, Loss: 0.7476656436920166\n",
      "Iteration: 12133, Loss: 0.747664749622345\n",
      "Iteration: 12134, Loss: 0.7476638555526733\n",
      "Iteration: 12135, Loss: 0.7476629614830017\n",
      "Iteration: 12136, Loss: 0.7476620674133301\n",
      "Iteration: 12137, Loss: 0.7476615309715271\n",
      "Iteration: 12138, Loss: 0.7476606965065002\n",
      "Iteration: 12139, Loss: 0.7476598024368286\n",
      "Iteration: 12140, Loss: 0.747658908367157\n",
      "Iteration: 12141, Loss: 0.7476580142974854\n",
      "Iteration: 12142, Loss: 0.7476571202278137\n",
      "Iteration: 12143, Loss: 0.7476562857627869\n",
      "Iteration: 12144, Loss: 0.7476553916931152\n",
      "Iteration: 12145, Loss: 0.7476544976234436\n",
      "Iteration: 12146, Loss: 0.747653603553772\n",
      "Iteration: 12147, Loss: 0.7476527690887451\n",
      "Iteration: 12148, Loss: 0.7476518750190735\n",
      "Iteration: 12149, Loss: 0.7476509809494019\n",
      "Iteration: 12150, Loss: 0.747650146484375\n",
      "Iteration: 12151, Loss: 0.7476493120193481\n",
      "Iteration: 12152, Loss: 0.7476484179496765\n",
      "Iteration: 12153, Loss: 0.7476475834846497\n",
      "Iteration: 12154, Loss: 0.747646689414978\n",
      "Iteration: 12155, Loss: 0.7476457953453064\n",
      "Iteration: 12156, Loss: 0.7476449608802795\n",
      "Iteration: 12157, Loss: 0.7476441860198975\n",
      "Iteration: 12158, Loss: 0.7476432919502258\n",
      "Iteration: 12159, Loss: 0.747642457485199\n",
      "Iteration: 12160, Loss: 0.7476415634155273\n",
      "Iteration: 12161, Loss: 0.7476406693458557\n",
      "Iteration: 12162, Loss: 0.7476398348808289\n",
      "Iteration: 12163, Loss: 0.7476389408111572\n",
      "Iteration: 12164, Loss: 0.7476380467414856\n",
      "Iteration: 12165, Loss: 0.7476372122764587\n",
      "Iteration: 12166, Loss: 0.7476363182067871\n",
      "Iteration: 12167, Loss: 0.7476354241371155\n",
      "Iteration: 12168, Loss: 0.7476345896720886\n",
      "Iteration: 12169, Loss: 0.747633695602417\n",
      "Iteration: 12170, Loss: 0.7476328015327454\n",
      "Iteration: 12171, Loss: 0.7476319670677185\n",
      "Iteration: 12172, Loss: 0.7476310729980469\n",
      "Iteration: 12173, Loss: 0.74763023853302\n",
      "Iteration: 12174, Loss: 0.7476293444633484\n",
      "Iteration: 12175, Loss: 0.7476284503936768\n",
      "Iteration: 12176, Loss: 0.7476277947425842\n",
      "Iteration: 12177, Loss: 0.7476269602775574\n",
      "Iteration: 12178, Loss: 0.7476260662078857\n",
      "Iteration: 12179, Loss: 0.7476252317428589\n",
      "Iteration: 12180, Loss: 0.7476243376731873\n",
      "Iteration: 12181, Loss: 0.7476235032081604\n",
      "Iteration: 12182, Loss: 0.7476226091384888\n",
      "Iteration: 12183, Loss: 0.7476217746734619\n",
      "Iteration: 12184, Loss: 0.7476208806037903\n",
      "Iteration: 12185, Loss: 0.7476201057434082\n",
      "Iteration: 12186, Loss: 0.7476192116737366\n",
      "Iteration: 12187, Loss: 0.7476183772087097\n",
      "Iteration: 12188, Loss: 0.7476174831390381\n",
      "Iteration: 12189, Loss: 0.7476165890693665\n",
      "Iteration: 12190, Loss: 0.7476156949996948\n",
      "Iteration: 12191, Loss: 0.747614860534668\n",
      "Iteration: 12192, Loss: 0.7476140260696411\n",
      "Iteration: 12193, Loss: 0.7476131319999695\n",
      "Iteration: 12194, Loss: 0.7476122975349426\n",
      "Iteration: 12195, Loss: 0.747611403465271\n",
      "Iteration: 12196, Loss: 0.7476105690002441\n",
      "Iteration: 12197, Loss: 0.7476097345352173\n",
      "Iteration: 12198, Loss: 0.7476089000701904\n",
      "Iteration: 12199, Loss: 0.7476080656051636\n",
      "Iteration: 12200, Loss: 0.7476071715354919\n",
      "Iteration: 12201, Loss: 0.7476063370704651\n",
      "Iteration: 12202, Loss: 0.7476054430007935\n",
      "Iteration: 12203, Loss: 0.7476046085357666\n",
      "Iteration: 12204, Loss: 0.7476037740707397\n",
      "Iteration: 12205, Loss: 0.7476028800010681\n",
      "Iteration: 12206, Loss: 0.7476020455360413\n",
      "Iteration: 12207, Loss: 0.7476011514663696\n",
      "Iteration: 12208, Loss: 0.7476003170013428\n",
      "Iteration: 12209, Loss: 0.7475994825363159\n",
      "Iteration: 12210, Loss: 0.7475985884666443\n",
      "Iteration: 12211, Loss: 0.7475977540016174\n",
      "Iteration: 12212, Loss: 0.7475969195365906\n",
      "Iteration: 12213, Loss: 0.747596025466919\n",
      "Iteration: 12214, Loss: 0.7475951910018921\n",
      "Iteration: 12215, Loss: 0.7475943565368652\n",
      "Iteration: 12216, Loss: 0.7475935220718384\n",
      "Iteration: 12217, Loss: 0.7475926876068115\n",
      "Iteration: 12218, Loss: 0.7475917935371399\n",
      "Iteration: 12219, Loss: 0.747590959072113\n",
      "Iteration: 12220, Loss: 0.7475901246070862\n",
      "Iteration: 12221, Loss: 0.7475892305374146\n",
      "Iteration: 12222, Loss: 0.7475883960723877\n",
      "Iteration: 12223, Loss: 0.7475875616073608\n",
      "Iteration: 12224, Loss: 0.7475866675376892\n",
      "Iteration: 12225, Loss: 0.7475858330726624\n",
      "Iteration: 12226, Loss: 0.747585117816925\n",
      "Iteration: 12227, Loss: 0.7475842237472534\n",
      "Iteration: 12228, Loss: 0.7475833892822266\n",
      "Iteration: 12229, Loss: 0.7475825548171997\n",
      "Iteration: 12230, Loss: 0.7475816607475281\n",
      "Iteration: 12231, Loss: 0.7475808262825012\n",
      "Iteration: 12232, Loss: 0.7475799918174744\n",
      "Iteration: 12233, Loss: 0.7475791573524475\n",
      "Iteration: 12234, Loss: 0.7475783228874207\n",
      "Iteration: 12235, Loss: 0.7475774884223938\n",
      "Iteration: 12236, Loss: 0.7475766539573669\n",
      "Iteration: 12237, Loss: 0.7475757598876953\n",
      "Iteration: 12238, Loss: 0.7475749254226685\n",
      "Iteration: 12239, Loss: 0.7475740909576416\n",
      "Iteration: 12240, Loss: 0.74757319688797\n",
      "Iteration: 12241, Loss: 0.7475723624229431\n",
      "Iteration: 12242, Loss: 0.7475716471672058\n",
      "Iteration: 12243, Loss: 0.747570812702179\n",
      "Iteration: 12244, Loss: 0.7475699782371521\n",
      "Iteration: 12245, Loss: 0.7475690841674805\n",
      "Iteration: 12246, Loss: 0.7475682497024536\n",
      "Iteration: 12247, Loss: 0.7475674152374268\n",
      "Iteration: 12248, Loss: 0.7475665807723999\n",
      "Iteration: 12249, Loss: 0.747565746307373\n",
      "Iteration: 12250, Loss: 0.7475649118423462\n",
      "Iteration: 12251, Loss: 0.7475640177726746\n",
      "Iteration: 12252, Loss: 0.7475632429122925\n",
      "Iteration: 12253, Loss: 0.7475624084472656\n",
      "Iteration: 12254, Loss: 0.7475615739822388\n",
      "Iteration: 12255, Loss: 0.7475607395172119\n",
      "Iteration: 12256, Loss: 0.7475599050521851\n",
      "Iteration: 12257, Loss: 0.7475590705871582\n",
      "Iteration: 12258, Loss: 0.7475582361221313\n",
      "Iteration: 12259, Loss: 0.7475574612617493\n",
      "Iteration: 12260, Loss: 0.7475566267967224\n",
      "Iteration: 12261, Loss: 0.7475557923316956\n",
      "Iteration: 12262, Loss: 0.7475549578666687\n",
      "Iteration: 12263, Loss: 0.7475541234016418\n",
      "Iteration: 12264, Loss: 0.747553288936615\n",
      "Iteration: 12265, Loss: 0.7475524544715881\n",
      "Iteration: 12266, Loss: 0.7475516200065613\n",
      "Iteration: 12267, Loss: 0.7475509643554688\n",
      "Iteration: 12268, Loss: 0.7475501298904419\n",
      "Iteration: 12269, Loss: 0.7475494146347046\n",
      "Iteration: 12270, Loss: 0.7475485801696777\n",
      "Iteration: 12271, Loss: 0.7475477457046509\n",
      "Iteration: 12272, Loss: 0.7475468516349792\n",
      "Iteration: 12273, Loss: 0.7475460171699524\n",
      "Iteration: 12274, Loss: 0.7475451827049255\n",
      "Iteration: 12275, Loss: 0.7475443482398987\n",
      "Iteration: 12276, Loss: 0.7475435137748718\n",
      "Iteration: 12277, Loss: 0.747542679309845\n",
      "Iteration: 12278, Loss: 0.7475419044494629\n",
      "Iteration: 12279, Loss: 0.747541069984436\n",
      "Iteration: 12280, Loss: 0.7475402355194092\n",
      "Iteration: 12281, Loss: 0.7475394010543823\n",
      "Iteration: 12282, Loss: 0.7475385665893555\n",
      "Iteration: 12283, Loss: 0.7475377917289734\n",
      "Iteration: 12284, Loss: 0.7475369572639465\n",
      "Iteration: 12285, Loss: 0.7475361227989197\n",
      "Iteration: 12286, Loss: 0.7475354075431824\n",
      "Iteration: 12287, Loss: 0.7475346326828003\n",
      "Iteration: 12288, Loss: 0.7475337982177734\n",
      "Iteration: 12289, Loss: 0.7475329637527466\n",
      "Iteration: 12290, Loss: 0.7475321292877197\n",
      "Iteration: 12291, Loss: 0.7475312948226929\n",
      "Iteration: 12292, Loss: 0.7475305199623108\n",
      "Iteration: 12293, Loss: 0.7475296854972839\n",
      "Iteration: 12294, Loss: 0.7475288510322571\n",
      "Iteration: 12295, Loss: 0.7475280165672302\n",
      "Iteration: 12296, Loss: 0.7475272417068481\n",
      "Iteration: 12297, Loss: 0.7475264072418213\n",
      "Iteration: 12298, Loss: 0.7475255727767944\n",
      "Iteration: 12299, Loss: 0.7475247383117676\n",
      "Iteration: 12300, Loss: 0.7475239038467407\n",
      "Iteration: 12301, Loss: 0.7475231289863586\n",
      "Iteration: 12302, Loss: 0.7475222945213318\n",
      "Iteration: 12303, Loss: 0.7475214600563049\n",
      "Iteration: 12304, Loss: 0.7475206255912781\n",
      "Iteration: 12305, Loss: 0.7475199103355408\n",
      "Iteration: 12306, Loss: 0.7475190758705139\n",
      "Iteration: 12307, Loss: 0.7475182414054871\n",
      "Iteration: 12308, Loss: 0.7475174069404602\n",
      "Iteration: 12309, Loss: 0.7475166320800781\n",
      "Iteration: 12310, Loss: 0.7475157976150513\n",
      "Iteration: 12311, Loss: 0.7475149631500244\n",
      "Iteration: 12312, Loss: 0.7475142478942871\n",
      "Iteration: 12313, Loss: 0.7475134134292603\n",
      "Iteration: 12314, Loss: 0.7475125789642334\n",
      "Iteration: 12315, Loss: 0.7475118041038513\n",
      "Iteration: 12316, Loss: 0.7475109696388245\n",
      "Iteration: 12317, Loss: 0.7475101351737976\n",
      "Iteration: 12318, Loss: 0.7475093603134155\n",
      "Iteration: 12319, Loss: 0.7475085258483887\n",
      "Iteration: 12320, Loss: 0.7475077509880066\n",
      "Iteration: 12321, Loss: 0.7475069761276245\n",
      "Iteration: 12322, Loss: 0.7475061416625977\n",
      "Iteration: 12323, Loss: 0.7475053668022156\n",
      "Iteration: 12324, Loss: 0.7475045323371887\n",
      "Iteration: 12325, Loss: 0.7475036978721619\n",
      "Iteration: 12326, Loss: 0.7475029230117798\n",
      "Iteration: 12327, Loss: 0.7475020885467529\n",
      "Iteration: 12328, Loss: 0.7475012540817261\n",
      "Iteration: 12329, Loss: 0.747500479221344\n",
      "Iteration: 12330, Loss: 0.7474996447563171\n",
      "Iteration: 12331, Loss: 0.7474988698959351\n",
      "Iteration: 12332, Loss: 0.7474980354309082\n",
      "Iteration: 12333, Loss: 0.7474972009658813\n",
      "Iteration: 12334, Loss: 0.7474964261054993\n",
      "Iteration: 12335, Loss: 0.7474955916404724\n",
      "Iteration: 12336, Loss: 0.7474947571754456\n",
      "Iteration: 12337, Loss: 0.7474939823150635\n",
      "Iteration: 12338, Loss: 0.7474931478500366\n",
      "Iteration: 12339, Loss: 0.7474923729896545\n",
      "Iteration: 12340, Loss: 0.7474915385246277\n",
      "Iteration: 12341, Loss: 0.7474907040596008\n",
      "Iteration: 12342, Loss: 0.7474899291992188\n",
      "Iteration: 12343, Loss: 0.7474890947341919\n",
      "Iteration: 12344, Loss: 0.7474883794784546\n",
      "Iteration: 12345, Loss: 0.7474876046180725\n",
      "Iteration: 12346, Loss: 0.7474867701530457\n",
      "Iteration: 12347, Loss: 0.7474859952926636\n",
      "Iteration: 12348, Loss: 0.7474851608276367\n",
      "Iteration: 12349, Loss: 0.7474843263626099\n",
      "Iteration: 12350, Loss: 0.7474835515022278\n",
      "Iteration: 12351, Loss: 0.7474827170372009\n",
      "Iteration: 12352, Loss: 0.7474819421768188\n",
      "Iteration: 12353, Loss: 0.747481107711792\n",
      "Iteration: 12354, Loss: 0.7474802732467651\n",
      "Iteration: 12355, Loss: 0.7474795579910278\n",
      "Iteration: 12356, Loss: 0.747478723526001\n",
      "Iteration: 12357, Loss: 0.7474779486656189\n",
      "Iteration: 12358, Loss: 0.747477114200592\n",
      "Iteration: 12359, Loss: 0.7474762797355652\n",
      "Iteration: 12360, Loss: 0.7474755048751831\n",
      "Iteration: 12361, Loss: 0.7474746704101562\n",
      "Iteration: 12362, Loss: 0.7474738955497742\n",
      "Iteration: 12363, Loss: 0.7474730610847473\n",
      "Iteration: 12364, Loss: 0.7474722862243652\n",
      "Iteration: 12365, Loss: 0.7474714517593384\n",
      "Iteration: 12366, Loss: 0.7474706768989563\n",
      "Iteration: 12367, Loss: 0.747469961643219\n",
      "Iteration: 12368, Loss: 0.7474691867828369\n",
      "Iteration: 12369, Loss: 0.7474683523178101\n",
      "Iteration: 12370, Loss: 0.747467577457428\n",
      "Iteration: 12371, Loss: 0.7474667429924011\n",
      "Iteration: 12372, Loss: 0.747465968132019\n",
      "Iteration: 12373, Loss: 0.747465193271637\n",
      "Iteration: 12374, Loss: 0.7474643588066101\n",
      "Iteration: 12375, Loss: 0.747463583946228\n",
      "Iteration: 12376, Loss: 0.7474627494812012\n",
      "Iteration: 12377, Loss: 0.7474619746208191\n",
      "Iteration: 12378, Loss: 0.747461199760437\n",
      "Iteration: 12379, Loss: 0.7474603652954102\n",
      "Iteration: 12380, Loss: 0.7474595904350281\n",
      "Iteration: 12381, Loss: 0.7474587559700012\n",
      "Iteration: 12382, Loss: 0.7474579811096191\n",
      "Iteration: 12383, Loss: 0.7474572062492371\n",
      "Iteration: 12384, Loss: 0.7474563717842102\n",
      "Iteration: 12385, Loss: 0.7474555969238281\n",
      "Iteration: 12386, Loss: 0.747454822063446\n",
      "Iteration: 12387, Loss: 0.7474539875984192\n",
      "Iteration: 12388, Loss: 0.7474532127380371\n",
      "Iteration: 12389, Loss: 0.7474526166915894\n",
      "Iteration: 12390, Loss: 0.7474517822265625\n",
      "Iteration: 12391, Loss: 0.7474510073661804\n",
      "Iteration: 12392, Loss: 0.7474502325057983\n",
      "Iteration: 12393, Loss: 0.7474493980407715\n",
      "Iteration: 12394, Loss: 0.7474486231803894\n",
      "Iteration: 12395, Loss: 0.7474478483200073\n",
      "Iteration: 12396, Loss: 0.7474470138549805\n",
      "Iteration: 12397, Loss: 0.7474462389945984\n",
      "Iteration: 12398, Loss: 0.7474454641342163\n",
      "Iteration: 12399, Loss: 0.747444748878479\n",
      "Iteration: 12400, Loss: 0.7474439144134521\n",
      "Iteration: 12401, Loss: 0.7474431395530701\n",
      "Iteration: 12402, Loss: 0.747442364692688\n",
      "Iteration: 12403, Loss: 0.7474415302276611\n",
      "Iteration: 12404, Loss: 0.747440755367279\n",
      "Iteration: 12405, Loss: 0.747439980506897\n",
      "Iteration: 12406, Loss: 0.7474392056465149\n",
      "Iteration: 12407, Loss: 0.7474384903907776\n",
      "Iteration: 12408, Loss: 0.7474376559257507\n",
      "Iteration: 12409, Loss: 0.7474368810653687\n",
      "Iteration: 12410, Loss: 0.7474361062049866\n",
      "Iteration: 12411, Loss: 0.7474353909492493\n",
      "Iteration: 12412, Loss: 0.7474345564842224\n",
      "Iteration: 12413, Loss: 0.7474337816238403\n",
      "Iteration: 12414, Loss: 0.7474330067634583\n",
      "Iteration: 12415, Loss: 0.7474322319030762\n",
      "Iteration: 12416, Loss: 0.7474314570426941\n",
      "Iteration: 12417, Loss: 0.7474306225776672\n",
      "Iteration: 12418, Loss: 0.7474299669265747\n",
      "Iteration: 12419, Loss: 0.7474291920661926\n",
      "Iteration: 12420, Loss: 0.7474284768104553\n",
      "Iteration: 12421, Loss: 0.7474277019500732\n",
      "Iteration: 12422, Loss: 0.7474269270896912\n",
      "Iteration: 12423, Loss: 0.7474261522293091\n",
      "Iteration: 12424, Loss: 0.747425377368927\n",
      "Iteration: 12425, Loss: 0.7474246025085449\n",
      "Iteration: 12426, Loss: 0.7474237680435181\n",
      "Iteration: 12427, Loss: 0.747422993183136\n",
      "Iteration: 12428, Loss: 0.7474222183227539\n",
      "Iteration: 12429, Loss: 0.7474214434623718\n",
      "Iteration: 12430, Loss: 0.7474206686019897\n",
      "Iteration: 12431, Loss: 0.7474197745323181\n",
      "Iteration: 12432, Loss: 0.7474191784858704\n",
      "Iteration: 12433, Loss: 0.7474184036254883\n",
      "Iteration: 12434, Loss: 0.7474176287651062\n",
      "Iteration: 12435, Loss: 0.7474168539047241\n",
      "Iteration: 12436, Loss: 0.747416079044342\n",
      "Iteration: 12437, Loss: 0.74741530418396\n",
      "Iteration: 12438, Loss: 0.7474145293235779\n",
      "Iteration: 12439, Loss: 0.7474137544631958\n",
      "Iteration: 12440, Loss: 0.7474129796028137\n",
      "Iteration: 12441, Loss: 0.7474122047424316\n",
      "Iteration: 12442, Loss: 0.7474114298820496\n",
      "Iteration: 12443, Loss: 0.7474106550216675\n",
      "Iteration: 12444, Loss: 0.7474098801612854\n",
      "Iteration: 12445, Loss: 0.7474091053009033\n",
      "Iteration: 12446, Loss: 0.7474083304405212\n",
      "Iteration: 12447, Loss: 0.7474075555801392\n",
      "Iteration: 12448, Loss: 0.7474067807197571\n",
      "Iteration: 12449, Loss: 0.7474058866500854\n",
      "Iteration: 12450, Loss: 0.7474051117897034\n",
      "Iteration: 12451, Loss: 0.7474043369293213\n",
      "Iteration: 12452, Loss: 0.7474035620689392\n",
      "Iteration: 12453, Loss: 0.7474027872085571\n",
      "Iteration: 12454, Loss: 0.7474021315574646\n",
      "Iteration: 12455, Loss: 0.7474013566970825\n",
      "Iteration: 12456, Loss: 0.7474006414413452\n",
      "Iteration: 12457, Loss: 0.7473998665809631\n",
      "Iteration: 12458, Loss: 0.747399091720581\n",
      "Iteration: 12459, Loss: 0.747398316860199\n",
      "Iteration: 12460, Loss: 0.7473975419998169\n",
      "Iteration: 12461, Loss: 0.7473967671394348\n",
      "Iteration: 12462, Loss: 0.7473959922790527\n",
      "Iteration: 12463, Loss: 0.7473952174186707\n",
      "Iteration: 12464, Loss: 0.7473944425582886\n",
      "Iteration: 12465, Loss: 0.7473936676979065\n",
      "Iteration: 12466, Loss: 0.7473927736282349\n",
      "Iteration: 12467, Loss: 0.7473919987678528\n",
      "Iteration: 12468, Loss: 0.7473912239074707\n",
      "Iteration: 12469, Loss: 0.7473904490470886\n",
      "Iteration: 12470, Loss: 0.7473899126052856\n",
      "Iteration: 12471, Loss: 0.7473891973495483\n",
      "Iteration: 12472, Loss: 0.7473884224891663\n",
      "Iteration: 12473, Loss: 0.7473876476287842\n",
      "Iteration: 12474, Loss: 0.7473871111869812\n",
      "Iteration: 12475, Loss: 0.7473863363265991\n",
      "Iteration: 12476, Loss: 0.7473856210708618\n",
      "Iteration: 12477, Loss: 0.7473848462104797\n",
      "Iteration: 12478, Loss: 0.7473841309547424\n",
      "Iteration: 12479, Loss: 0.7473832964897156\n",
      "Iteration: 12480, Loss: 0.7473825812339783\n",
      "Iteration: 12481, Loss: 0.7473818063735962\n",
      "Iteration: 12482, Loss: 0.7473810911178589\n",
      "Iteration: 12483, Loss: 0.7473803162574768\n",
      "Iteration: 12484, Loss: 0.7473795413970947\n",
      "Iteration: 12485, Loss: 0.7473788857460022\n",
      "Iteration: 12486, Loss: 0.7473781108856201\n",
      "Iteration: 12487, Loss: 0.747377336025238\n",
      "Iteration: 12488, Loss: 0.7473766803741455\n",
      "Iteration: 12489, Loss: 0.7473758459091187\n",
      "Iteration: 12490, Loss: 0.7473751306533813\n",
      "Iteration: 12491, Loss: 0.747374415397644\n",
      "Iteration: 12492, Loss: 0.7473735809326172\n",
      "Iteration: 12493, Loss: 0.7473730444908142\n",
      "Iteration: 12494, Loss: 0.7473722696304321\n",
      "Iteration: 12495, Loss: 0.74737149477005\n",
      "Iteration: 12496, Loss: 0.7473707795143127\n",
      "Iteration: 12497, Loss: 0.7473700046539307\n",
      "Iteration: 12498, Loss: 0.7473692297935486\n",
      "Iteration: 12499, Loss: 0.7473685145378113\n",
      "Iteration: 12500, Loss: 0.7473677396774292\n",
      "Iteration: 12501, Loss: 0.7473669052124023\n",
      "Iteration: 12502, Loss: 0.747366189956665\n",
      "Iteration: 12503, Loss: 0.7473654747009277\n",
      "Iteration: 12504, Loss: 0.7473646998405457\n",
      "Iteration: 12505, Loss: 0.7473639845848083\n",
      "Iteration: 12506, Loss: 0.747363269329071\n",
      "Iteration: 12507, Loss: 0.747362494468689\n",
      "Iteration: 12508, Loss: 0.7473617792129517\n",
      "Iteration: 12509, Loss: 0.7473609447479248\n",
      "Iteration: 12510, Loss: 0.7473602294921875\n",
      "Iteration: 12511, Loss: 0.7473594546318054\n",
      "Iteration: 12512, Loss: 0.7473587393760681\n",
      "Iteration: 12513, Loss: 0.747357964515686\n",
      "Iteration: 12514, Loss: 0.747357189655304\n",
      "Iteration: 12515, Loss: 0.7473564743995667\n",
      "Iteration: 12516, Loss: 0.7473556995391846\n",
      "Iteration: 12517, Loss: 0.7473549842834473\n",
      "Iteration: 12518, Loss: 0.7473542094230652\n",
      "Iteration: 12519, Loss: 0.7473534941673279\n",
      "Iteration: 12520, Loss: 0.7473527193069458\n",
      "Iteration: 12521, Loss: 0.7473520040512085\n",
      "Iteration: 12522, Loss: 0.7473511695861816\n",
      "Iteration: 12523, Loss: 0.7473504543304443\n",
      "Iteration: 12524, Loss: 0.7473496794700623\n",
      "Iteration: 12525, Loss: 0.7473489046096802\n",
      "Iteration: 12526, Loss: 0.7473482489585876\n",
      "Iteration: 12527, Loss: 0.7473474740982056\n",
      "Iteration: 12528, Loss: 0.7473467588424683\n",
      "Iteration: 12529, Loss: 0.7473459243774414\n",
      "Iteration: 12530, Loss: 0.7473451495170593\n",
      "Iteration: 12531, Loss: 0.7473446130752563\n",
      "Iteration: 12532, Loss: 0.7473438382148743\n",
      "Iteration: 12533, Loss: 0.747343122959137\n",
      "Iteration: 12534, Loss: 0.7473423480987549\n",
      "Iteration: 12535, Loss: 0.7473416328430176\n",
      "Iteration: 12536, Loss: 0.7473408579826355\n",
      "Iteration: 12537, Loss: 0.7473400831222534\n",
      "Iteration: 12538, Loss: 0.7473394274711609\n",
      "Iteration: 12539, Loss: 0.7473386526107788\n",
      "Iteration: 12540, Loss: 0.7473379373550415\n",
      "Iteration: 12541, Loss: 0.7473371624946594\n",
      "Iteration: 12542, Loss: 0.7473363280296326\n",
      "Iteration: 12543, Loss: 0.7473356127738953\n",
      "Iteration: 12544, Loss: 0.7473349571228027\n",
      "Iteration: 12545, Loss: 0.7473342418670654\n",
      "Iteration: 12546, Loss: 0.7473334670066833\n",
      "Iteration: 12547, Loss: 0.747332751750946\n",
      "Iteration: 12548, Loss: 0.747331976890564\n",
      "Iteration: 12549, Loss: 0.7473312616348267\n",
      "Iteration: 12550, Loss: 0.7473304867744446\n",
      "Iteration: 12551, Loss: 0.7473297715187073\n",
      "Iteration: 12552, Loss: 0.7473289966583252\n",
      "Iteration: 12553, Loss: 0.7473282814025879\n",
      "Iteration: 12554, Loss: 0.7473275065422058\n",
      "Iteration: 12555, Loss: 0.7473267912864685\n",
      "Iteration: 12556, Loss: 0.7473260760307312\n",
      "Iteration: 12557, Loss: 0.7473253011703491\n",
      "Iteration: 12558, Loss: 0.7473245859146118\n",
      "Iteration: 12559, Loss: 0.7473238110542297\n",
      "Iteration: 12560, Loss: 0.747323215007782\n",
      "Iteration: 12561, Loss: 0.7473224997520447\n",
      "Iteration: 12562, Loss: 0.7473217248916626\n",
      "Iteration: 12563, Loss: 0.7473210096359253\n",
      "Iteration: 12564, Loss: 0.7473202347755432\n",
      "Iteration: 12565, Loss: 0.7473194599151611\n",
      "Iteration: 12566, Loss: 0.7473187446594238\n",
      "Iteration: 12567, Loss: 0.7473180294036865\n",
      "Iteration: 12568, Loss: 0.7473172545433044\n",
      "Iteration: 12569, Loss: 0.7473165392875671\n",
      "Iteration: 12570, Loss: 0.7473158240318298\n",
      "Iteration: 12571, Loss: 0.7473151087760925\n",
      "Iteration: 12572, Loss: 0.7473143339157104\n",
      "Iteration: 12573, Loss: 0.7473136186599731\n",
      "Iteration: 12574, Loss: 0.7473130822181702\n",
      "Iteration: 12575, Loss: 0.7473123669624329\n",
      "Iteration: 12576, Loss: 0.7473115921020508\n",
      "Iteration: 12577, Loss: 0.7473108768463135\n",
      "Iteration: 12578, Loss: 0.7473101615905762\n",
      "Iteration: 12579, Loss: 0.7473094463348389\n",
      "Iteration: 12580, Loss: 0.7473087310791016\n",
      "Iteration: 12581, Loss: 0.7473080158233643\n",
      "Iteration: 12582, Loss: 0.7473072409629822\n",
      "Iteration: 12583, Loss: 0.7473065853118896\n",
      "Iteration: 12584, Loss: 0.7473058700561523\n",
      "Iteration: 12585, Loss: 0.7473049759864807\n",
      "Iteration: 12586, Loss: 0.7473042607307434\n",
      "Iteration: 12587, Loss: 0.7473035454750061\n",
      "Iteration: 12588, Loss: 0.7473028302192688\n",
      "Iteration: 12589, Loss: 0.7473021149635315\n",
      "Iteration: 12590, Loss: 0.7473013401031494\n",
      "Iteration: 12591, Loss: 0.7473006844520569\n",
      "Iteration: 12592, Loss: 0.7472999691963196\n",
      "Iteration: 12593, Loss: 0.7472992539405823\n",
      "Iteration: 12594, Loss: 0.747298538684845\n",
      "Iteration: 12595, Loss: 0.7472978234291077\n",
      "Iteration: 12596, Loss: 0.7472971081733704\n",
      "Iteration: 12597, Loss: 0.7472965121269226\n",
      "Iteration: 12598, Loss: 0.7472957968711853\n",
      "Iteration: 12599, Loss: 0.747295081615448\n",
      "Iteration: 12600, Loss: 0.7472943067550659\n",
      "Iteration: 12601, Loss: 0.7472935914993286\n",
      "Iteration: 12602, Loss: 0.7472928166389465\n",
      "Iteration: 12603, Loss: 0.7472921013832092\n",
      "Iteration: 12604, Loss: 0.7472913861274719\n",
      "Iteration: 12605, Loss: 0.7472907304763794\n",
      "Iteration: 12606, Loss: 0.7472900152206421\n",
      "Iteration: 12607, Loss: 0.7472892999649048\n",
      "Iteration: 12608, Loss: 0.7472884654998779\n",
      "Iteration: 12609, Loss: 0.7472877502441406\n",
      "Iteration: 12610, Loss: 0.7472870945930481\n",
      "Iteration: 12611, Loss: 0.7472863793373108\n",
      "Iteration: 12612, Loss: 0.7472856044769287\n",
      "Iteration: 12613, Loss: 0.7472848892211914\n",
      "Iteration: 12614, Loss: 0.7472841739654541\n",
      "Iteration: 12615, Loss: 0.7472834587097168\n",
      "Iteration: 12616, Loss: 0.7472827434539795\n",
      "Iteration: 12617, Loss: 0.7472820281982422\n",
      "Iteration: 12618, Loss: 0.7472813129425049\n",
      "Iteration: 12619, Loss: 0.7472805380821228\n",
      "Iteration: 12620, Loss: 0.7472798228263855\n",
      "Iteration: 12621, Loss: 0.7472792267799377\n",
      "Iteration: 12622, Loss: 0.7472783923149109\n",
      "Iteration: 12623, Loss: 0.7472777962684631\n",
      "Iteration: 12624, Loss: 0.7472770810127258\n",
      "Iteration: 12625, Loss: 0.7472763657569885\n",
      "Iteration: 12626, Loss: 0.7472756505012512\n",
      "Iteration: 12627, Loss: 0.7472749352455139\n",
      "Iteration: 12628, Loss: 0.7472742199897766\n",
      "Iteration: 12629, Loss: 0.7472735643386841\n",
      "Iteration: 12630, Loss: 0.7472728490829468\n",
      "Iteration: 12631, Loss: 0.7472720742225647\n",
      "Iteration: 12632, Loss: 0.7472713589668274\n",
      "Iteration: 12633, Loss: 0.7472706437110901\n",
      "Iteration: 12634, Loss: 0.7472699284553528\n",
      "Iteration: 12635, Loss: 0.7472692728042603\n",
      "Iteration: 12636, Loss: 0.7472684979438782\n",
      "Iteration: 12637, Loss: 0.7472677826881409\n",
      "Iteration: 12638, Loss: 0.7472671270370483\n",
      "Iteration: 12639, Loss: 0.7472664713859558\n",
      "Iteration: 12640, Loss: 0.7472657561302185\n",
      "Iteration: 12641, Loss: 0.7472650408744812\n",
      "Iteration: 12642, Loss: 0.7472643256187439\n",
      "Iteration: 12643, Loss: 0.7472636103630066\n",
      "Iteration: 12644, Loss: 0.7472628951072693\n",
      "Iteration: 12645, Loss: 0.747262179851532\n",
      "Iteration: 12646, Loss: 0.7472614645957947\n",
      "Iteration: 12647, Loss: 0.7472607493400574\n",
      "Iteration: 12648, Loss: 0.7472600340843201\n",
      "Iteration: 12649, Loss: 0.7472593188285828\n",
      "Iteration: 12650, Loss: 0.7472586035728455\n",
      "Iteration: 12651, Loss: 0.7472578883171082\n",
      "Iteration: 12652, Loss: 0.7472571730613708\n",
      "Iteration: 12653, Loss: 0.7472564578056335\n",
      "Iteration: 12654, Loss: 0.7472557425498962\n",
      "Iteration: 12655, Loss: 0.7472550868988037\n",
      "Iteration: 12656, Loss: 0.7472543716430664\n",
      "Iteration: 12657, Loss: 0.7472536563873291\n",
      "Iteration: 12658, Loss: 0.7472529411315918\n",
      "Iteration: 12659, Loss: 0.7472522854804993\n",
      "Iteration: 12660, Loss: 0.747251570224762\n",
      "Iteration: 12661, Loss: 0.7472508549690247\n",
      "Iteration: 12662, Loss: 0.7472501993179321\n",
      "Iteration: 12663, Loss: 0.7472494840621948\n",
      "Iteration: 12664, Loss: 0.7472488284111023\n",
      "Iteration: 12665, Loss: 0.747248113155365\n",
      "Iteration: 12666, Loss: 0.7472473978996277\n",
      "Iteration: 12667, Loss: 0.7472467422485352\n",
      "Iteration: 12668, Loss: 0.7472460865974426\n",
      "Iteration: 12669, Loss: 0.7472453713417053\n",
      "Iteration: 12670, Loss: 0.7472447156906128\n",
      "Iteration: 12671, Loss: 0.7472440600395203\n",
      "Iteration: 12672, Loss: 0.747243344783783\n",
      "Iteration: 12673, Loss: 0.7472426295280457\n",
      "Iteration: 12674, Loss: 0.7472418546676636\n",
      "Iteration: 12675, Loss: 0.7472411394119263\n",
      "Iteration: 12676, Loss: 0.747240424156189\n",
      "Iteration: 12677, Loss: 0.7472397685050964\n",
      "Iteration: 12678, Loss: 0.7472391128540039\n",
      "Iteration: 12679, Loss: 0.7472383975982666\n",
      "Iteration: 12680, Loss: 0.7472378611564636\n",
      "Iteration: 12681, Loss: 0.7472372055053711\n",
      "Iteration: 12682, Loss: 0.7472364902496338\n",
      "Iteration: 12683, Loss: 0.7472357749938965\n",
      "Iteration: 12684, Loss: 0.7472350597381592\n",
      "Iteration: 12685, Loss: 0.7472344040870667\n",
      "Iteration: 12686, Loss: 0.7472336292266846\n",
      "Iteration: 12687, Loss: 0.7472329139709473\n",
      "Iteration: 12688, Loss: 0.7472321391105652\n",
      "Iteration: 12689, Loss: 0.7472316026687622\n",
      "Iteration: 12690, Loss: 0.7472308874130249\n",
      "Iteration: 12691, Loss: 0.7472302317619324\n",
      "Iteration: 12692, Loss: 0.7472295165061951\n",
      "Iteration: 12693, Loss: 0.7472288012504578\n",
      "Iteration: 12694, Loss: 0.7472281455993652\n",
      "Iteration: 12695, Loss: 0.7472274303436279\n",
      "Iteration: 12696, Loss: 0.7472267150878906\n",
      "Iteration: 12697, Loss: 0.7472260594367981\n",
      "Iteration: 12698, Loss: 0.7472253441810608\n",
      "Iteration: 12699, Loss: 0.7472246289253235\n",
      "Iteration: 12700, Loss: 0.747223973274231\n",
      "Iteration: 12701, Loss: 0.7472232580184937\n",
      "Iteration: 12702, Loss: 0.7472226023674011\n",
      "Iteration: 12703, Loss: 0.7472218871116638\n",
      "Iteration: 12704, Loss: 0.7472211718559265\n",
      "Iteration: 12705, Loss: 0.747220516204834\n",
      "Iteration: 12706, Loss: 0.7472198009490967\n",
      "Iteration: 12707, Loss: 0.7472191452980042\n",
      "Iteration: 12708, Loss: 0.7472184300422668\n",
      "Iteration: 12709, Loss: 0.7472177147865295\n",
      "Iteration: 12710, Loss: 0.747217059135437\n",
      "Iteration: 12711, Loss: 0.7472163438796997\n",
      "Iteration: 12712, Loss: 0.7472156882286072\n",
      "Iteration: 12713, Loss: 0.7472149729728699\n",
      "Iteration: 12714, Loss: 0.7472143173217773\n",
      "Iteration: 12715, Loss: 0.7472136616706848\n",
      "Iteration: 12716, Loss: 0.7472129464149475\n",
      "Iteration: 12717, Loss: 0.747212290763855\n",
      "Iteration: 12718, Loss: 0.7472115755081177\n",
      "Iteration: 12719, Loss: 0.7472109198570251\n",
      "Iteration: 12720, Loss: 0.7472102046012878\n",
      "Iteration: 12721, Loss: 0.7472095489501953\n",
      "Iteration: 12722, Loss: 0.747208833694458\n",
      "Iteration: 12723, Loss: 0.7472081184387207\n",
      "Iteration: 12724, Loss: 0.7472074627876282\n",
      "Iteration: 12725, Loss: 0.7472067475318909\n",
      "Iteration: 12726, Loss: 0.7472060918807983\n",
      "Iteration: 12727, Loss: 0.747205376625061\n",
      "Iteration: 12728, Loss: 0.7472047209739685\n",
      "Iteration: 12729, Loss: 0.7472040057182312\n",
      "Iteration: 12730, Loss: 0.7472033500671387\n",
      "Iteration: 12731, Loss: 0.7472026348114014\n",
      "Iteration: 12732, Loss: 0.7472019195556641\n",
      "Iteration: 12733, Loss: 0.7472012639045715\n",
      "Iteration: 12734, Loss: 0.7472005486488342\n",
      "Iteration: 12735, Loss: 0.7471998929977417\n",
      "Iteration: 12736, Loss: 0.747199296951294\n",
      "Iteration: 12737, Loss: 0.7471986413002014\n",
      "Iteration: 12738, Loss: 0.7471979260444641\n",
      "Iteration: 12739, Loss: 0.7471972703933716\n",
      "Iteration: 12740, Loss: 0.7471965551376343\n",
      "Iteration: 12741, Loss: 0.7471958994865417\n",
      "Iteration: 12742, Loss: 0.7471951842308044\n",
      "Iteration: 12743, Loss: 0.7471945285797119\n",
      "Iteration: 12744, Loss: 0.7471938729286194\n",
      "Iteration: 12745, Loss: 0.7471932172775269\n",
      "Iteration: 12746, Loss: 0.7471926808357239\n",
      "Iteration: 12747, Loss: 0.7471919655799866\n",
      "Iteration: 12748, Loss: 0.747191309928894\n",
      "Iteration: 12749, Loss: 0.7471905946731567\n",
      "Iteration: 12750, Loss: 0.7471899390220642\n",
      "Iteration: 12751, Loss: 0.7471892237663269\n",
      "Iteration: 12752, Loss: 0.7471885681152344\n",
      "Iteration: 12753, Loss: 0.7471878528594971\n",
      "Iteration: 12754, Loss: 0.7471871972084045\n",
      "Iteration: 12755, Loss: 0.747186541557312\n",
      "Iteration: 12756, Loss: 0.7471858263015747\n",
      "Iteration: 12757, Loss: 0.7471851706504822\n",
      "Iteration: 12758, Loss: 0.7471844553947449\n",
      "Iteration: 12759, Loss: 0.7471837997436523\n",
      "Iteration: 12760, Loss: 0.7471831440925598\n",
      "Iteration: 12761, Loss: 0.7471824288368225\n",
      "Iteration: 12762, Loss: 0.74718177318573\n",
      "Iteration: 12763, Loss: 0.7471810579299927\n",
      "Iteration: 12764, Loss: 0.7471804022789001\n",
      "Iteration: 12765, Loss: 0.7471797466278076\n",
      "Iteration: 12766, Loss: 0.7471790313720703\n",
      "Iteration: 12767, Loss: 0.7471783757209778\n",
      "Iteration: 12768, Loss: 0.74717777967453\n",
      "Iteration: 12769, Loss: 0.7471770644187927\n",
      "Iteration: 12770, Loss: 0.7471764087677002\n",
      "Iteration: 12771, Loss: 0.7471757531166077\n",
      "Iteration: 12772, Loss: 0.7471750378608704\n",
      "Iteration: 12773, Loss: 0.7471743822097778\n",
      "Iteration: 12774, Loss: 0.7471737265586853\n",
      "Iteration: 12775, Loss: 0.747173011302948\n",
      "Iteration: 12776, Loss: 0.7471723556518555\n",
      "Iteration: 12777, Loss: 0.7471717000007629\n",
      "Iteration: 12778, Loss: 0.7471709847450256\n",
      "Iteration: 12779, Loss: 0.7471703290939331\n",
      "Iteration: 12780, Loss: 0.7471696734428406\n",
      "Iteration: 12781, Loss: 0.7471689581871033\n",
      "Iteration: 12782, Loss: 0.7471683025360107\n",
      "Iteration: 12783, Loss: 0.7471676468849182\n",
      "Iteration: 12784, Loss: 0.7471669912338257\n",
      "Iteration: 12785, Loss: 0.7471662759780884\n",
      "Iteration: 12786, Loss: 0.7471656203269958\n",
      "Iteration: 12787, Loss: 0.7471649646759033\n",
      "Iteration: 12788, Loss: 0.7471643090248108\n",
      "Iteration: 12789, Loss: 0.7471635937690735\n",
      "Iteration: 12790, Loss: 0.747162938117981\n",
      "Iteration: 12791, Loss: 0.7471623420715332\n",
      "Iteration: 12792, Loss: 0.7471616864204407\n",
      "Iteration: 12793, Loss: 0.7471609711647034\n",
      "Iteration: 12794, Loss: 0.7471603155136108\n",
      "Iteration: 12795, Loss: 0.7471596598625183\n",
      "Iteration: 12796, Loss: 0.7471590042114258\n",
      "Iteration: 12797, Loss: 0.7471583485603333\n",
      "Iteration: 12798, Loss: 0.7471576929092407\n",
      "Iteration: 12799, Loss: 0.7471569776535034\n",
      "Iteration: 12800, Loss: 0.7471563220024109\n",
      "Iteration: 12801, Loss: 0.7471556663513184\n",
      "Iteration: 12802, Loss: 0.7471550107002258\n",
      "Iteration: 12803, Loss: 0.7471543550491333\n",
      "Iteration: 12804, Loss: 0.7471536993980408\n",
      "Iteration: 12805, Loss: 0.7471529841423035\n",
      "Iteration: 12806, Loss: 0.7471523880958557\n",
      "Iteration: 12807, Loss: 0.7471516728401184\n",
      "Iteration: 12808, Loss: 0.7471510171890259\n",
      "Iteration: 12809, Loss: 0.7471503615379333\n",
      "Iteration: 12810, Loss: 0.7471497654914856\n",
      "Iteration: 12811, Loss: 0.7471491694450378\n",
      "Iteration: 12812, Loss: 0.7471485137939453\n",
      "Iteration: 12813, Loss: 0.7471478581428528\n",
      "Iteration: 12814, Loss: 0.7471472024917603\n",
      "Iteration: 12815, Loss: 0.7471465468406677\n",
      "Iteration: 12816, Loss: 0.7471458911895752\n",
      "Iteration: 12817, Loss: 0.7471451759338379\n",
      "Iteration: 12818, Loss: 0.7471445202827454\n",
      "Iteration: 12819, Loss: 0.7471438646316528\n",
      "Iteration: 12820, Loss: 0.7471432089805603\n",
      "Iteration: 12821, Loss: 0.7471427917480469\n",
      "Iteration: 12822, Loss: 0.7471421360969543\n",
      "Iteration: 12823, Loss: 0.7471414804458618\n",
      "Iteration: 12824, Loss: 0.7471408247947693\n",
      "Iteration: 12825, Loss: 0.7471401691436768\n",
      "Iteration: 12826, Loss: 0.7471395134925842\n",
      "Iteration: 12827, Loss: 0.7471388578414917\n",
      "Iteration: 12828, Loss: 0.7471382021903992\n",
      "Iteration: 12829, Loss: 0.7471374869346619\n",
      "Iteration: 12830, Loss: 0.7471368312835693\n",
      "Iteration: 12831, Loss: 0.7471361756324768\n",
      "Iteration: 12832, Loss: 0.7471356391906738\n",
      "Iteration: 12833, Loss: 0.7471349835395813\n",
      "Iteration: 12834, Loss: 0.7471344470977783\n",
      "Iteration: 12835, Loss: 0.7471337914466858\n",
      "Iteration: 12836, Loss: 0.7471331357955933\n",
      "Iteration: 12837, Loss: 0.7471324801445007\n",
      "Iteration: 12838, Loss: 0.7471318244934082\n",
      "Iteration: 12839, Loss: 0.7471311688423157\n",
      "Iteration: 12840, Loss: 0.7471303939819336\n",
      "Iteration: 12841, Loss: 0.7471297383308411\n",
      "Iteration: 12842, Loss: 0.7471291422843933\n",
      "Iteration: 12843, Loss: 0.7471284866333008\n",
      "Iteration: 12844, Loss: 0.7471278309822083\n",
      "Iteration: 12845, Loss: 0.7471271753311157\n",
      "Iteration: 12846, Loss: 0.747126579284668\n",
      "Iteration: 12847, Loss: 0.7471259236335754\n",
      "Iteration: 12848, Loss: 0.7471252679824829\n",
      "Iteration: 12849, Loss: 0.7471246123313904\n",
      "Iteration: 12850, Loss: 0.7471239566802979\n",
      "Iteration: 12851, Loss: 0.7471233010292053\n",
      "Iteration: 12852, Loss: 0.7471226453781128\n",
      "Iteration: 12853, Loss: 0.7471219301223755\n",
      "Iteration: 12854, Loss: 0.747121274471283\n",
      "Iteration: 12855, Loss: 0.7471206784248352\n",
      "Iteration: 12856, Loss: 0.7471200227737427\n",
      "Iteration: 12857, Loss: 0.7471193671226501\n",
      "Iteration: 12858, Loss: 0.7471187710762024\n",
      "Iteration: 12859, Loss: 0.7471181154251099\n",
      "Iteration: 12860, Loss: 0.7471173405647278\n",
      "Iteration: 12861, Loss: 0.7471166849136353\n",
      "Iteration: 12862, Loss: 0.7471160292625427\n",
      "Iteration: 12863, Loss: 0.7471153736114502\n",
      "Iteration: 12864, Loss: 0.7471150159835815\n",
      "Iteration: 12865, Loss: 0.747114360332489\n",
      "Iteration: 12866, Loss: 0.7471135854721069\n",
      "Iteration: 12867, Loss: 0.7471129894256592\n",
      "Iteration: 12868, Loss: 0.7471123337745667\n",
      "Iteration: 12869, Loss: 0.7471117377281189\n",
      "Iteration: 12870, Loss: 0.7471110820770264\n",
      "Iteration: 12871, Loss: 0.7471104264259338\n",
      "Iteration: 12872, Loss: 0.7471097707748413\n",
      "Iteration: 12873, Loss: 0.7471091151237488\n",
      "Iteration: 12874, Loss: 0.747108519077301\n",
      "Iteration: 12875, Loss: 0.7471078634262085\n",
      "Iteration: 12876, Loss: 0.7471073269844055\n",
      "Iteration: 12877, Loss: 0.747106671333313\n",
      "Iteration: 12878, Loss: 0.7471060156822205\n",
      "Iteration: 12879, Loss: 0.7471054196357727\n",
      "Iteration: 12880, Loss: 0.747104823589325\n",
      "Iteration: 12881, Loss: 0.7471041679382324\n",
      "Iteration: 12882, Loss: 0.7471034526824951\n",
      "Iteration: 12883, Loss: 0.7471029162406921\n",
      "Iteration: 12884, Loss: 0.7471022605895996\n",
      "Iteration: 12885, Loss: 0.7471016049385071\n",
      "Iteration: 12886, Loss: 0.7471009492874146\n",
      "Iteration: 12887, Loss: 0.7471002340316772\n",
      "Iteration: 12888, Loss: 0.7470995783805847\n",
      "Iteration: 12889, Loss: 0.7470989227294922\n",
      "Iteration: 12890, Loss: 0.7470982670783997\n",
      "Iteration: 12891, Loss: 0.7470979690551758\n",
      "Iteration: 12892, Loss: 0.7470972537994385\n",
      "Iteration: 12893, Loss: 0.747096598148346\n",
      "Iteration: 12894, Loss: 0.747096061706543\n",
      "Iteration: 12895, Loss: 0.7470954060554504\n",
      "Iteration: 12896, Loss: 0.7470947504043579\n",
      "Iteration: 12897, Loss: 0.7470941543579102\n",
      "Iteration: 12898, Loss: 0.7470935583114624\n",
      "Iteration: 12899, Loss: 0.7470929622650146\n",
      "Iteration: 12900, Loss: 0.7470922470092773\n",
      "Iteration: 12901, Loss: 0.7470916509628296\n",
      "Iteration: 12902, Loss: 0.7470909953117371\n",
      "Iteration: 12903, Loss: 0.7470903992652893\n",
      "Iteration: 12904, Loss: 0.7470897436141968\n",
      "Iteration: 12905, Loss: 0.747089147567749\n",
      "Iteration: 12906, Loss: 0.7470884919166565\n",
      "Iteration: 12907, Loss: 0.7470878958702087\n",
      "Iteration: 12908, Loss: 0.7470872402191162\n",
      "Iteration: 12909, Loss: 0.7470865249633789\n",
      "Iteration: 12910, Loss: 0.7470858693122864\n",
      "Iteration: 12911, Loss: 0.7470853328704834\n",
      "Iteration: 12912, Loss: 0.7470848560333252\n",
      "Iteration: 12913, Loss: 0.7470842003822327\n",
      "Iteration: 12914, Loss: 0.7470836043357849\n",
      "Iteration: 12915, Loss: 0.7470829486846924\n",
      "Iteration: 12916, Loss: 0.7470822930335999\n",
      "Iteration: 12917, Loss: 0.7470816373825073\n",
      "Iteration: 12918, Loss: 0.7470810413360596\n",
      "Iteration: 12919, Loss: 0.7470804452896118\n",
      "Iteration: 12920, Loss: 0.7470797896385193\n",
      "Iteration: 12921, Loss: 0.7470791935920715\n",
      "Iteration: 12922, Loss: 0.747078537940979\n",
      "Iteration: 12923, Loss: 0.7470779418945312\n",
      "Iteration: 12924, Loss: 0.7470772862434387\n",
      "Iteration: 12925, Loss: 0.747076690196991\n",
      "Iteration: 12926, Loss: 0.7470760345458984\n",
      "Iteration: 12927, Loss: 0.7470754384994507\n",
      "Iteration: 12928, Loss: 0.7470747828483582\n",
      "Iteration: 12929, Loss: 0.7470741868019104\n",
      "Iteration: 12930, Loss: 0.7470735907554626\n",
      "Iteration: 12931, Loss: 0.7470729947090149\n",
      "Iteration: 12932, Loss: 0.7470723986625671\n",
      "Iteration: 12933, Loss: 0.7470717430114746\n",
      "Iteration: 12934, Loss: 0.7470711469650269\n",
      "Iteration: 12935, Loss: 0.7470705509185791\n",
      "Iteration: 12936, Loss: 0.7470699548721313\n",
      "Iteration: 12937, Loss: 0.7470692992210388\n",
      "Iteration: 12938, Loss: 0.7470687031745911\n",
      "Iteration: 12939, Loss: 0.7470681071281433\n",
      "Iteration: 12940, Loss: 0.7470674514770508\n",
      "Iteration: 12941, Loss: 0.747066855430603\n",
      "Iteration: 12942, Loss: 0.7470662593841553\n",
      "Iteration: 12943, Loss: 0.7470656633377075\n",
      "Iteration: 12944, Loss: 0.7470650672912598\n",
      "Iteration: 12945, Loss: 0.747064471244812\n",
      "Iteration: 12946, Loss: 0.7470638751983643\n",
      "Iteration: 12947, Loss: 0.7470632791519165\n",
      "Iteration: 12948, Loss: 0.747062623500824\n",
      "Iteration: 12949, Loss: 0.7470620274543762\n",
      "Iteration: 12950, Loss: 0.7470614314079285\n",
      "Iteration: 12951, Loss: 0.7470607757568359\n",
      "Iteration: 12952, Loss: 0.7470601797103882\n",
      "Iteration: 12953, Loss: 0.7470595836639404\n",
      "Iteration: 12954, Loss: 0.7470589876174927\n",
      "Iteration: 12955, Loss: 0.7470583915710449\n",
      "Iteration: 12956, Loss: 0.7470577359199524\n",
      "Iteration: 12957, Loss: 0.7470571398735046\n",
      "Iteration: 12958, Loss: 0.7470564842224121\n",
      "Iteration: 12959, Loss: 0.7470558881759644\n",
      "Iteration: 12960, Loss: 0.7470553517341614\n",
      "Iteration: 12961, Loss: 0.7470546960830688\n",
      "Iteration: 12962, Loss: 0.7470541000366211\n",
      "Iteration: 12963, Loss: 0.7470535039901733\n",
      "Iteration: 12964, Loss: 0.7470529079437256\n",
      "Iteration: 12965, Loss: 0.7470522522926331\n",
      "Iteration: 12966, Loss: 0.7470516562461853\n",
      "Iteration: 12967, Loss: 0.7470510601997375\n",
      "Iteration: 12968, Loss: 0.7470504641532898\n",
      "Iteration: 12969, Loss: 0.7470499277114868\n",
      "Iteration: 12970, Loss: 0.7470492720603943\n",
      "Iteration: 12971, Loss: 0.7470486760139465\n",
      "Iteration: 12972, Loss: 0.7470480799674988\n",
      "Iteration: 12973, Loss: 0.747047483921051\n",
      "Iteration: 12974, Loss: 0.7470468878746033\n",
      "Iteration: 12975, Loss: 0.7470463514328003\n",
      "Iteration: 12976, Loss: 0.7470457553863525\n",
      "Iteration: 12977, Loss: 0.7470451593399048\n",
      "Iteration: 12978, Loss: 0.747044563293457\n",
      "Iteration: 12979, Loss: 0.7470439672470093\n",
      "Iteration: 12980, Loss: 0.7470433115959167\n",
      "Iteration: 12981, Loss: 0.747042715549469\n",
      "Iteration: 12982, Loss: 0.7470421195030212\n",
      "Iteration: 12983, Loss: 0.7470415234565735\n",
      "Iteration: 12984, Loss: 0.7470409274101257\n",
      "Iteration: 12985, Loss: 0.747040331363678\n",
      "Iteration: 12986, Loss: 0.7470397353172302\n",
      "Iteration: 12987, Loss: 0.7470390796661377\n",
      "Iteration: 12988, Loss: 0.7470384836196899\n",
      "Iteration: 12989, Loss: 0.747037947177887\n",
      "Iteration: 12990, Loss: 0.7470373511314392\n",
      "Iteration: 12991, Loss: 0.7470367550849915\n",
      "Iteration: 12992, Loss: 0.7470361590385437\n",
      "Iteration: 12993, Loss: 0.747035562992096\n",
      "Iteration: 12994, Loss: 0.7470349669456482\n",
      "Iteration: 12995, Loss: 0.7470343708992004\n",
      "Iteration: 12996, Loss: 0.7470337748527527\n",
      "Iteration: 12997, Loss: 0.7470330595970154\n",
      "Iteration: 12998, Loss: 0.7470324039459229\n",
      "Iteration: 12999, Loss: 0.7470318078994751\n",
      "Iteration: 13000, Loss: 0.7470312118530273\n",
      "Iteration: 13001, Loss: 0.7470306158065796\n",
      "Iteration: 13002, Loss: 0.7470300197601318\n",
      "Iteration: 13003, Loss: 0.7470294237136841\n",
      "Iteration: 13004, Loss: 0.7470290660858154\n",
      "Iteration: 13005, Loss: 0.7470284700393677\n",
      "Iteration: 13006, Loss: 0.7470278739929199\n",
      "Iteration: 13007, Loss: 0.7470272779464722\n",
      "Iteration: 13008, Loss: 0.7470269203186035\n",
      "Iteration: 13009, Loss: 0.7470263242721558\n",
      "Iteration: 13010, Loss: 0.747025728225708\n",
      "Iteration: 13011, Loss: 0.7470251321792603\n",
      "Iteration: 13012, Loss: 0.7470244765281677\n",
      "Iteration: 13013, Loss: 0.74702388048172\n",
      "Iteration: 13014, Loss: 0.7470232844352722\n",
      "Iteration: 13015, Loss: 0.7470227479934692\n",
      "Iteration: 13016, Loss: 0.7470220923423767\n",
      "Iteration: 13017, Loss: 0.747021496295929\n",
      "Iteration: 13018, Loss: 0.7470209002494812\n",
      "Iteration: 13019, Loss: 0.7470203042030334\n",
      "Iteration: 13020, Loss: 0.7470197081565857\n",
      "Iteration: 13021, Loss: 0.7470192313194275\n",
      "Iteration: 13022, Loss: 0.747018575668335\n",
      "Iteration: 13023, Loss: 0.7470179796218872\n",
      "Iteration: 13024, Loss: 0.7470173835754395\n",
      "Iteration: 13025, Loss: 0.7470167875289917\n",
      "Iteration: 13026, Loss: 0.7470162510871887\n",
      "Iteration: 13027, Loss: 0.747015655040741\n",
      "Iteration: 13028, Loss: 0.7470150589942932\n",
      "Iteration: 13029, Loss: 0.7470144629478455\n",
      "Iteration: 13030, Loss: 0.7470138669013977\n",
      "Iteration: 13031, Loss: 0.7470132112503052\n",
      "Iteration: 13032, Loss: 0.7470126748085022\n",
      "Iteration: 13033, Loss: 0.7470120787620544\n",
      "Iteration: 13034, Loss: 0.7470114827156067\n",
      "Iteration: 13035, Loss: 0.7470108270645142\n",
      "Iteration: 13036, Loss: 0.7470102310180664\n",
      "Iteration: 13037, Loss: 0.7470096349716187\n",
      "Iteration: 13038, Loss: 0.7470090985298157\n",
      "Iteration: 13039, Loss: 0.7470085620880127\n",
      "Iteration: 13040, Loss: 0.7470079660415649\n",
      "Iteration: 13041, Loss: 0.7470073699951172\n",
      "Iteration: 13042, Loss: 0.7470067739486694\n",
      "Iteration: 13043, Loss: 0.7470062375068665\n",
      "Iteration: 13044, Loss: 0.7470056414604187\n",
      "Iteration: 13045, Loss: 0.747005045413971\n",
      "Iteration: 13046, Loss: 0.7470044493675232\n",
      "Iteration: 13047, Loss: 0.7470039129257202\n",
      "Iteration: 13048, Loss: 0.7470033168792725\n",
      "Iteration: 13049, Loss: 0.7470027208328247\n",
      "Iteration: 13050, Loss: 0.7470021843910217\n",
      "Iteration: 13051, Loss: 0.747001588344574\n",
      "Iteration: 13052, Loss: 0.7470009922981262\n",
      "Iteration: 13053, Loss: 0.7470003366470337\n",
      "Iteration: 13054, Loss: 0.7469997406005859\n",
      "Iteration: 13055, Loss: 0.7469991445541382\n",
      "Iteration: 13056, Loss: 0.7469986081123352\n",
      "Iteration: 13057, Loss: 0.7469980716705322\n",
      "Iteration: 13058, Loss: 0.7469974756240845\n",
      "Iteration: 13059, Loss: 0.7469968795776367\n",
      "Iteration: 13060, Loss: 0.7469963431358337\n",
      "Iteration: 13061, Loss: 0.7469958066940308\n",
      "Iteration: 13062, Loss: 0.7469953298568726\n",
      "Iteration: 13063, Loss: 0.7469947338104248\n",
      "Iteration: 13064, Loss: 0.7469940185546875\n",
      "Iteration: 13065, Loss: 0.7469934225082397\n",
      "Iteration: 13066, Loss: 0.7469928860664368\n",
      "Iteration: 13067, Loss: 0.7469923496246338\n",
      "Iteration: 13068, Loss: 0.746991753578186\n",
      "Iteration: 13069, Loss: 0.7469912767410278\n",
      "Iteration: 13070, Loss: 0.7469907402992249\n",
      "Iteration: 13071, Loss: 0.7469901442527771\n",
      "Iteration: 13072, Loss: 0.7469895482063293\n",
      "Iteration: 13073, Loss: 0.7469890713691711\n",
      "Iteration: 13074, Loss: 0.7469884753227234\n",
      "Iteration: 13075, Loss: 0.7469878792762756\n",
      "Iteration: 13076, Loss: 0.7469872832298279\n",
      "Iteration: 13077, Loss: 0.7469867467880249\n",
      "Iteration: 13078, Loss: 0.7469861507415771\n",
      "Iteration: 13079, Loss: 0.7469856142997742\n",
      "Iteration: 13080, Loss: 0.7469850182533264\n",
      "Iteration: 13081, Loss: 0.7469844222068787\n",
      "Iteration: 13082, Loss: 0.7469838857650757\n",
      "Iteration: 13083, Loss: 0.7469832897186279\n",
      "Iteration: 13084, Loss: 0.746982753276825\n",
      "Iteration: 13085, Loss: 0.746982216835022\n",
      "Iteration: 13086, Loss: 0.7469818592071533\n",
      "Iteration: 13087, Loss: 0.7469812631607056\n",
      "Iteration: 13088, Loss: 0.7469807267189026\n",
      "Iteration: 13089, Loss: 0.7469801306724548\n",
      "Iteration: 13090, Loss: 0.7469795942306519\n",
      "Iteration: 13091, Loss: 0.7469789981842041\n",
      "Iteration: 13092, Loss: 0.7469782829284668\n",
      "Iteration: 13093, Loss: 0.7469777464866638\n",
      "Iteration: 13094, Loss: 0.7469772100448608\n",
      "Iteration: 13095, Loss: 0.7469766139984131\n",
      "Iteration: 13096, Loss: 0.7469760775566101\n",
      "Iteration: 13097, Loss: 0.7469754815101624\n",
      "Iteration: 13098, Loss: 0.7469750046730042\n",
      "Iteration: 13099, Loss: 0.7469744682312012\n",
      "Iteration: 13100, Loss: 0.7469739317893982\n",
      "Iteration: 13101, Loss: 0.7469732761383057\n",
      "Iteration: 13102, Loss: 0.7469727396965027\n",
      "Iteration: 13103, Loss: 0.7469721436500549\n",
      "Iteration: 13104, Loss: 0.746971607208252\n",
      "Iteration: 13105, Loss: 0.7469710111618042\n",
      "Iteration: 13106, Loss: 0.7469704747200012\n",
      "Iteration: 13107, Loss: 0.7469698786735535\n",
      "Iteration: 13108, Loss: 0.7469693422317505\n",
      "Iteration: 13109, Loss: 0.7469688057899475\n",
      "Iteration: 13110, Loss: 0.7469682097434998\n",
      "Iteration: 13111, Loss: 0.7469676733016968\n",
      "Iteration: 13112, Loss: 0.7469671368598938\n",
      "Iteration: 13113, Loss: 0.7469666004180908\n",
      "Iteration: 13114, Loss: 0.7469660043716431\n",
      "Iteration: 13115, Loss: 0.7469654679298401\n",
      "Iteration: 13116, Loss: 0.7469649314880371\n",
      "Iteration: 13117, Loss: 0.7469643354415894\n",
      "Iteration: 13118, Loss: 0.7469637989997864\n",
      "Iteration: 13119, Loss: 0.7469632029533386\n",
      "Iteration: 13120, Loss: 0.7469627261161804\n",
      "Iteration: 13121, Loss: 0.7469621896743774\n",
      "Iteration: 13122, Loss: 0.7469617128372192\n",
      "Iteration: 13123, Loss: 0.7469611763954163\n",
      "Iteration: 13124, Loss: 0.7469605207443237\n",
      "Iteration: 13125, Loss: 0.746959924697876\n",
      "Iteration: 13126, Loss: 0.746959388256073\n",
      "Iteration: 13127, Loss: 0.7469589114189148\n",
      "Iteration: 13128, Loss: 0.7469583749771118\n",
      "Iteration: 13129, Loss: 0.7469580173492432\n",
      "Iteration: 13130, Loss: 0.7469573616981506\n",
      "Iteration: 13131, Loss: 0.7469568252563477\n",
      "Iteration: 13132, Loss: 0.7469562888145447\n",
      "Iteration: 13133, Loss: 0.7469557523727417\n",
      "Iteration: 13134, Loss: 0.7469550967216492\n",
      "Iteration: 13135, Loss: 0.7469545602798462\n",
      "Iteration: 13136, Loss: 0.7469540238380432\n",
      "Iteration: 13137, Loss: 0.7469534873962402\n",
      "Iteration: 13138, Loss: 0.7469528913497925\n",
      "Iteration: 13139, Loss: 0.7469523549079895\n",
      "Iteration: 13140, Loss: 0.7469518184661865\n",
      "Iteration: 13141, Loss: 0.7469512820243835\n",
      "Iteration: 13142, Loss: 0.7469507455825806\n",
      "Iteration: 13143, Loss: 0.7469502091407776\n",
      "Iteration: 13144, Loss: 0.7469496726989746\n",
      "Iteration: 13145, Loss: 0.7469490170478821\n",
      "Iteration: 13146, Loss: 0.7469484806060791\n",
      "Iteration: 13147, Loss: 0.7469478845596313\n",
      "Iteration: 13148, Loss: 0.7469474077224731\n",
      "Iteration: 13149, Loss: 0.7469468712806702\n",
      "Iteration: 13150, Loss: 0.7469463348388672\n",
      "Iteration: 13151, Loss: 0.7469457983970642\n",
      "Iteration: 13152, Loss: 0.7469452023506165\n",
      "Iteration: 13153, Loss: 0.7469446659088135\n",
      "Iteration: 13154, Loss: 0.7469441294670105\n",
      "Iteration: 13155, Loss: 0.7469435930252075\n",
      "Iteration: 13156, Loss: 0.7469430565834045\n",
      "Iteration: 13157, Loss: 0.7469424605369568\n",
      "Iteration: 13158, Loss: 0.7469419240951538\n",
      "Iteration: 13159, Loss: 0.7469415068626404\n",
      "Iteration: 13160, Loss: 0.7469409108161926\n",
      "Iteration: 13161, Loss: 0.7469403743743896\n",
      "Iteration: 13162, Loss: 0.7469398379325867\n",
      "Iteration: 13163, Loss: 0.7469392418861389\n",
      "Iteration: 13164, Loss: 0.7469388246536255\n",
      "Iteration: 13165, Loss: 0.7469382882118225\n",
      "Iteration: 13166, Loss: 0.7469376921653748\n",
      "Iteration: 13167, Loss: 0.7469373941421509\n",
      "Iteration: 13168, Loss: 0.7469369173049927\n",
      "Iteration: 13169, Loss: 0.7469363212585449\n",
      "Iteration: 13170, Loss: 0.7469357848167419\n",
      "Iteration: 13171, Loss: 0.746935248374939\n",
      "Iteration: 13172, Loss: 0.746934711933136\n",
      "Iteration: 13173, Loss: 0.7469341158866882\n",
      "Iteration: 13174, Loss: 0.74693363904953\n",
      "Iteration: 13175, Loss: 0.746933102607727\n",
      "Iteration: 13176, Loss: 0.7469326257705688\n",
      "Iteration: 13177, Loss: 0.7469320893287659\n",
      "Iteration: 13178, Loss: 0.7469315528869629\n",
      "Iteration: 13179, Loss: 0.7469310164451599\n",
      "Iteration: 13180, Loss: 0.7469304800033569\n",
      "Iteration: 13181, Loss: 0.7469300031661987\n",
      "Iteration: 13182, Loss: 0.7469294667243958\n",
      "Iteration: 13183, Loss: 0.7469289302825928\n",
      "Iteration: 13184, Loss: 0.7469283938407898\n",
      "Iteration: 13185, Loss: 0.7469278573989868\n",
      "Iteration: 13186, Loss: 0.7469273805618286\n",
      "Iteration: 13187, Loss: 0.7469268441200256\n",
      "Iteration: 13188, Loss: 0.7469263076782227\n",
      "Iteration: 13189, Loss: 0.7469257712364197\n",
      "Iteration: 13190, Loss: 0.7469252347946167\n",
      "Iteration: 13191, Loss: 0.7469245791435242\n",
      "Iteration: 13192, Loss: 0.7469240427017212\n",
      "Iteration: 13193, Loss: 0.7469235062599182\n",
      "Iteration: 13194, Loss: 0.7469229698181152\n",
      "Iteration: 13195, Loss: 0.7469225525856018\n",
      "Iteration: 13196, Loss: 0.7469218969345093\n",
      "Iteration: 13197, Loss: 0.7469214200973511\n",
      "Iteration: 13198, Loss: 0.7469208836555481\n",
      "Iteration: 13199, Loss: 0.7469203472137451\n",
      "Iteration: 13200, Loss: 0.7469199299812317\n",
      "Iteration: 13201, Loss: 0.7469192743301392\n",
      "Iteration: 13202, Loss: 0.7469187378883362\n",
      "Iteration: 13203, Loss: 0.7469182014465332\n",
      "Iteration: 13204, Loss: 0.7469176650047302\n",
      "Iteration: 13205, Loss: 0.746917188167572\n",
      "Iteration: 13206, Loss: 0.746916651725769\n",
      "Iteration: 13207, Loss: 0.7469161152839661\n",
      "Iteration: 13208, Loss: 0.7469155788421631\n",
      "Iteration: 13209, Loss: 0.7469150424003601\n",
      "Iteration: 13210, Loss: 0.7469145059585571\n",
      "Iteration: 13211, Loss: 0.7469139099121094\n",
      "Iteration: 13212, Loss: 0.7469133734703064\n",
      "Iteration: 13213, Loss: 0.7469128370285034\n",
      "Iteration: 13214, Loss: 0.7469123601913452\n",
      "Iteration: 13215, Loss: 0.7469118237495422\n",
      "Iteration: 13216, Loss: 0.746911346912384\n",
      "Iteration: 13217, Loss: 0.746910810470581\n",
      "Iteration: 13218, Loss: 0.7469102740287781\n",
      "Iteration: 13219, Loss: 0.7469098567962646\n",
      "Iteration: 13220, Loss: 0.7469093203544617\n",
      "Iteration: 13221, Loss: 0.7469087839126587\n",
      "Iteration: 13222, Loss: 0.7469084858894348\n",
      "Iteration: 13223, Loss: 0.7469079494476318\n",
      "Iteration: 13224, Loss: 0.7469072341918945\n",
      "Iteration: 13225, Loss: 0.7469068765640259\n",
      "Iteration: 13226, Loss: 0.7469063401222229\n",
      "Iteration: 13227, Loss: 0.7469058036804199\n",
      "Iteration: 13228, Loss: 0.7469052672386169\n",
      "Iteration: 13229, Loss: 0.746904730796814\n",
      "Iteration: 13230, Loss: 0.746904194355011\n",
      "Iteration: 13231, Loss: 0.7469037175178528\n",
      "Iteration: 13232, Loss: 0.7469031810760498\n",
      "Iteration: 13233, Loss: 0.7469027638435364\n",
      "Iteration: 13234, Loss: 0.7469022274017334\n",
      "Iteration: 13235, Loss: 0.7469016909599304\n",
      "Iteration: 13236, Loss: 0.7469012141227722\n",
      "Iteration: 13237, Loss: 0.746900737285614\n",
      "Iteration: 13238, Loss: 0.746900200843811\n",
      "Iteration: 13239, Loss: 0.7468996047973633\n",
      "Iteration: 13240, Loss: 0.7468990683555603\n",
      "Iteration: 13241, Loss: 0.7468985319137573\n",
      "Iteration: 13242, Loss: 0.7468980550765991\n",
      "Iteration: 13243, Loss: 0.7468975186347961\n",
      "Iteration: 13244, Loss: 0.7468970417976379\n",
      "Iteration: 13245, Loss: 0.746896505355835\n",
      "Iteration: 13246, Loss: 0.746895968914032\n",
      "Iteration: 13247, Loss: 0.7468954920768738\n",
      "Iteration: 13248, Loss: 0.7468949556350708\n",
      "Iteration: 13249, Loss: 0.7468944191932678\n",
      "Iteration: 13250, Loss: 0.7468940019607544\n",
      "Iteration: 13251, Loss: 0.7468935251235962\n",
      "Iteration: 13252, Loss: 0.7468929886817932\n",
      "Iteration: 13253, Loss: 0.7468924522399902\n",
      "Iteration: 13254, Loss: 0.746891975402832\n",
      "Iteration: 13255, Loss: 0.7468913793563843\n",
      "Iteration: 13256, Loss: 0.7468910217285156\n",
      "Iteration: 13257, Loss: 0.7468905448913574\n",
      "Iteration: 13258, Loss: 0.7468900084495544\n",
      "Iteration: 13259, Loss: 0.7468894720077515\n",
      "Iteration: 13260, Loss: 0.7468889951705933\n",
      "Iteration: 13261, Loss: 0.7468884587287903\n",
      "Iteration: 13262, Loss: 0.7468879222869873\n",
      "Iteration: 13263, Loss: 0.7468874454498291\n",
      "Iteration: 13264, Loss: 0.7468869090080261\n",
      "Iteration: 13265, Loss: 0.7468864917755127\n",
      "Iteration: 13266, Loss: 0.7468860149383545\n",
      "Iteration: 13267, Loss: 0.746885359287262\n",
      "Iteration: 13268, Loss: 0.7468848824501038\n",
      "Iteration: 13269, Loss: 0.7468843460083008\n",
      "Iteration: 13270, Loss: 0.746883749961853\n",
      "Iteration: 13271, Loss: 0.7468832731246948\n",
      "Iteration: 13272, Loss: 0.7468827366828918\n",
      "Iteration: 13273, Loss: 0.7468822598457336\n",
      "Iteration: 13274, Loss: 0.7468817830085754\n",
      "Iteration: 13275, Loss: 0.7468812465667725\n",
      "Iteration: 13276, Loss: 0.7468807697296143\n",
      "Iteration: 13277, Loss: 0.7468802332878113\n",
      "Iteration: 13278, Loss: 0.7468797564506531\n",
      "Iteration: 13279, Loss: 0.7468792200088501\n",
      "Iteration: 13280, Loss: 0.7468787431716919\n",
      "Iteration: 13281, Loss: 0.7468783855438232\n",
      "Iteration: 13282, Loss: 0.746877908706665\n",
      "Iteration: 13283, Loss: 0.7468773722648621\n",
      "Iteration: 13284, Loss: 0.7468768358230591\n",
      "Iteration: 13285, Loss: 0.7468763589859009\n",
      "Iteration: 13286, Loss: 0.7468758225440979\n",
      "Iteration: 13287, Loss: 0.7468752861022949\n",
      "Iteration: 13288, Loss: 0.7468748092651367\n",
      "Iteration: 13289, Loss: 0.7468743324279785\n",
      "Iteration: 13290, Loss: 0.7468737959861755\n",
      "Iteration: 13291, Loss: 0.7468733191490173\n",
      "Iteration: 13292, Loss: 0.7468727827072144\n",
      "Iteration: 13293, Loss: 0.7468723654747009\n",
      "Iteration: 13294, Loss: 0.746871829032898\n",
      "Iteration: 13295, Loss: 0.7468713521957397\n",
      "Iteration: 13296, Loss: 0.7468706965446472\n",
      "Iteration: 13297, Loss: 0.746870219707489\n",
      "Iteration: 13298, Loss: 0.7468697428703308\n",
      "Iteration: 13299, Loss: 0.7468692660331726\n",
      "Iteration: 13300, Loss: 0.7468687295913696\n",
      "Iteration: 13301, Loss: 0.7468682527542114\n",
      "Iteration: 13302, Loss: 0.7468677163124084\n",
      "Iteration: 13303, Loss: 0.7468671798706055\n",
      "Iteration: 13304, Loss: 0.7468666434288025\n",
      "Iteration: 13305, Loss: 0.7468661665916443\n",
      "Iteration: 13306, Loss: 0.7468658089637756\n",
      "Iteration: 13307, Loss: 0.7468652725219727\n",
      "Iteration: 13308, Loss: 0.7468647956848145\n",
      "Iteration: 13309, Loss: 0.7468642592430115\n",
      "Iteration: 13310, Loss: 0.7468637824058533\n",
      "Iteration: 13311, Loss: 0.7468632459640503\n",
      "Iteration: 13312, Loss: 0.7468627691268921\n",
      "Iteration: 13313, Loss: 0.7468622922897339\n",
      "Iteration: 13314, Loss: 0.7468617558479309\n",
      "Iteration: 13315, Loss: 0.7468612194061279\n",
      "Iteration: 13316, Loss: 0.7468608021736145\n",
      "Iteration: 13317, Loss: 0.7468603253364563\n",
      "Iteration: 13318, Loss: 0.7468598484992981\n",
      "Iteration: 13319, Loss: 0.7468593120574951\n",
      "Iteration: 13320, Loss: 0.7468588352203369\n",
      "Iteration: 13321, Loss: 0.7468582987785339\n",
      "Iteration: 13322, Loss: 0.7468578219413757\n",
      "Iteration: 13323, Loss: 0.7468575239181519\n",
      "Iteration: 13324, Loss: 0.7468569874763489\n",
      "Iteration: 13325, Loss: 0.7468565106391907\n",
      "Iteration: 13326, Loss: 0.7468560338020325\n",
      "Iteration: 13327, Loss: 0.7468554973602295\n",
      "Iteration: 13328, Loss: 0.7468550205230713\n",
      "Iteration: 13329, Loss: 0.7468544840812683\n",
      "Iteration: 13330, Loss: 0.7468539476394653\n",
      "Iteration: 13331, Loss: 0.7468534708023071\n",
      "Iteration: 13332, Loss: 0.7468529939651489\n",
      "Iteration: 13333, Loss: 0.746852457523346\n",
      "Iteration: 13334, Loss: 0.7468520402908325\n",
      "Iteration: 13335, Loss: 0.7468516230583191\n",
      "Iteration: 13336, Loss: 0.7468511462211609\n",
      "Iteration: 13337, Loss: 0.7468506097793579\n",
      "Iteration: 13338, Loss: 0.7468501925468445\n",
      "Iteration: 13339, Loss: 0.7468497157096863\n",
      "Iteration: 13340, Loss: 0.7468491792678833\n",
      "Iteration: 13341, Loss: 0.7468487024307251\n",
      "Iteration: 13342, Loss: 0.7468482255935669\n",
      "Iteration: 13343, Loss: 0.7468476891517639\n",
      "Iteration: 13344, Loss: 0.7468471527099609\n",
      "Iteration: 13345, Loss: 0.7468467354774475\n",
      "Iteration: 13346, Loss: 0.7468462586402893\n",
      "Iteration: 13347, Loss: 0.7468457221984863\n",
      "Iteration: 13348, Loss: 0.7468452453613281\n",
      "Iteration: 13349, Loss: 0.7468447685241699\n",
      "Iteration: 13350, Loss: 0.7468442916870117\n",
      "Iteration: 13351, Loss: 0.7468437552452087\n",
      "Iteration: 13352, Loss: 0.7468432784080505\n",
      "Iteration: 13353, Loss: 0.7468428015708923\n",
      "Iteration: 13354, Loss: 0.7468423247337341\n",
      "Iteration: 13355, Loss: 0.7468417882919312\n",
      "Iteration: 13356, Loss: 0.746841311454773\n",
      "Iteration: 13357, Loss: 0.7468408346176147\n",
      "Iteration: 13358, Loss: 0.7468403577804565\n",
      "Iteration: 13359, Loss: 0.7468398809432983\n",
      "Iteration: 13360, Loss: 0.7468394041061401\n",
      "Iteration: 13361, Loss: 0.7468388676643372\n",
      "Iteration: 13362, Loss: 0.746838390827179\n",
      "Iteration: 13363, Loss: 0.7468380331993103\n",
      "Iteration: 13364, Loss: 0.7468375563621521\n",
      "Iteration: 13365, Loss: 0.7468370795249939\n",
      "Iteration: 13366, Loss: 0.7468366026878357\n",
      "Iteration: 13367, Loss: 0.7468360662460327\n",
      "Iteration: 13368, Loss: 0.7468355894088745\n",
      "Iteration: 13369, Loss: 0.7468349933624268\n",
      "Iteration: 13370, Loss: 0.7468345165252686\n",
      "Iteration: 13371, Loss: 0.7468340396881104\n",
      "Iteration: 13372, Loss: 0.7468335628509521\n",
      "Iteration: 13373, Loss: 0.7468330264091492\n",
      "Iteration: 13374, Loss: 0.7468326091766357\n",
      "Iteration: 13375, Loss: 0.7468321323394775\n",
      "Iteration: 13376, Loss: 0.7468315958976746\n",
      "Iteration: 13377, Loss: 0.7468311190605164\n",
      "Iteration: 13378, Loss: 0.7468306422233582\n",
      "Iteration: 13379, Loss: 0.7468302249908447\n",
      "Iteration: 13380, Loss: 0.7468297481536865\n",
      "Iteration: 13381, Loss: 0.7468292713165283\n",
      "Iteration: 13382, Loss: 0.7468287944793701\n",
      "Iteration: 13383, Loss: 0.7468283176422119\n",
      "Iteration: 13384, Loss: 0.7468277812004089\n",
      "Iteration: 13385, Loss: 0.7468273043632507\n",
      "Iteration: 13386, Loss: 0.7468268275260925\n",
      "Iteration: 13387, Loss: 0.7468263506889343\n",
      "Iteration: 13388, Loss: 0.7468259334564209\n",
      "Iteration: 13389, Loss: 0.7468254566192627\n",
      "Iteration: 13390, Loss: 0.7468249797821045\n",
      "Iteration: 13391, Loss: 0.7468245625495911\n",
      "Iteration: 13392, Loss: 0.7468240857124329\n",
      "Iteration: 13393, Loss: 0.7468236088752747\n",
      "Iteration: 13394, Loss: 0.7468231320381165\n",
      "Iteration: 13395, Loss: 0.7468226552009583\n",
      "Iteration: 13396, Loss: 0.7468221783638\n",
      "Iteration: 13397, Loss: 0.7468217015266418\n",
      "Iteration: 13398, Loss: 0.7468212246894836\n",
      "Iteration: 13399, Loss: 0.7468208074569702\n",
      "Iteration: 13400, Loss: 0.7468202710151672\n",
      "Iteration: 13401, Loss: 0.746819794178009\n",
      "Iteration: 13402, Loss: 0.7468193173408508\n",
      "Iteration: 13403, Loss: 0.7468188405036926\n",
      "Iteration: 13404, Loss: 0.7468183636665344\n",
      "Iteration: 13405, Loss: 0.7468180060386658\n",
      "Iteration: 13406, Loss: 0.7468175292015076\n",
      "Iteration: 13407, Loss: 0.7468170523643494\n",
      "Iteration: 13408, Loss: 0.7468165755271912\n",
      "Iteration: 13409, Loss: 0.746816098690033\n",
      "Iteration: 13410, Loss: 0.7468156218528748\n",
      "Iteration: 13411, Loss: 0.7468151450157166\n",
      "Iteration: 13412, Loss: 0.7468147277832031\n",
      "Iteration: 13413, Loss: 0.7468143701553345\n",
      "Iteration: 13414, Loss: 0.7468138933181763\n",
      "Iteration: 13415, Loss: 0.7468134164810181\n",
      "Iteration: 13416, Loss: 0.7468129396438599\n",
      "Iteration: 13417, Loss: 0.7468124628067017\n",
      "Iteration: 13418, Loss: 0.7468119859695435\n",
      "Iteration: 13419, Loss: 0.7468115091323853\n",
      "Iteration: 13420, Loss: 0.746811032295227\n",
      "Iteration: 13421, Loss: 0.7468105554580688\n",
      "Iteration: 13422, Loss: 0.7468100786209106\n",
      "Iteration: 13423, Loss: 0.7468095421791077\n",
      "Iteration: 13424, Loss: 0.7468091249465942\n",
      "Iteration: 13425, Loss: 0.7468087077140808\n",
      "Iteration: 13426, Loss: 0.7468082308769226\n",
      "Iteration: 13427, Loss: 0.7468077540397644\n",
      "Iteration: 13428, Loss: 0.7468072772026062\n",
      "Iteration: 13429, Loss: 0.746806800365448\n",
      "Iteration: 13430, Loss: 0.7468063235282898\n",
      "Iteration: 13431, Loss: 0.7468058466911316\n",
      "Iteration: 13432, Loss: 0.7468053698539734\n",
      "Iteration: 13433, Loss: 0.74680495262146\n",
      "Iteration: 13434, Loss: 0.7468044757843018\n",
      "Iteration: 13435, Loss: 0.7468040585517883\n",
      "Iteration: 13436, Loss: 0.7468035817146301\n",
      "Iteration: 13437, Loss: 0.7468031048774719\n",
      "Iteration: 13438, Loss: 0.7468026280403137\n",
      "Iteration: 13439, Loss: 0.7468022108078003\n",
      "Iteration: 13440, Loss: 0.7468017339706421\n",
      "Iteration: 13441, Loss: 0.7468012571334839\n",
      "Iteration: 13442, Loss: 0.7468007802963257\n",
      "Iteration: 13443, Loss: 0.7468003034591675\n",
      "Iteration: 13444, Loss: 0.746799886226654\n",
      "Iteration: 13445, Loss: 0.7467994093894958\n",
      "Iteration: 13446, Loss: 0.7467989325523376\n",
      "Iteration: 13447, Loss: 0.7467986345291138\n",
      "Iteration: 13448, Loss: 0.7467981576919556\n",
      "Iteration: 13449, Loss: 0.7467977404594421\n",
      "Iteration: 13450, Loss: 0.7467973232269287\n",
      "Iteration: 13451, Loss: 0.7467968463897705\n",
      "Iteration: 13452, Loss: 0.7467964887619019\n",
      "Iteration: 13453, Loss: 0.7467960119247437\n",
      "Iteration: 13454, Loss: 0.7467955350875854\n",
      "Iteration: 13455, Loss: 0.746795117855072\n",
      "Iteration: 13456, Loss: 0.7467946410179138\n",
      "Iteration: 13457, Loss: 0.7467942237854004\n",
      "Iteration: 13458, Loss: 0.7467937469482422\n",
      "Iteration: 13459, Loss: 0.746793270111084\n",
      "Iteration: 13460, Loss: 0.7467928528785706\n",
      "Iteration: 13461, Loss: 0.7467923760414124\n",
      "Iteration: 13462, Loss: 0.7467918992042542\n",
      "Iteration: 13463, Loss: 0.7467914819717407\n",
      "Iteration: 13464, Loss: 0.7467910051345825\n",
      "Iteration: 13465, Loss: 0.7467905879020691\n",
      "Iteration: 13466, Loss: 0.7467901110649109\n",
      "Iteration: 13467, Loss: 0.7467896938323975\n",
      "Iteration: 13468, Loss: 0.7467892169952393\n",
      "Iteration: 13469, Loss: 0.746788740158081\n",
      "Iteration: 13470, Loss: 0.7467884421348572\n",
      "Iteration: 13471, Loss: 0.746787965297699\n",
      "Iteration: 13472, Loss: 0.7467875480651855\n",
      "Iteration: 13473, Loss: 0.7467870712280273\n",
      "Iteration: 13474, Loss: 0.7467866539955139\n",
      "Iteration: 13475, Loss: 0.7467861175537109\n",
      "Iteration: 13476, Loss: 0.7467858195304871\n",
      "Iteration: 13477, Loss: 0.7467853426933289\n",
      "Iteration: 13478, Loss: 0.7467848658561707\n",
      "Iteration: 13479, Loss: 0.746784508228302\n",
      "Iteration: 13480, Loss: 0.7467840313911438\n",
      "Iteration: 13481, Loss: 0.746783435344696\n",
      "Iteration: 13482, Loss: 0.7467829585075378\n",
      "Iteration: 13483, Loss: 0.7467825412750244\n",
      "Iteration: 13484, Loss: 0.7467820048332214\n",
      "Iteration: 13485, Loss: 0.746781587600708\n",
      "Iteration: 13486, Loss: 0.7467811107635498\n",
      "Iteration: 13487, Loss: 0.7467806935310364\n",
      "Iteration: 13488, Loss: 0.7467802166938782\n",
      "Iteration: 13489, Loss: 0.7467797994613647\n",
      "Iteration: 13490, Loss: 0.7467793226242065\n",
      "Iteration: 13491, Loss: 0.7467789053916931\n",
      "Iteration: 13492, Loss: 0.7467784285545349\n",
      "Iteration: 13493, Loss: 0.7467779517173767\n",
      "Iteration: 13494, Loss: 0.7467775344848633\n",
      "Iteration: 13495, Loss: 0.7467770576477051\n",
      "Iteration: 13496, Loss: 0.7467766404151917\n",
      "Iteration: 13497, Loss: 0.7467761635780334\n",
      "Iteration: 13498, Loss: 0.74677574634552\n",
      "Iteration: 13499, Loss: 0.7467752695083618\n",
      "Iteration: 13500, Loss: 0.7467749118804932\n",
      "Iteration: 13501, Loss: 0.7467744946479797\n",
      "Iteration: 13502, Loss: 0.7467740178108215\n",
      "Iteration: 13503, Loss: 0.7467736005783081\n",
      "Iteration: 13504, Loss: 0.7467731237411499\n",
      "Iteration: 13505, Loss: 0.7467727065086365\n",
      "Iteration: 13506, Loss: 0.7467722296714783\n",
      "Iteration: 13507, Loss: 0.7467717528343201\n",
      "Iteration: 13508, Loss: 0.7467714548110962\n",
      "Iteration: 13509, Loss: 0.746770977973938\n",
      "Iteration: 13510, Loss: 0.7467705607414246\n",
      "Iteration: 13511, Loss: 0.7467700839042664\n",
      "Iteration: 13512, Loss: 0.7467696666717529\n",
      "Iteration: 13513, Loss: 0.7467691898345947\n",
      "Iteration: 13514, Loss: 0.7467687726020813\n",
      "Iteration: 13515, Loss: 0.7467682957649231\n",
      "Iteration: 13516, Loss: 0.7467678189277649\n",
      "Iteration: 13517, Loss: 0.7467674016952515\n",
      "Iteration: 13518, Loss: 0.7467671036720276\n",
      "Iteration: 13519, Loss: 0.7467666864395142\n",
      "Iteration: 13520, Loss: 0.746766209602356\n",
      "Iteration: 13521, Loss: 0.7467657923698425\n",
      "Iteration: 13522, Loss: 0.7467653155326843\n",
      "Iteration: 13523, Loss: 0.7467649579048157\n",
      "Iteration: 13524, Loss: 0.7467644810676575\n",
      "Iteration: 13525, Loss: 0.7467639446258545\n",
      "Iteration: 13526, Loss: 0.7467635273933411\n",
      "Iteration: 13527, Loss: 0.7467630505561829\n",
      "Iteration: 13528, Loss: 0.7467626333236694\n",
      "Iteration: 13529, Loss: 0.7467621564865112\n",
      "Iteration: 13530, Loss: 0.7467617392539978\n",
      "Iteration: 13531, Loss: 0.7467613220214844\n",
      "Iteration: 13532, Loss: 0.7467608451843262\n",
      "Iteration: 13533, Loss: 0.7467604279518127\n",
      "Iteration: 13534, Loss: 0.7467602491378784\n",
      "Iteration: 13535, Loss: 0.746759831905365\n",
      "Iteration: 13536, Loss: 0.7467594146728516\n",
      "Iteration: 13537, Loss: 0.7467589378356934\n",
      "Iteration: 13538, Loss: 0.7467585802078247\n",
      "Iteration: 13539, Loss: 0.7467581629753113\n",
      "Iteration: 13540, Loss: 0.7467576861381531\n",
      "Iteration: 13541, Loss: 0.7467572093009949\n",
      "Iteration: 13542, Loss: 0.7467567920684814\n",
      "Iteration: 13543, Loss: 0.746756374835968\n",
      "Iteration: 13544, Loss: 0.7467559576034546\n",
      "Iteration: 13545, Loss: 0.7467554807662964\n",
      "Iteration: 13546, Loss: 0.746755063533783\n",
      "Iteration: 13547, Loss: 0.7467545866966248\n",
      "Iteration: 13548, Loss: 0.7467541694641113\n",
      "Iteration: 13549, Loss: 0.7467537522315979\n",
      "Iteration: 13550, Loss: 0.7467532753944397\n",
      "Iteration: 13551, Loss: 0.7467528581619263\n",
      "Iteration: 13552, Loss: 0.7467524409294128\n",
      "Iteration: 13553, Loss: 0.7467520236968994\n",
      "Iteration: 13554, Loss: 0.7467518448829651\n",
      "Iteration: 13555, Loss: 0.7467514276504517\n",
      "Iteration: 13556, Loss: 0.746751070022583\n",
      "Iteration: 13557, Loss: 0.74675053358078\n",
      "Iteration: 13558, Loss: 0.7467501163482666\n",
      "Iteration: 13559, Loss: 0.7467496991157532\n",
      "Iteration: 13560, Loss: 0.7467494010925293\n",
      "Iteration: 13561, Loss: 0.7467489838600159\n",
      "Iteration: 13562, Loss: 0.7467485070228577\n",
      "Iteration: 13563, Loss: 0.7467479705810547\n",
      "Iteration: 13564, Loss: 0.7467474937438965\n",
      "Iteration: 13565, Loss: 0.7467470765113831\n",
      "Iteration: 13566, Loss: 0.7467466592788696\n",
      "Iteration: 13567, Loss: 0.7467462420463562\n",
      "Iteration: 13568, Loss: 0.7467458248138428\n",
      "Iteration: 13569, Loss: 0.7467454075813293\n",
      "Iteration: 13570, Loss: 0.7467449903488159\n",
      "Iteration: 13571, Loss: 0.7467445731163025\n",
      "Iteration: 13572, Loss: 0.7467440366744995\n",
      "Iteration: 13573, Loss: 0.7467436790466309\n",
      "Iteration: 13574, Loss: 0.7467432022094727\n",
      "Iteration: 13575, Loss: 0.7467427849769592\n",
      "Iteration: 13576, Loss: 0.746742308139801\n",
      "Iteration: 13577, Loss: 0.7467418909072876\n",
      "Iteration: 13578, Loss: 0.7467414140701294\n",
      "Iteration: 13579, Loss: 0.746740996837616\n",
      "Iteration: 13580, Loss: 0.7467406392097473\n",
      "Iteration: 13581, Loss: 0.7467402219772339\n",
      "Iteration: 13582, Loss: 0.7467398047447205\n",
      "Iteration: 13583, Loss: 0.7467392086982727\n",
      "Iteration: 13584, Loss: 0.7467389702796936\n",
      "Iteration: 13585, Loss: 0.7467385530471802\n",
      "Iteration: 13586, Loss: 0.7467381358146667\n",
      "Iteration: 13587, Loss: 0.7467377185821533\n",
      "Iteration: 13588, Loss: 0.7467372417449951\n",
      "Iteration: 13589, Loss: 0.7467368245124817\n",
      "Iteration: 13590, Loss: 0.7467364072799683\n",
      "Iteration: 13591, Loss: 0.7467359900474548\n",
      "Iteration: 13592, Loss: 0.7467355728149414\n",
      "Iteration: 13593, Loss: 0.7467352151870728\n",
      "Iteration: 13594, Loss: 0.7467347383499146\n",
      "Iteration: 13595, Loss: 0.7467342019081116\n",
      "Iteration: 13596, Loss: 0.7467337846755981\n",
      "Iteration: 13597, Loss: 0.7467336058616638\n",
      "Iteration: 13598, Loss: 0.7467331886291504\n",
      "Iteration: 13599, Loss: 0.746732771396637\n",
      "Iteration: 13600, Loss: 0.7467323541641235\n",
      "Iteration: 13601, Loss: 0.7467319369316101\n",
      "Iteration: 13602, Loss: 0.7467315793037415\n",
      "Iteration: 13603, Loss: 0.7467309832572937\n",
      "Iteration: 13604, Loss: 0.7467305064201355\n",
      "Iteration: 13605, Loss: 0.7467300295829773\n",
      "Iteration: 13606, Loss: 0.7467295527458191\n",
      "Iteration: 13607, Loss: 0.7467291951179504\n",
      "Iteration: 13608, Loss: 0.7467288374900818\n",
      "Iteration: 13609, Loss: 0.7467284798622131\n",
      "Iteration: 13610, Loss: 0.7467280626296997\n",
      "Iteration: 13611, Loss: 0.7467277646064758\n",
      "Iteration: 13612, Loss: 0.7467273473739624\n",
      "Iteration: 13613, Loss: 0.7467268705368042\n",
      "Iteration: 13614, Loss: 0.7467264533042908\n",
      "Iteration: 13615, Loss: 0.7467260360717773\n",
      "Iteration: 13616, Loss: 0.7467255592346191\n",
      "Iteration: 13617, Loss: 0.7467251420021057\n",
      "Iteration: 13618, Loss: 0.7467247247695923\n",
      "Iteration: 13619, Loss: 0.7467243671417236\n",
      "Iteration: 13620, Loss: 0.7467239499092102\n",
      "Iteration: 13621, Loss: 0.7467235326766968\n",
      "Iteration: 13622, Loss: 0.7467231154441833\n",
      "Iteration: 13623, Loss: 0.7467228174209595\n",
      "Iteration: 13624, Loss: 0.746722400188446\n",
      "Iteration: 13625, Loss: 0.7467219233512878\n",
      "Iteration: 13626, Loss: 0.7467215061187744\n",
      "Iteration: 13627, Loss: 0.746721088886261\n",
      "Iteration: 13628, Loss: 0.7467207312583923\n",
      "Iteration: 13629, Loss: 0.7467203140258789\n",
      "Iteration: 13630, Loss: 0.7467200756072998\n",
      "Iteration: 13631, Loss: 0.7467196583747864\n",
      "Iteration: 13632, Loss: 0.746719241142273\n",
      "Iteration: 13633, Loss: 0.7467189431190491\n",
      "Iteration: 13634, Loss: 0.7467184066772461\n",
      "Iteration: 13635, Loss: 0.7467179894447327\n",
      "Iteration: 13636, Loss: 0.7467175722122192\n",
      "Iteration: 13637, Loss: 0.7467171549797058\n",
      "Iteration: 13638, Loss: 0.7467167377471924\n",
      "Iteration: 13639, Loss: 0.746716320514679\n",
      "Iteration: 13640, Loss: 0.7467159628868103\n",
      "Iteration: 13641, Loss: 0.7467155456542969\n",
      "Iteration: 13642, Loss: 0.7467151284217834\n",
      "Iteration: 13643, Loss: 0.7467147707939148\n",
      "Iteration: 13644, Loss: 0.7467143535614014\n",
      "Iteration: 13645, Loss: 0.7467139363288879\n",
      "Iteration: 13646, Loss: 0.746713399887085\n",
      "Iteration: 13647, Loss: 0.7467129826545715\n",
      "Iteration: 13648, Loss: 0.7467125654220581\n",
      "Iteration: 13649, Loss: 0.7467121481895447\n",
      "Iteration: 13650, Loss: 0.7467117309570312\n",
      "Iteration: 13651, Loss: 0.7467113137245178\n",
      "Iteration: 13652, Loss: 0.7467109560966492\n",
      "Iteration: 13653, Loss: 0.7467105388641357\n",
      "Iteration: 13654, Loss: 0.7467101216316223\n",
      "Iteration: 13655, Loss: 0.7467097043991089\n",
      "Iteration: 13656, Loss: 0.7467092871665955\n",
      "Iteration: 13657, Loss: 0.746708869934082\n",
      "Iteration: 13658, Loss: 0.7467084527015686\n",
      "Iteration: 13659, Loss: 0.7467080950737\n",
      "Iteration: 13660, Loss: 0.7467076778411865\n",
      "Iteration: 13661, Loss: 0.7467072606086731\n",
      "Iteration: 13662, Loss: 0.7467067837715149\n",
      "Iteration: 13663, Loss: 0.7467063665390015\n",
      "Iteration: 13664, Loss: 0.746705949306488\n",
      "Iteration: 13665, Loss: 0.7467054724693298\n",
      "Iteration: 13666, Loss: 0.7467051148414612\n",
      "Iteration: 13667, Loss: 0.7467046976089478\n",
      "Iteration: 13668, Loss: 0.7467042803764343\n",
      "Iteration: 13669, Loss: 0.7467039823532104\n",
      "Iteration: 13670, Loss: 0.746703565120697\n",
      "Iteration: 13671, Loss: 0.7467031478881836\n",
      "Iteration: 13672, Loss: 0.7467027306556702\n",
      "Iteration: 13673, Loss: 0.7467024326324463\n",
      "Iteration: 13674, Loss: 0.7467020750045776\n",
      "Iteration: 13675, Loss: 0.7467016577720642\n",
      "Iteration: 13676, Loss: 0.7467012405395508\n",
      "Iteration: 13677, Loss: 0.7467008233070374\n",
      "Iteration: 13678, Loss: 0.7467004060745239\n",
      "Iteration: 13679, Loss: 0.7466999888420105\n",
      "Iteration: 13680, Loss: 0.7466995716094971\n",
      "Iteration: 13681, Loss: 0.7466991543769836\n",
      "Iteration: 13682, Loss: 0.7466987371444702\n",
      "Iteration: 13683, Loss: 0.7466984391212463\n",
      "Iteration: 13684, Loss: 0.7466980218887329\n",
      "Iteration: 13685, Loss: 0.7466976046562195\n",
      "Iteration: 13686, Loss: 0.746697187423706\n",
      "Iteration: 13687, Loss: 0.7466967701911926\n",
      "Iteration: 13688, Loss: 0.7466963529586792\n",
      "Iteration: 13689, Loss: 0.7466959357261658\n",
      "Iteration: 13690, Loss: 0.7466955184936523\n",
      "Iteration: 13691, Loss: 0.7466952204704285\n",
      "Iteration: 13692, Loss: 0.746694803237915\n",
      "Iteration: 13693, Loss: 0.7466943860054016\n",
      "Iteration: 13694, Loss: 0.7466939687728882\n",
      "Iteration: 13695, Loss: 0.7466935515403748\n",
      "Iteration: 13696, Loss: 0.7466931939125061\n",
      "Iteration: 13697, Loss: 0.7466927766799927\n",
      "Iteration: 13698, Loss: 0.7466923594474792\n",
      "Iteration: 13699, Loss: 0.7466919422149658\n",
      "Iteration: 13700, Loss: 0.7466915249824524\n",
      "Iteration: 13701, Loss: 0.7466911673545837\n",
      "Iteration: 13702, Loss: 0.7466908693313599\n",
      "Iteration: 13703, Loss: 0.7466904520988464\n",
      "Iteration: 13704, Loss: 0.746690034866333\n",
      "Iteration: 13705, Loss: 0.7466896772384644\n",
      "Iteration: 13706, Loss: 0.7466892600059509\n",
      "Iteration: 13707, Loss: 0.7466888427734375\n",
      "Iteration: 13708, Loss: 0.7466884255409241\n",
      "Iteration: 13709, Loss: 0.7466880083084106\n",
      "Iteration: 13710, Loss: 0.7466877102851868\n",
      "Iteration: 13711, Loss: 0.7466872930526733\n",
      "Iteration: 13712, Loss: 0.7466869354248047\n",
      "Iteration: 13713, Loss: 0.7466865181922913\n",
      "Iteration: 13714, Loss: 0.7466861605644226\n",
      "Iteration: 13715, Loss: 0.7466857433319092\n",
      "Iteration: 13716, Loss: 0.7466853260993958\n",
      "Iteration: 13717, Loss: 0.7466849684715271\n",
      "Iteration: 13718, Loss: 0.7466845512390137\n",
      "Iteration: 13719, Loss: 0.7466841340065002\n",
      "Iteration: 13720, Loss: 0.7466837763786316\n",
      "Iteration: 13721, Loss: 0.7466833591461182\n",
      "Iteration: 13722, Loss: 0.7466829419136047\n",
      "Iteration: 13723, Loss: 0.7466825842857361\n",
      "Iteration: 13724, Loss: 0.7466821670532227\n",
      "Iteration: 13725, Loss: 0.7466817498207092\n",
      "Iteration: 13726, Loss: 0.7466815114021301\n",
      "Iteration: 13727, Loss: 0.7466810941696167\n",
      "Iteration: 13728, Loss: 0.7466806769371033\n",
      "Iteration: 13729, Loss: 0.7466803193092346\n",
      "Iteration: 13730, Loss: 0.7466799020767212\n",
      "Iteration: 13731, Loss: 0.7466793060302734\n",
      "Iteration: 13732, Loss: 0.7466789484024048\n",
      "Iteration: 13733, Loss: 0.7466785311698914\n",
      "Iteration: 13734, Loss: 0.7466781139373779\n",
      "Iteration: 13735, Loss: 0.7466777563095093\n",
      "Iteration: 13736, Loss: 0.7466773390769958\n",
      "Iteration: 13737, Loss: 0.7466769218444824\n",
      "Iteration: 13738, Loss: 0.7466766238212585\n",
      "Iteration: 13739, Loss: 0.7466762065887451\n",
      "Iteration: 13740, Loss: 0.7466757893562317\n",
      "Iteration: 13741, Loss: 0.746675431728363\n",
      "Iteration: 13742, Loss: 0.7466750144958496\n",
      "Iteration: 13743, Loss: 0.7466745972633362\n",
      "Iteration: 13744, Loss: 0.7466742396354675\n",
      "Iteration: 13745, Loss: 0.7466738224029541\n",
      "Iteration: 13746, Loss: 0.7466735243797302\n",
      "Iteration: 13747, Loss: 0.7466731667518616\n",
      "Iteration: 13748, Loss: 0.7466728091239929\n",
      "Iteration: 13749, Loss: 0.7466724514961243\n",
      "Iteration: 13750, Loss: 0.7466720342636108\n",
      "Iteration: 13751, Loss: 0.7466716170310974\n",
      "Iteration: 13752, Loss: 0.7466712594032288\n",
      "Iteration: 13753, Loss: 0.7466708421707153\n",
      "Iteration: 13754, Loss: 0.7466704249382019\n",
      "Iteration: 13755, Loss: 0.7466700673103333\n",
      "Iteration: 13756, Loss: 0.7466696500778198\n",
      "Iteration: 13757, Loss: 0.7466692924499512\n",
      "Iteration: 13758, Loss: 0.7466688752174377\n",
      "Iteration: 13759, Loss: 0.7466684579849243\n",
      "Iteration: 13760, Loss: 0.7466681003570557\n",
      "Iteration: 13761, Loss: 0.7466676831245422\n",
      "Iteration: 13762, Loss: 0.7466673254966736\n",
      "Iteration: 13763, Loss: 0.7466669082641602\n",
      "Iteration: 13764, Loss: 0.7466665506362915\n",
      "Iteration: 13765, Loss: 0.7466661930084229\n",
      "Iteration: 13766, Loss: 0.7466658353805542\n",
      "Iteration: 13767, Loss: 0.7466654181480408\n",
      "Iteration: 13768, Loss: 0.7466652989387512\n",
      "Iteration: 13769, Loss: 0.7466649413108826\n",
      "Iteration: 13770, Loss: 0.7466645240783691\n",
      "Iteration: 13771, Loss: 0.7466641068458557\n",
      "Iteration: 13772, Loss: 0.7466636896133423\n",
      "Iteration: 13773, Loss: 0.7466633319854736\n",
      "Iteration: 13774, Loss: 0.746662974357605\n",
      "Iteration: 13775, Loss: 0.7466625571250916\n",
      "Iteration: 13776, Loss: 0.7466621994972229\n",
      "Iteration: 13777, Loss: 0.7466617822647095\n",
      "Iteration: 13778, Loss: 0.7466614246368408\n",
      "Iteration: 13779, Loss: 0.7466611266136169\n",
      "Iteration: 13780, Loss: 0.7466607093811035\n",
      "Iteration: 13781, Loss: 0.7466603517532349\n",
      "Iteration: 13782, Loss: 0.7466599345207214\n",
      "Iteration: 13783, Loss: 0.7466595768928528\n",
      "Iteration: 13784, Loss: 0.7466591000556946\n",
      "Iteration: 13785, Loss: 0.7466586828231812\n",
      "Iteration: 13786, Loss: 0.7466583251953125\n",
      "Iteration: 13787, Loss: 0.7466579675674438\n",
      "Iteration: 13788, Loss: 0.7466575503349304\n",
      "Iteration: 13789, Loss: 0.7466571927070618\n",
      "Iteration: 13790, Loss: 0.7466568350791931\n",
      "Iteration: 13791, Loss: 0.7466564178466797\n",
      "Iteration: 13792, Loss: 0.746656060218811\n",
      "Iteration: 13793, Loss: 0.7466556429862976\n",
      "Iteration: 13794, Loss: 0.7466554045677185\n",
      "Iteration: 13795, Loss: 0.7466549277305603\n",
      "Iteration: 13796, Loss: 0.7466545701026917\n",
      "Iteration: 13797, Loss: 0.746654212474823\n",
      "Iteration: 13798, Loss: 0.7466537952423096\n",
      "Iteration: 13799, Loss: 0.7466534376144409\n",
      "Iteration: 13800, Loss: 0.7466530203819275\n",
      "Iteration: 13801, Loss: 0.7466526627540588\n",
      "Iteration: 13802, Loss: 0.7466522455215454\n",
      "Iteration: 13803, Loss: 0.746651828289032\n",
      "Iteration: 13804, Loss: 0.7466514706611633\n",
      "Iteration: 13805, Loss: 0.7466510534286499\n",
      "Iteration: 13806, Loss: 0.7466506958007812\n",
      "Iteration: 13807, Loss: 0.7466503381729126\n",
      "Iteration: 13808, Loss: 0.7466499209403992\n",
      "Iteration: 13809, Loss: 0.7466496825218201\n",
      "Iteration: 13810, Loss: 0.7466492652893066\n",
      "Iteration: 13811, Loss: 0.746648907661438\n",
      "Iteration: 13812, Loss: 0.7466485500335693\n",
      "Iteration: 13813, Loss: 0.7466481924057007\n",
      "Iteration: 13814, Loss: 0.746647834777832\n",
      "Iteration: 13815, Loss: 0.7466472387313843\n",
      "Iteration: 13816, Loss: 0.7466469407081604\n",
      "Iteration: 13817, Loss: 0.7466465830802917\n",
      "Iteration: 13818, Loss: 0.7466461658477783\n",
      "Iteration: 13819, Loss: 0.7466458082199097\n",
      "Iteration: 13820, Loss: 0.746645450592041\n",
      "Iteration: 13821, Loss: 0.7466450333595276\n",
      "Iteration: 13822, Loss: 0.7466446757316589\n",
      "Iteration: 13823, Loss: 0.7466443181037903\n",
      "Iteration: 13824, Loss: 0.7466441988945007\n",
      "Iteration: 13825, Loss: 0.7466437220573425\n",
      "Iteration: 13826, Loss: 0.7466433644294739\n",
      "Iteration: 13827, Loss: 0.7466446161270142\n",
      "Iteration: 13828, Loss: 0.7466442584991455\n",
      "Iteration: 13829, Loss: 0.7466439008712769\n",
      "Iteration: 13830, Loss: 0.7466434836387634\n",
      "Iteration: 13831, Loss: 0.7466430068016052\n",
      "Iteration: 13832, Loss: 0.7466427087783813\n",
      "Iteration: 13833, Loss: 0.7466424107551575\n",
      "Iteration: 13834, Loss: 0.746641993522644\n",
      "Iteration: 13835, Loss: 0.7466414570808411\n",
      "Iteration: 13836, Loss: 0.7466410398483276\n",
      "Iteration: 13837, Loss: 0.7466407418251038\n",
      "Iteration: 13838, Loss: 0.7466405034065247\n",
      "Iteration: 13839, Loss: 0.7466400265693665\n",
      "Iteration: 13840, Loss: 0.7466395497322083\n",
      "Iteration: 13841, Loss: 0.7466393113136292\n",
      "Iteration: 13842, Loss: 0.7466390132904053\n",
      "Iteration: 13843, Loss: 0.7466386556625366\n",
      "Iteration: 13844, Loss: 0.7466383576393127\n",
      "Iteration: 13845, Loss: 0.7466379404067993\n",
      "Iteration: 13846, Loss: 0.7466375231742859\n",
      "Iteration: 13847, Loss: 0.746637225151062\n",
      "Iteration: 13848, Loss: 0.7466368079185486\n",
      "Iteration: 13849, Loss: 0.7466362714767456\n",
      "Iteration: 13850, Loss: 0.7466359734535217\n",
      "Iteration: 13851, Loss: 0.7466357350349426\n",
      "Iteration: 13852, Loss: 0.7466352581977844\n",
      "Iteration: 13853, Loss: 0.7466349005699158\n",
      "Iteration: 13854, Loss: 0.7466346025466919\n",
      "Iteration: 13855, Loss: 0.7466341257095337\n",
      "Iteration: 13856, Loss: 0.7466338872909546\n",
      "Iteration: 13857, Loss: 0.7466334700584412\n",
      "Iteration: 13858, Loss: 0.7466331124305725\n",
      "Iteration: 13859, Loss: 0.7466327548027039\n",
      "Iteration: 13860, Loss: 0.7466323375701904\n",
      "Iteration: 13861, Loss: 0.746631920337677\n",
      "Iteration: 13862, Loss: 0.7466315031051636\n",
      "Iteration: 13863, Loss: 0.7466311454772949\n",
      "Iteration: 13864, Loss: 0.7466309070587158\n",
      "Iteration: 13865, Loss: 0.7466306090354919\n",
      "Iteration: 13866, Loss: 0.7466301918029785\n",
      "Iteration: 13867, Loss: 0.7466298341751099\n",
      "Iteration: 13868, Loss: 0.746629536151886\n",
      "Iteration: 13869, Loss: 0.7466291785240173\n",
      "Iteration: 13870, Loss: 0.7466285824775696\n",
      "Iteration: 13871, Loss: 0.7466282248497009\n",
      "Iteration: 13872, Loss: 0.7466278672218323\n",
      "Iteration: 13873, Loss: 0.7466274499893188\n",
      "Iteration: 13874, Loss: 0.7466270923614502\n",
      "Iteration: 13875, Loss: 0.7466267347335815\n",
      "Iteration: 13876, Loss: 0.7466263771057129\n",
      "Iteration: 13877, Loss: 0.7466261386871338\n",
      "Iteration: 13878, Loss: 0.7466257810592651\n",
      "Iteration: 13879, Loss: 0.7466253638267517\n",
      "Iteration: 13880, Loss: 0.7466250061988831\n",
      "Iteration: 13881, Loss: 0.7466244697570801\n",
      "Iteration: 13882, Loss: 0.7466241121292114\n",
      "Iteration: 13883, Loss: 0.7466239333152771\n",
      "Iteration: 13884, Loss: 0.7466235756874084\n",
      "Iteration: 13885, Loss: 0.7466232180595398\n",
      "Iteration: 13886, Loss: 0.7466228604316711\n",
      "Iteration: 13887, Loss: 0.7466224431991577\n",
      "Iteration: 13888, Loss: 0.7466221451759338\n",
      "Iteration: 13889, Loss: 0.74662184715271\n",
      "Iteration: 13890, Loss: 0.7466214895248413\n",
      "Iteration: 13891, Loss: 0.7466211318969727\n",
      "Iteration: 13892, Loss: 0.7466207146644592\n",
      "Iteration: 13893, Loss: 0.7466203570365906\n",
      "Iteration: 13894, Loss: 0.7466199994087219\n",
      "Iteration: 13895, Loss: 0.7466196417808533\n",
      "Iteration: 13896, Loss: 0.7466192841529846\n",
      "Iteration: 13897, Loss: 0.7466189861297607\n",
      "Iteration: 13898, Loss: 0.7466186285018921\n",
      "Iteration: 13899, Loss: 0.7466182708740234\n",
      "Iteration: 13900, Loss: 0.7466179728507996\n",
      "Iteration: 13901, Loss: 0.7466175556182861\n",
      "Iteration: 13902, Loss: 0.7466171979904175\n",
      "Iteration: 13903, Loss: 0.7466168403625488\n",
      "Iteration: 13904, Loss: 0.7466164827346802\n",
      "Iteration: 13905, Loss: 0.7466161847114563\n",
      "Iteration: 13906, Loss: 0.7466158270835876\n",
      "Iteration: 13907, Loss: 0.746615469455719\n",
      "Iteration: 13908, Loss: 0.7466149926185608\n",
      "Iteration: 13909, Loss: 0.7466145753860474\n",
      "Iteration: 13910, Loss: 0.7466141581535339\n",
      "Iteration: 13911, Loss: 0.7466138005256653\n",
      "Iteration: 13912, Loss: 0.7466134428977966\n",
      "Iteration: 13913, Loss: 0.746613085269928\n",
      "Iteration: 13914, Loss: 0.7466127872467041\n",
      "Iteration: 13915, Loss: 0.7466124296188354\n",
      "Iteration: 13916, Loss: 0.7466121315956116\n",
      "Iteration: 13917, Loss: 0.7466117739677429\n",
      "Iteration: 13918, Loss: 0.7466114163398743\n",
      "Iteration: 13919, Loss: 0.7466110587120056\n",
      "Iteration: 13920, Loss: 0.746610701084137\n",
      "Iteration: 13921, Loss: 0.7466103434562683\n",
      "Iteration: 13922, Loss: 0.7466100454330444\n",
      "Iteration: 13923, Loss: 0.7466096878051758\n",
      "Iteration: 13924, Loss: 0.7466093301773071\n",
      "Iteration: 13925, Loss: 0.7466091513633728\n",
      "Iteration: 13926, Loss: 0.7466087937355042\n",
      "Iteration: 13927, Loss: 0.7466083765029907\n",
      "Iteration: 13928, Loss: 0.7466080188751221\n",
      "Iteration: 13929, Loss: 0.746607780456543\n",
      "Iteration: 13930, Loss: 0.7466075420379639\n",
      "Iteration: 13931, Loss: 0.7466071844100952\n",
      "Iteration: 13932, Loss: 0.7466068267822266\n",
      "Iteration: 13933, Loss: 0.7466064691543579\n",
      "Iteration: 13934, Loss: 0.7466061115264893\n",
      "Iteration: 13935, Loss: 0.7466058135032654\n",
      "Iteration: 13936, Loss: 0.7466054558753967\n",
      "Iteration: 13937, Loss: 0.7466050386428833\n",
      "Iteration: 13938, Loss: 0.7466047406196594\n",
      "Iteration: 13939, Loss: 0.7466045618057251\n",
      "Iteration: 13940, Loss: 0.7466042041778564\n",
      "Iteration: 13941, Loss: 0.7466038465499878\n",
      "Iteration: 13942, Loss: 0.7466036081314087\n",
      "Iteration: 13943, Loss: 0.74660325050354\n",
      "Iteration: 13944, Loss: 0.7466028332710266\n",
      "Iteration: 13945, Loss: 0.7466025352478027\n",
      "Iteration: 13946, Loss: 0.7466021776199341\n",
      "Iteration: 13947, Loss: 0.7466017603874207\n",
      "Iteration: 13948, Loss: 0.7466014623641968\n",
      "Iteration: 13949, Loss: 0.7466011047363281\n",
      "Iteration: 13950, Loss: 0.7466007471084595\n",
      "Iteration: 13951, Loss: 0.7466005682945251\n",
      "Iteration: 13952, Loss: 0.746600329875946\n",
      "Iteration: 13953, Loss: 0.7466000318527222\n",
      "Iteration: 13954, Loss: 0.7465994954109192\n",
      "Iteration: 13955, Loss: 0.7465991973876953\n",
      "Iteration: 13956, Loss: 0.7465986609458923\n",
      "Iteration: 13957, Loss: 0.7465984225273132\n",
      "Iteration: 13958, Loss: 0.7465981245040894\n",
      "Iteration: 13959, Loss: 0.7465978860855103\n",
      "Iteration: 13960, Loss: 0.7465975880622864\n",
      "Iteration: 13961, Loss: 0.7465972304344177\n",
      "Iteration: 13962, Loss: 0.7465969324111938\n",
      "Iteration: 13963, Loss: 0.7465963363647461\n",
      "Iteration: 13964, Loss: 0.7465959787368774\n",
      "Iteration: 13965, Loss: 0.746595561504364\n",
      "Iteration: 13966, Loss: 0.7465952038764954\n",
      "Iteration: 13967, Loss: 0.746595025062561\n",
      "Iteration: 13968, Loss: 0.7465946078300476\n",
      "Iteration: 13969, Loss: 0.7465943098068237\n",
      "Iteration: 13970, Loss: 0.746594250202179\n",
      "Iteration: 13971, Loss: 0.7465938925743103\n",
      "Iteration: 13972, Loss: 0.7465934753417969\n",
      "Iteration: 13973, Loss: 0.7465933561325073\n",
      "Iteration: 13974, Loss: 0.7465929985046387\n",
      "Iteration: 13975, Loss: 0.7465925216674805\n",
      "Iteration: 13976, Loss: 0.7465922236442566\n",
      "Iteration: 13977, Loss: 0.7465918660163879\n",
      "Iteration: 13978, Loss: 0.7465916872024536\n",
      "Iteration: 13979, Loss: 0.746591329574585\n",
      "Iteration: 13980, Loss: 0.7465909719467163\n",
      "Iteration: 13981, Loss: 0.7465906739234924\n",
      "Iteration: 13982, Loss: 0.7465903162956238\n",
      "Iteration: 13983, Loss: 0.7465900182723999\n",
      "Iteration: 13984, Loss: 0.7465896606445312\n",
      "Iteration: 13985, Loss: 0.7465893626213074\n",
      "Iteration: 13986, Loss: 0.7465890049934387\n",
      "Iteration: 13987, Loss: 0.7465886473655701\n",
      "Iteration: 13988, Loss: 0.7465883493423462\n",
      "Iteration: 13989, Loss: 0.7465879917144775\n",
      "Iteration: 13990, Loss: 0.7465876936912537\n",
      "Iteration: 13991, Loss: 0.746587336063385\n",
      "Iteration: 13992, Loss: 0.7465870380401611\n",
      "Iteration: 13993, Loss: 0.7465866804122925\n",
      "Iteration: 13994, Loss: 0.7465863823890686\n",
      "Iteration: 13995, Loss: 0.7465860247612\n",
      "Iteration: 13996, Loss: 0.7465856671333313\n",
      "Iteration: 13997, Loss: 0.7465853691101074\n",
      "Iteration: 13998, Loss: 0.746584951877594\n",
      "Iteration: 13999, Loss: 0.7465846538543701\n",
      "Iteration: 14000, Loss: 0.7465842962265015\n",
      "Iteration: 14001, Loss: 0.7465839982032776\n",
      "Iteration: 14002, Loss: 0.7465836405754089\n",
      "Iteration: 14003, Loss: 0.7465833425521851\n",
      "Iteration: 14004, Loss: 0.7465829849243164\n",
      "Iteration: 14005, Loss: 0.7465826869010925\n",
      "Iteration: 14006, Loss: 0.7465823292732239\n",
      "Iteration: 14007, Loss: 0.7465819716453552\n",
      "Iteration: 14008, Loss: 0.7465816736221313\n",
      "Iteration: 14009, Loss: 0.7465813159942627\n",
      "Iteration: 14010, Loss: 0.7465810179710388\n",
      "Iteration: 14011, Loss: 0.7465806603431702\n",
      "Iteration: 14012, Loss: 0.7465803623199463\n",
      "Iteration: 14013, Loss: 0.7465800046920776\n",
      "Iteration: 14014, Loss: 0.7465797066688538\n",
      "Iteration: 14015, Loss: 0.7465793490409851\n",
      "Iteration: 14016, Loss: 0.7465791702270508\n",
      "Iteration: 14017, Loss: 0.7465788125991821\n",
      "Iteration: 14018, Loss: 0.7465785145759583\n",
      "Iteration: 14019, Loss: 0.7465781569480896\n",
      "Iteration: 14020, Loss: 0.7465778589248657\n",
      "Iteration: 14021, Loss: 0.7465775012969971\n",
      "Iteration: 14022, Loss: 0.7465772032737732\n",
      "Iteration: 14023, Loss: 0.7465768456459045\n",
      "Iteration: 14024, Loss: 0.7465764880180359\n",
      "Iteration: 14025, Loss: 0.746576189994812\n",
      "Iteration: 14026, Loss: 0.7465757727622986\n",
      "Iteration: 14027, Loss: 0.7465754747390747\n",
      "Iteration: 14028, Loss: 0.746575117111206\n",
      "Iteration: 14029, Loss: 0.7465748190879822\n",
      "Iteration: 14030, Loss: 0.7465745210647583\n",
      "Iteration: 14031, Loss: 0.7465742230415344\n",
      "Iteration: 14032, Loss: 0.7465738654136658\n",
      "Iteration: 14033, Loss: 0.7465735673904419\n",
      "Iteration: 14034, Loss: 0.7465732097625732\n",
      "Iteration: 14035, Loss: 0.7465729117393494\n",
      "Iteration: 14036, Loss: 0.7465725541114807\n",
      "Iteration: 14037, Loss: 0.7465722560882568\n",
      "Iteration: 14038, Loss: 0.7465718984603882\n",
      "Iteration: 14039, Loss: 0.7465716600418091\n",
      "Iteration: 14040, Loss: 0.74657142162323\n",
      "Iteration: 14041, Loss: 0.7465711236000061\n",
      "Iteration: 14042, Loss: 0.7465707659721375\n",
      "Iteration: 14043, Loss: 0.7465704679489136\n",
      "Iteration: 14044, Loss: 0.7465701103210449\n",
      "Iteration: 14045, Loss: 0.746569812297821\n",
      "Iteration: 14046, Loss: 0.7465694546699524\n",
      "Iteration: 14047, Loss: 0.7465691566467285\n",
      "Iteration: 14048, Loss: 0.7465687990188599\n",
      "Iteration: 14049, Loss: 0.746568500995636\n",
      "Iteration: 14050, Loss: 0.7465683221817017\n",
      "Iteration: 14051, Loss: 0.746567964553833\n",
      "Iteration: 14052, Loss: 0.7465676665306091\n",
      "Iteration: 14053, Loss: 0.7465673089027405\n",
      "Iteration: 14054, Loss: 0.7465670108795166\n",
      "Iteration: 14055, Loss: 0.7465667128562927\n",
      "Iteration: 14056, Loss: 0.7465663552284241\n",
      "Iteration: 14057, Loss: 0.7465660572052002\n",
      "Iteration: 14058, Loss: 0.7465658783912659\n",
      "Iteration: 14059, Loss: 0.746565580368042\n",
      "Iteration: 14060, Loss: 0.7465652227401733\n",
      "Iteration: 14061, Loss: 0.7465649247169495\n",
      "Iteration: 14062, Loss: 0.7465646266937256\n",
      "Iteration: 14063, Loss: 0.7465643882751465\n",
      "Iteration: 14064, Loss: 0.7465639710426331\n",
      "Iteration: 14065, Loss: 0.7465639114379883\n",
      "Iteration: 14066, Loss: 0.7465634942054749\n",
      "Iteration: 14067, Loss: 0.746563196182251\n",
      "Iteration: 14068, Loss: 0.7465628981590271\n",
      "Iteration: 14069, Loss: 0.7465625405311584\n",
      "Iteration: 14070, Loss: 0.7465622425079346\n",
      "Iteration: 14071, Loss: 0.7465617656707764\n",
      "Iteration: 14072, Loss: 0.7465614676475525\n",
      "Iteration: 14073, Loss: 0.7465611696243286\n",
      "Iteration: 14074, Loss: 0.7465609908103943\n",
      "Iteration: 14075, Loss: 0.7465606927871704\n",
      "Iteration: 14076, Loss: 0.7465603947639465\n",
      "Iteration: 14077, Loss: 0.7465600967407227\n",
      "Iteration: 14078, Loss: 0.746559739112854\n",
      "Iteration: 14079, Loss: 0.7465595006942749\n",
      "Iteration: 14080, Loss: 0.746559202671051\n",
      "Iteration: 14081, Loss: 0.7465589046478271\n",
      "Iteration: 14082, Loss: 0.7465586066246033\n",
      "Iteration: 14083, Loss: 0.7465583086013794\n",
      "Iteration: 14084, Loss: 0.7465580701828003\n",
      "Iteration: 14085, Loss: 0.7465575337409973\n",
      "Iteration: 14086, Loss: 0.7465571761131287\n",
      "Iteration: 14087, Loss: 0.7465568780899048\n",
      "Iteration: 14088, Loss: 0.7465566396713257\n",
      "Iteration: 14089, Loss: 0.746556282043457\n",
      "Iteration: 14090, Loss: 0.7465561032295227\n",
      "Iteration: 14091, Loss: 0.7465558648109436\n",
      "Iteration: 14092, Loss: 0.7465553879737854\n",
      "Iteration: 14093, Loss: 0.7465551495552063\n",
      "Iteration: 14094, Loss: 0.7465548515319824\n",
      "Iteration: 14095, Loss: 0.7465545535087585\n",
      "Iteration: 14096, Loss: 0.7465543150901794\n",
      "Iteration: 14097, Loss: 0.7465539574623108\n",
      "Iteration: 14098, Loss: 0.7465535402297974\n",
      "Iteration: 14099, Loss: 0.7465532422065735\n",
      "Iteration: 14100, Loss: 0.7465530633926392\n",
      "Iteration: 14101, Loss: 0.7465527653694153\n",
      "Iteration: 14102, Loss: 0.7465522885322571\n",
      "Iteration: 14103, Loss: 0.7465521097183228\n",
      "Iteration: 14104, Loss: 0.7465518116950989\n",
      "Iteration: 14105, Loss: 0.746551513671875\n",
      "Iteration: 14106, Loss: 0.7465512156486511\n",
      "Iteration: 14107, Loss: 0.7465508580207825\n",
      "Iteration: 14108, Loss: 0.7465505599975586\n",
      "Iteration: 14109, Loss: 0.7465502619743347\n",
      "Iteration: 14110, Loss: 0.7465499639511108\n",
      "Iteration: 14111, Loss: 0.7465496063232422\n",
      "Iteration: 14112, Loss: 0.7465493083000183\n",
      "Iteration: 14113, Loss: 0.7465490102767944\n",
      "Iteration: 14114, Loss: 0.7465487122535706\n",
      "Iteration: 14115, Loss: 0.7465484142303467\n",
      "Iteration: 14116, Loss: 0.7465481758117676\n",
      "Iteration: 14117, Loss: 0.7465478777885437\n",
      "Iteration: 14118, Loss: 0.7465475797653198\n",
      "Iteration: 14119, Loss: 0.746547281742096\n",
      "Iteration: 14120, Loss: 0.7465469837188721\n",
      "Iteration: 14121, Loss: 0.7465466856956482\n",
      "Iteration: 14122, Loss: 0.7465463876724243\n",
      "Iteration: 14123, Loss: 0.7465460300445557\n",
      "Iteration: 14124, Loss: 0.7465457320213318\n",
      "Iteration: 14125, Loss: 0.7465454339981079\n",
      "Iteration: 14126, Loss: 0.746545135974884\n",
      "Iteration: 14127, Loss: 0.7465448975563049\n",
      "Iteration: 14128, Loss: 0.746544599533081\n",
      "Iteration: 14129, Loss: 0.7465443015098572\n",
      "Iteration: 14130, Loss: 0.7465440034866333\n",
      "Iteration: 14131, Loss: 0.7465436458587646\n",
      "Iteration: 14132, Loss: 0.7465433478355408\n",
      "Iteration: 14133, Loss: 0.7465430498123169\n",
      "Iteration: 14134, Loss: 0.7465428113937378\n",
      "Iteration: 14135, Loss: 0.7465425133705139\n",
      "Iteration: 14136, Loss: 0.74654221534729\n",
      "Iteration: 14137, Loss: 0.7465419173240662\n",
      "Iteration: 14138, Loss: 0.7465416193008423\n",
      "Iteration: 14139, Loss: 0.7465413212776184\n",
      "Iteration: 14140, Loss: 0.7465412020683289\n",
      "Iteration: 14141, Loss: 0.746540904045105\n",
      "Iteration: 14142, Loss: 0.7465406060218811\n",
      "Iteration: 14143, Loss: 0.7465403079986572\n",
      "Iteration: 14144, Loss: 0.7465400099754333\n",
      "Iteration: 14145, Loss: 0.7465397119522095\n",
      "Iteration: 14146, Loss: 0.7465394139289856\n",
      "Iteration: 14147, Loss: 0.7465391159057617\n",
      "Iteration: 14148, Loss: 0.7465388178825378\n",
      "Iteration: 14149, Loss: 0.7465385794639587\n",
      "Iteration: 14150, Loss: 0.7465382814407349\n",
      "Iteration: 14151, Loss: 0.746537983417511\n",
      "Iteration: 14152, Loss: 0.7465377449989319\n",
      "Iteration: 14153, Loss: 0.746537446975708\n",
      "Iteration: 14154, Loss: 0.7465371489524841\n",
      "Iteration: 14155, Loss: 0.7465368509292603\n",
      "Iteration: 14156, Loss: 0.7465364933013916\n",
      "Iteration: 14157, Loss: 0.7465361952781677\n",
      "Iteration: 14158, Loss: 0.7465359568595886\n",
      "Iteration: 14159, Loss: 0.7465356588363647\n",
      "Iteration: 14160, Loss: 0.7465354204177856\n",
      "Iteration: 14161, Loss: 0.7465351223945618\n",
      "Iteration: 14162, Loss: 0.7465348243713379\n",
      "Iteration: 14163, Loss: 0.746534526348114\n",
      "Iteration: 14164, Loss: 0.7465342283248901\n",
      "Iteration: 14165, Loss: 0.7465339303016663\n",
      "Iteration: 14166, Loss: 0.7465336918830872\n",
      "Iteration: 14167, Loss: 0.7465333938598633\n",
      "Iteration: 14168, Loss: 0.7465330958366394\n",
      "Iteration: 14169, Loss: 0.7465327978134155\n",
      "Iteration: 14170, Loss: 0.7465324997901917\n",
      "Iteration: 14171, Loss: 0.7465322017669678\n",
      "Iteration: 14172, Loss: 0.7465319633483887\n",
      "Iteration: 14173, Loss: 0.7465316653251648\n",
      "Iteration: 14174, Loss: 0.7465313673019409\n",
      "Iteration: 14175, Loss: 0.7465310096740723\n",
      "Iteration: 14176, Loss: 0.7465307116508484\n",
      "Iteration: 14177, Loss: 0.7465305924415588\n",
      "Iteration: 14178, Loss: 0.7465304136276245\n",
      "Iteration: 14179, Loss: 0.7465301156044006\n",
      "Iteration: 14180, Loss: 0.7465298175811768\n",
      "Iteration: 14181, Loss: 0.7465295791625977\n",
      "Iteration: 14182, Loss: 0.7465292811393738\n",
      "Iteration: 14183, Loss: 0.7465289831161499\n",
      "Iteration: 14184, Loss: 0.7465287446975708\n",
      "Iteration: 14185, Loss: 0.7465284466743469\n",
      "Iteration: 14186, Loss: 0.746528148651123\n",
      "Iteration: 14187, Loss: 0.746527910232544\n",
      "Iteration: 14188, Loss: 0.7465276122093201\n",
      "Iteration: 14189, Loss: 0.7465272545814514\n",
      "Iteration: 14190, Loss: 0.7465269565582275\n",
      "Iteration: 14191, Loss: 0.7465266585350037\n",
      "Iteration: 14192, Loss: 0.7465263605117798\n",
      "Iteration: 14193, Loss: 0.7465260624885559\n",
      "Iteration: 14194, Loss: 0.7465257048606873\n",
      "Iteration: 14195, Loss: 0.7465254068374634\n",
      "Iteration: 14196, Loss: 0.7465251088142395\n",
      "Iteration: 14197, Loss: 0.7465248107910156\n",
      "Iteration: 14198, Loss: 0.7465245723724365\n",
      "Iteration: 14199, Loss: 0.746524453163147\n",
      "Iteration: 14200, Loss: 0.7465240955352783\n",
      "Iteration: 14201, Loss: 0.7465238571166992\n",
      "Iteration: 14202, Loss: 0.7465236186981201\n",
      "Iteration: 14203, Loss: 0.7465233206748962\n",
      "Iteration: 14204, Loss: 0.7465230822563171\n",
      "Iteration: 14205, Loss: 0.7465227842330933\n",
      "Iteration: 14206, Loss: 0.7465224862098694\n",
      "Iteration: 14207, Loss: 0.7465221881866455\n",
      "Iteration: 14208, Loss: 0.7465218901634216\n",
      "Iteration: 14209, Loss: 0.7465217709541321\n",
      "Iteration: 14210, Loss: 0.7465214729309082\n",
      "Iteration: 14211, Loss: 0.7465211749076843\n",
      "Iteration: 14212, Loss: 0.7465208768844604\n",
      "Iteration: 14213, Loss: 0.7465206384658813\n",
      "Iteration: 14214, Loss: 0.7465203404426575\n",
      "Iteration: 14215, Loss: 0.7465200424194336\n",
      "Iteration: 14216, Loss: 0.7465198040008545\n",
      "Iteration: 14217, Loss: 0.7465195655822754\n",
      "Iteration: 14218, Loss: 0.7465192675590515\n",
      "Iteration: 14219, Loss: 0.7465189099311829\n",
      "Iteration: 14220, Loss: 0.7465186715126038\n",
      "Iteration: 14221, Loss: 0.7465183734893799\n",
      "Iteration: 14222, Loss: 0.746518075466156\n",
      "Iteration: 14223, Loss: 0.7465178370475769\n",
      "Iteration: 14224, Loss: 0.7465175986289978\n",
      "Iteration: 14225, Loss: 0.7465172410011292\n",
      "Iteration: 14226, Loss: 0.74651700258255\n",
      "Iteration: 14227, Loss: 0.7465167045593262\n",
      "Iteration: 14228, Loss: 0.7465164661407471\n",
      "Iteration: 14229, Loss: 0.7465161681175232\n",
      "Iteration: 14230, Loss: 0.7465158700942993\n",
      "Iteration: 14231, Loss: 0.746515691280365\n",
      "Iteration: 14232, Loss: 0.7465153932571411\n",
      "Iteration: 14233, Loss: 0.7465150952339172\n",
      "Iteration: 14234, Loss: 0.7465148568153381\n",
      "Iteration: 14235, Loss: 0.7465145587921143\n",
      "Iteration: 14236, Loss: 0.7465143203735352\n",
      "Iteration: 14237, Loss: 0.7465141415596008\n",
      "Iteration: 14238, Loss: 0.7465139031410217\n",
      "Iteration: 14239, Loss: 0.7465136051177979\n",
      "Iteration: 14240, Loss: 0.7465133666992188\n",
      "Iteration: 14241, Loss: 0.7465130686759949\n",
      "Iteration: 14242, Loss: 0.746512770652771\n",
      "Iteration: 14243, Loss: 0.7465125918388367\n",
      "Iteration: 14244, Loss: 0.7465122938156128\n",
      "Iteration: 14245, Loss: 0.7465120553970337\n",
      "Iteration: 14246, Loss: 0.7465117573738098\n",
      "Iteration: 14247, Loss: 0.7465115189552307\n",
      "Iteration: 14248, Loss: 0.7465112209320068\n",
      "Iteration: 14249, Loss: 0.7465109825134277\n",
      "Iteration: 14250, Loss: 0.7465107440948486\n",
      "Iteration: 14251, Loss: 0.7465104460716248\n",
      "Iteration: 14252, Loss: 0.7465102672576904\n",
      "Iteration: 14253, Loss: 0.7465099692344666\n",
      "Iteration: 14254, Loss: 0.7465096712112427\n",
      "Iteration: 14255, Loss: 0.7465093731880188\n",
      "Iteration: 14256, Loss: 0.7465091347694397\n",
      "Iteration: 14257, Loss: 0.7465088367462158\n",
      "Iteration: 14258, Loss: 0.7465085983276367\n",
      "Iteration: 14259, Loss: 0.7465084195137024\n",
      "Iteration: 14260, Loss: 0.7465081810951233\n",
      "Iteration: 14261, Loss: 0.7465079426765442\n",
      "Iteration: 14262, Loss: 0.7465076446533203\n",
      "Iteration: 14263, Loss: 0.7465074062347412\n",
      "Iteration: 14264, Loss: 0.7465071678161621\n",
      "Iteration: 14265, Loss: 0.746506929397583\n",
      "Iteration: 14266, Loss: 0.7465066313743591\n",
      "Iteration: 14267, Loss: 0.74650639295578\n",
      "Iteration: 14268, Loss: 0.7465061545372009\n",
      "Iteration: 14269, Loss: 0.746505856513977\n",
      "Iteration: 14270, Loss: 0.746505618095398\n",
      "Iteration: 14271, Loss: 0.7465053200721741\n",
      "Iteration: 14272, Loss: 0.7465051412582397\n",
      "Iteration: 14273, Loss: 0.7465048432350159\n",
      "Iteration: 14274, Loss: 0.746504545211792\n",
      "Iteration: 14275, Loss: 0.7465043663978577\n",
      "Iteration: 14276, Loss: 0.7465041279792786\n",
      "Iteration: 14277, Loss: 0.7465039491653442\n",
      "Iteration: 14278, Loss: 0.7465037107467651\n",
      "Iteration: 14279, Loss: 0.7465032935142517\n",
      "Iteration: 14280, Loss: 0.7465030550956726\n",
      "Iteration: 14281, Loss: 0.7465028166770935\n",
      "Iteration: 14282, Loss: 0.7465025782585144\n",
      "Iteration: 14283, Loss: 0.746502161026001\n",
      "Iteration: 14284, Loss: 0.7465019822120667\n",
      "Iteration: 14285, Loss: 0.7465018033981323\n",
      "Iteration: 14286, Loss: 0.7465014457702637\n",
      "Iteration: 14287, Loss: 0.7465012073516846\n",
      "Iteration: 14288, Loss: 0.7465009093284607\n",
      "Iteration: 14289, Loss: 0.7465006709098816\n",
      "Iteration: 14290, Loss: 0.746500551700592\n",
      "Iteration: 14291, Loss: 0.7465002536773682\n",
      "Iteration: 14292, Loss: 0.7465000748634338\n",
      "Iteration: 14293, Loss: 0.74649977684021\n",
      "Iteration: 14294, Loss: 0.7464995980262756\n",
      "Iteration: 14295, Loss: 0.7464993000030518\n",
      "Iteration: 14296, Loss: 0.7464990615844727\n",
      "Iteration: 14297, Loss: 0.746498703956604\n",
      "Iteration: 14298, Loss: 0.7464984059333801\n",
      "Iteration: 14299, Loss: 0.746498167514801\n",
      "Iteration: 14300, Loss: 0.7464978694915771\n",
      "Iteration: 14301, Loss: 0.746497631072998\n",
      "Iteration: 14302, Loss: 0.7464974522590637\n",
      "Iteration: 14303, Loss: 0.7464972138404846\n",
      "Iteration: 14304, Loss: 0.7464970946311951\n",
      "Iteration: 14305, Loss: 0.746496856212616\n",
      "Iteration: 14306, Loss: 0.7464965581893921\n",
      "Iteration: 14307, Loss: 0.746496319770813\n",
      "Iteration: 14308, Loss: 0.7464961409568787\n",
      "Iteration: 14309, Loss: 0.7464958429336548\n",
      "Iteration: 14310, Loss: 0.7464956045150757\n",
      "Iteration: 14311, Loss: 0.7464953660964966\n",
      "Iteration: 14312, Loss: 0.7464950680732727\n",
      "Iteration: 14313, Loss: 0.7464948296546936\n",
      "Iteration: 14314, Loss: 0.7464945912361145\n",
      "Iteration: 14315, Loss: 0.7464943528175354\n",
      "Iteration: 14316, Loss: 0.7464940547943115\n",
      "Iteration: 14317, Loss: 0.7464938163757324\n",
      "Iteration: 14318, Loss: 0.7464936971664429\n",
      "Iteration: 14319, Loss: 0.7464934587478638\n",
      "Iteration: 14320, Loss: 0.7464931607246399\n",
      "Iteration: 14321, Loss: 0.7464929223060608\n",
      "Iteration: 14322, Loss: 0.7464928030967712\n",
      "Iteration: 14323, Loss: 0.7464925646781921\n",
      "Iteration: 14324, Loss: 0.746492326259613\n",
      "Iteration: 14325, Loss: 0.7464920878410339\n",
      "Iteration: 14326, Loss: 0.7464918494224548\n",
      "Iteration: 14327, Loss: 0.746491551399231\n",
      "Iteration: 14328, Loss: 0.7464913129806519\n",
      "Iteration: 14329, Loss: 0.7464910745620728\n",
      "Iteration: 14330, Loss: 0.7464908361434937\n",
      "Iteration: 14331, Loss: 0.7464905977249146\n",
      "Iteration: 14332, Loss: 0.7464904189109802\n",
      "Iteration: 14333, Loss: 0.7464901804924011\n",
      "Iteration: 14334, Loss: 0.746489942073822\n",
      "Iteration: 14335, Loss: 0.7464897036552429\n",
      "Iteration: 14336, Loss: 0.7464894652366638\n",
      "Iteration: 14337, Loss: 0.7464891672134399\n",
      "Iteration: 14338, Loss: 0.7464889287948608\n",
      "Iteration: 14339, Loss: 0.7464887499809265\n",
      "Iteration: 14340, Loss: 0.7464885115623474\n",
      "Iteration: 14341, Loss: 0.7464882731437683\n",
      "Iteration: 14342, Loss: 0.7464880347251892\n",
      "Iteration: 14343, Loss: 0.7464877963066101\n",
      "Iteration: 14344, Loss: 0.746487557888031\n",
      "Iteration: 14345, Loss: 0.7464873194694519\n",
      "Iteration: 14346, Loss: 0.7464872598648071\n",
      "Iteration: 14347, Loss: 0.7464870810508728\n",
      "Iteration: 14348, Loss: 0.7464868426322937\n",
      "Iteration: 14349, Loss: 0.7464866042137146\n",
      "Iteration: 14350, Loss: 0.7464863657951355\n",
      "Iteration: 14351, Loss: 0.7464861869812012\n",
      "Iteration: 14352, Loss: 0.7464859485626221\n",
      "Iteration: 14353, Loss: 0.746485710144043\n",
      "Iteration: 14354, Loss: 0.7464854717254639\n",
      "Iteration: 14355, Loss: 0.7464852333068848\n",
      "Iteration: 14356, Loss: 0.7464850544929504\n",
      "Iteration: 14357, Loss: 0.7464848160743713\n",
      "Iteration: 14358, Loss: 0.7464845776557922\n",
      "Iteration: 14359, Loss: 0.7464843392372131\n",
      "Iteration: 14360, Loss: 0.746484100818634\n",
      "Iteration: 14361, Loss: 0.7464838624000549\n",
      "Iteration: 14362, Loss: 0.7464836239814758\n",
      "Iteration: 14363, Loss: 0.7464833855628967\n",
      "Iteration: 14364, Loss: 0.7464832067489624\n",
      "Iteration: 14365, Loss: 0.7464829683303833\n",
      "Iteration: 14366, Loss: 0.746482789516449\n",
      "Iteration: 14367, Loss: 0.746482789516449\n",
      "Iteration: 14368, Loss: 0.7464825510978699\n",
      "Iteration: 14369, Loss: 0.7464821934700012\n",
      "Iteration: 14370, Loss: 0.7464819550514221\n",
      "Iteration: 14371, Loss: 0.746481716632843\n",
      "Iteration: 14372, Loss: 0.7464815974235535\n",
      "Iteration: 14373, Loss: 0.7464814186096191\n",
      "Iteration: 14374, Loss: 0.74648118019104\n",
      "Iteration: 14375, Loss: 0.7464809417724609\n",
      "Iteration: 14376, Loss: 0.7464807629585266\n",
      "Iteration: 14377, Loss: 0.7464805841445923\n",
      "Iteration: 14378, Loss: 0.7464803457260132\n",
      "Iteration: 14379, Loss: 0.7464800477027893\n",
      "Iteration: 14380, Loss: 0.746479868888855\n",
      "Iteration: 14381, Loss: 0.7464796304702759\n",
      "Iteration: 14382, Loss: 0.7464793920516968\n",
      "Iteration: 14383, Loss: 0.746479332447052\n",
      "Iteration: 14384, Loss: 0.7464792132377625\n",
      "Iteration: 14385, Loss: 0.7464789748191833\n",
      "Iteration: 14386, Loss: 0.746478796005249\n",
      "Iteration: 14387, Loss: 0.7464785575866699\n",
      "Iteration: 14388, Loss: 0.7464783787727356\n",
      "Iteration: 14389, Loss: 0.746478259563446\n",
      "Iteration: 14390, Loss: 0.7464780211448669\n",
      "Iteration: 14391, Loss: 0.7464778423309326\n",
      "Iteration: 14392, Loss: 0.746477484703064\n",
      "Iteration: 14393, Loss: 0.7464772462844849\n",
      "Iteration: 14394, Loss: 0.7464770078659058\n",
      "Iteration: 14395, Loss: 0.7464767694473267\n",
      "Iteration: 14396, Loss: 0.7464765906333923\n",
      "Iteration: 14397, Loss: 0.7464763522148132\n",
      "Iteration: 14398, Loss: 0.7464759945869446\n",
      "Iteration: 14399, Loss: 0.7464759349822998\n",
      "Iteration: 14400, Loss: 0.7464756965637207\n",
      "Iteration: 14401, Loss: 0.7464755177497864\n",
      "Iteration: 14402, Loss: 0.7464752793312073\n",
      "Iteration: 14403, Loss: 0.7464750409126282\n",
      "Iteration: 14404, Loss: 0.7464748620986938\n",
      "Iteration: 14405, Loss: 0.7464746832847595\n",
      "Iteration: 14406, Loss: 0.7464745044708252\n",
      "Iteration: 14407, Loss: 0.7464742660522461\n",
      "Iteration: 14408, Loss: 0.746474027633667\n",
      "Iteration: 14409, Loss: 0.7464738488197327\n",
      "Iteration: 14410, Loss: 0.7464736104011536\n",
      "Iteration: 14411, Loss: 0.7464733719825745\n",
      "Iteration: 14412, Loss: 0.7464731931686401\n",
      "Iteration: 14413, Loss: 0.746472954750061\n",
      "Iteration: 14414, Loss: 0.7464727163314819\n",
      "Iteration: 14415, Loss: 0.7464725971221924\n",
      "Iteration: 14416, Loss: 0.7464723587036133\n",
      "Iteration: 14417, Loss: 0.7464721202850342\n",
      "Iteration: 14418, Loss: 0.7464720010757446\n",
      "Iteration: 14419, Loss: 0.7464717626571655\n",
      "Iteration: 14420, Loss: 0.746471643447876\n",
      "Iteration: 14421, Loss: 0.7464714050292969\n",
      "Iteration: 14422, Loss: 0.746471107006073\n",
      "Iteration: 14423, Loss: 0.7464709877967834\n",
      "Iteration: 14424, Loss: 0.7464707493782043\n",
      "Iteration: 14425, Loss: 0.7464705109596252\n",
      "Iteration: 14426, Loss: 0.7464703321456909\n",
      "Iteration: 14427, Loss: 0.7464700937271118\n",
      "Iteration: 14428, Loss: 0.7464698553085327\n",
      "Iteration: 14429, Loss: 0.7464697360992432\n",
      "Iteration: 14430, Loss: 0.7464694976806641\n",
      "Iteration: 14431, Loss: 0.7464693188667297\n",
      "Iteration: 14432, Loss: 0.7464690804481506\n",
      "Iteration: 14433, Loss: 0.7464688420295715\n",
      "Iteration: 14434, Loss: 0.746468722820282\n",
      "Iteration: 14435, Loss: 0.7464684844017029\n",
      "Iteration: 14436, Loss: 0.7464682459831238\n",
      "Iteration: 14437, Loss: 0.7464681267738342\n",
      "Iteration: 14438, Loss: 0.7464678883552551\n",
      "Iteration: 14439, Loss: 0.7464677095413208\n",
      "Iteration: 14440, Loss: 0.7464674711227417\n",
      "Iteration: 14441, Loss: 0.7464672923088074\n",
      "Iteration: 14442, Loss: 0.7464670538902283\n",
      "Iteration: 14443, Loss: 0.7464669346809387\n",
      "Iteration: 14444, Loss: 0.7464667558670044\n",
      "Iteration: 14445, Loss: 0.7464665174484253\n",
      "Iteration: 14446, Loss: 0.746466338634491\n",
      "Iteration: 14447, Loss: 0.7464661002159119\n",
      "Iteration: 14448, Loss: 0.7464659214019775\n",
      "Iteration: 14449, Loss: 0.7464656829833984\n",
      "Iteration: 14450, Loss: 0.7464655041694641\n",
      "Iteration: 14451, Loss: 0.746465265750885\n",
      "Iteration: 14452, Loss: 0.7464650869369507\n",
      "Iteration: 14453, Loss: 0.7464649081230164\n",
      "Iteration: 14454, Loss: 0.746464729309082\n",
      "Iteration: 14455, Loss: 0.7464644908905029\n",
      "Iteration: 14456, Loss: 0.7464643716812134\n",
      "Iteration: 14457, Loss: 0.746464192867279\n",
      "Iteration: 14458, Loss: 0.7464640140533447\n",
      "Iteration: 14459, Loss: 0.7464638352394104\n",
      "Iteration: 14460, Loss: 0.7464635968208313\n",
      "Iteration: 14461, Loss: 0.746463418006897\n",
      "Iteration: 14462, Loss: 0.7464632391929626\n",
      "Iteration: 14463, Loss: 0.7464631199836731\n",
      "Iteration: 14464, Loss: 0.7464630603790283\n",
      "Iteration: 14465, Loss: 0.7464628219604492\n",
      "Iteration: 14466, Loss: 0.7464627623558044\n",
      "Iteration: 14467, Loss: 0.7464625239372253\n",
      "Iteration: 14468, Loss: 0.7464622855186462\n",
      "Iteration: 14469, Loss: 0.7464621067047119\n",
      "Iteration: 14470, Loss: 0.7464619278907776\n",
      "Iteration: 14471, Loss: 0.7464616894721985\n",
      "Iteration: 14472, Loss: 0.7464615106582642\n",
      "Iteration: 14473, Loss: 0.7464613318443298\n",
      "Iteration: 14474, Loss: 0.7464611530303955\n",
      "Iteration: 14475, Loss: 0.7464609146118164\n",
      "Iteration: 14476, Loss: 0.7464607954025269\n",
      "Iteration: 14477, Loss: 0.7464606165885925\n",
      "Iteration: 14478, Loss: 0.7464604377746582\n",
      "Iteration: 14479, Loss: 0.7464601993560791\n",
      "Iteration: 14480, Loss: 0.7464600205421448\n",
      "Iteration: 14481, Loss: 0.7464599013328552\n",
      "Iteration: 14482, Loss: 0.7464597225189209\n",
      "Iteration: 14483, Loss: 0.7464595437049866\n",
      "Iteration: 14484, Loss: 0.7464593648910522\n",
      "Iteration: 14485, Loss: 0.7464590072631836\n",
      "Iteration: 14486, Loss: 0.7464588284492493\n",
      "Iteration: 14487, Loss: 0.7464586496353149\n",
      "Iteration: 14488, Loss: 0.7464584708213806\n",
      "Iteration: 14489, Loss: 0.7464583516120911\n",
      "Iteration: 14490, Loss: 0.746458113193512\n",
      "Iteration: 14491, Loss: 0.7464579343795776\n",
      "Iteration: 14492, Loss: 0.7464577555656433\n",
      "Iteration: 14493, Loss: 0.746457576751709\n",
      "Iteration: 14494, Loss: 0.7464573979377747\n",
      "Iteration: 14495, Loss: 0.7464572191238403\n",
      "Iteration: 14496, Loss: 0.7464569807052612\n",
      "Iteration: 14497, Loss: 0.7464568018913269\n",
      "Iteration: 14498, Loss: 0.7464567422866821\n",
      "Iteration: 14499, Loss: 0.7464565634727478\n",
      "Iteration: 14500, Loss: 0.7464564442634583\n",
      "Iteration: 14501, Loss: 0.7464562058448792\n",
      "Iteration: 14502, Loss: 0.7464560270309448\n",
      "Iteration: 14503, Loss: 0.7464558482170105\n",
      "Iteration: 14504, Loss: 0.7464556694030762\n",
      "Iteration: 14505, Loss: 0.7464554905891418\n",
      "Iteration: 14506, Loss: 0.7464553713798523\n",
      "Iteration: 14507, Loss: 0.7464551329612732\n",
      "Iteration: 14508, Loss: 0.7464549541473389\n",
      "Iteration: 14509, Loss: 0.7464547753334045\n",
      "Iteration: 14510, Loss: 0.7464545965194702\n",
      "Iteration: 14511, Loss: 0.7464543581008911\n",
      "Iteration: 14512, Loss: 0.7464541792869568\n",
      "Iteration: 14513, Loss: 0.7464539408683777\n",
      "Iteration: 14514, Loss: 0.7464537620544434\n",
      "Iteration: 14515, Loss: 0.746453583240509\n",
      "Iteration: 14516, Loss: 0.7464534044265747\n",
      "Iteration: 14517, Loss: 0.7464532256126404\n",
      "Iteration: 14518, Loss: 0.7464529871940613\n",
      "Iteration: 14519, Loss: 0.746452808380127\n",
      "Iteration: 14520, Loss: 0.7464526295661926\n",
      "Iteration: 14521, Loss: 0.7464525103569031\n",
      "Iteration: 14522, Loss: 0.7464524507522583\n",
      "Iteration: 14523, Loss: 0.7464522123336792\n",
      "Iteration: 14524, Loss: 0.7464520335197449\n",
      "Iteration: 14525, Loss: 0.7464518547058105\n",
      "Iteration: 14526, Loss: 0.7464516758918762\n",
      "Iteration: 14527, Loss: 0.7464515566825867\n",
      "Iteration: 14528, Loss: 0.7464513182640076\n",
      "Iteration: 14529, Loss: 0.7464511394500732\n",
      "Iteration: 14530, Loss: 0.7464509606361389\n",
      "Iteration: 14531, Loss: 0.7464508414268494\n",
      "Iteration: 14532, Loss: 0.746450662612915\n",
      "Iteration: 14533, Loss: 0.7464504837989807\n",
      "Iteration: 14534, Loss: 0.7464504241943359\n",
      "Iteration: 14535, Loss: 0.7464501857757568\n",
      "Iteration: 14536, Loss: 0.7464500069618225\n",
      "Iteration: 14537, Loss: 0.7464498281478882\n",
      "Iteration: 14538, Loss: 0.7464496493339539\n",
      "Iteration: 14539, Loss: 0.7464494705200195\n",
      "Iteration: 14540, Loss: 0.7464492917060852\n",
      "Iteration: 14541, Loss: 0.7464491128921509\n",
      "Iteration: 14542, Loss: 0.7464489340782166\n",
      "Iteration: 14543, Loss: 0.7464487552642822\n",
      "Iteration: 14544, Loss: 0.7464485764503479\n",
      "Iteration: 14545, Loss: 0.7464483380317688\n",
      "Iteration: 14546, Loss: 0.7464481592178345\n",
      "Iteration: 14547, Loss: 0.7464479804039001\n",
      "Iteration: 14548, Loss: 0.7464479207992554\n",
      "Iteration: 14549, Loss: 0.746447741985321\n",
      "Iteration: 14550, Loss: 0.7464475631713867\n",
      "Iteration: 14551, Loss: 0.7464473843574524\n",
      "Iteration: 14552, Loss: 0.7464472055435181\n",
      "Iteration: 14553, Loss: 0.7464470267295837\n",
      "Iteration: 14554, Loss: 0.7464468479156494\n",
      "Iteration: 14555, Loss: 0.7464469075202942\n",
      "Iteration: 14556, Loss: 0.7464467287063599\n",
      "Iteration: 14557, Loss: 0.7464465498924255\n",
      "Iteration: 14558, Loss: 0.7464463710784912\n",
      "Iteration: 14559, Loss: 0.7464461922645569\n",
      "Iteration: 14560, Loss: 0.7464460134506226\n",
      "Iteration: 14561, Loss: 0.7464458346366882\n",
      "Iteration: 14562, Loss: 0.7464456558227539\n",
      "Iteration: 14563, Loss: 0.7464454770088196\n",
      "Iteration: 14564, Loss: 0.7464452981948853\n",
      "Iteration: 14565, Loss: 0.7464451789855957\n",
      "Iteration: 14566, Loss: 0.7464450001716614\n",
      "Iteration: 14567, Loss: 0.746444821357727\n",
      "Iteration: 14568, Loss: 0.7464446425437927\n",
      "Iteration: 14569, Loss: 0.7464444637298584\n",
      "Iteration: 14570, Loss: 0.7464442849159241\n",
      "Iteration: 14571, Loss: 0.7464441061019897\n",
      "Iteration: 14572, Loss: 0.7464439868927002\n",
      "Iteration: 14573, Loss: 0.7464438080787659\n",
      "Iteration: 14574, Loss: 0.7464436292648315\n",
      "Iteration: 14575, Loss: 0.746443510055542\n",
      "Iteration: 14576, Loss: 0.7464433312416077\n",
      "Iteration: 14577, Loss: 0.7464432120323181\n",
      "Iteration: 14578, Loss: 0.7464430332183838\n",
      "Iteration: 14579, Loss: 0.7464428544044495\n",
      "Iteration: 14580, Loss: 0.7464426755905151\n",
      "Iteration: 14581, Loss: 0.7464424967765808\n",
      "Iteration: 14582, Loss: 0.7464423179626465\n",
      "Iteration: 14583, Loss: 0.7464422583580017\n",
      "Iteration: 14584, Loss: 0.7464420795440674\n",
      "Iteration: 14585, Loss: 0.7464419603347778\n",
      "Iteration: 14586, Loss: 0.7464417815208435\n",
      "Iteration: 14587, Loss: 0.7464416027069092\n",
      "Iteration: 14588, Loss: 0.7464414238929749\n",
      "Iteration: 14589, Loss: 0.7464412450790405\n",
      "Iteration: 14590, Loss: 0.7464410662651062\n",
      "Iteration: 14591, Loss: 0.7464409470558167\n",
      "Iteration: 14592, Loss: 0.7464407682418823\n",
      "Iteration: 14593, Loss: 0.746440589427948\n",
      "Iteration: 14594, Loss: 0.7464404106140137\n",
      "Iteration: 14595, Loss: 0.7464402318000793\n",
      "Iteration: 14596, Loss: 0.746440052986145\n",
      "Iteration: 14597, Loss: 0.7464399337768555\n",
      "Iteration: 14598, Loss: 0.7464397549629211\n",
      "Iteration: 14599, Loss: 0.7464395761489868\n",
      "Iteration: 14600, Loss: 0.7464393973350525\n",
      "Iteration: 14601, Loss: 0.7464392185211182\n",
      "Iteration: 14602, Loss: 0.7464390993118286\n",
      "Iteration: 14603, Loss: 0.7464389204978943\n",
      "Iteration: 14604, Loss: 0.74643874168396\n",
      "Iteration: 14605, Loss: 0.7464385628700256\n",
      "Iteration: 14606, Loss: 0.7464383840560913\n",
      "Iteration: 14607, Loss: 0.7464382648468018\n",
      "Iteration: 14608, Loss: 0.7464380860328674\n",
      "Iteration: 14609, Loss: 0.7464379072189331\n",
      "Iteration: 14610, Loss: 0.7464377284049988\n",
      "Iteration: 14611, Loss: 0.7464373111724854\n",
      "Iteration: 14612, Loss: 0.746437132358551\n",
      "Iteration: 14613, Loss: 0.7464370131492615\n",
      "Iteration: 14614, Loss: 0.7464368343353271\n",
      "Iteration: 14615, Loss: 0.7464366555213928\n",
      "Iteration: 14616, Loss: 0.746436595916748\n",
      "Iteration: 14617, Loss: 0.7464364767074585\n",
      "Iteration: 14618, Loss: 0.746436357498169\n",
      "Iteration: 14619, Loss: 0.7464361786842346\n",
      "Iteration: 14620, Loss: 0.7464361190795898\n",
      "Iteration: 14621, Loss: 0.7464358806610107\n",
      "Iteration: 14622, Loss: 0.7464357614517212\n",
      "Iteration: 14623, Loss: 0.7464356422424316\n",
      "Iteration: 14624, Loss: 0.7464354634284973\n",
      "Iteration: 14625, Loss: 0.746435284614563\n",
      "Iteration: 14626, Loss: 0.7464351058006287\n",
      "Iteration: 14627, Loss: 0.7464350461959839\n",
      "Iteration: 14628, Loss: 0.7464348673820496\n",
      "Iteration: 14629, Loss: 0.74643474817276\n",
      "Iteration: 14630, Loss: 0.7464345693588257\n",
      "Iteration: 14631, Loss: 0.7464344501495361\n",
      "Iteration: 14632, Loss: 0.7464342713356018\n",
      "Iteration: 14633, Loss: 0.7464340329170227\n",
      "Iteration: 14634, Loss: 0.7464338541030884\n",
      "Iteration: 14635, Loss: 0.7464339137077332\n",
      "Iteration: 14636, Loss: 0.7464337348937988\n",
      "Iteration: 14637, Loss: 0.7464335560798645\n",
      "Iteration: 14638, Loss: 0.746433436870575\n",
      "Iteration: 14639, Loss: 0.7464332580566406\n",
      "Iteration: 14640, Loss: 0.7464331388473511\n",
      "Iteration: 14641, Loss: 0.7464330196380615\n",
      "Iteration: 14642, Loss: 0.7464328408241272\n",
      "Iteration: 14643, Loss: 0.7464327812194824\n",
      "Iteration: 14644, Loss: 0.7464326024055481\n",
      "Iteration: 14645, Loss: 0.7464324831962585\n",
      "Iteration: 14646, Loss: 0.7464321255683899\n",
      "Iteration: 14647, Loss: 0.7464319467544556\n",
      "Iteration: 14648, Loss: 0.746431827545166\n",
      "Iteration: 14649, Loss: 0.7464316487312317\n",
      "Iteration: 14650, Loss: 0.7464315295219421\n",
      "Iteration: 14651, Loss: 0.7464314103126526\n",
      "Iteration: 14652, Loss: 0.746431291103363\n",
      "Iteration: 14653, Loss: 0.7464309334754944\n",
      "Iteration: 14654, Loss: 0.7464308142662048\n",
      "Iteration: 14655, Loss: 0.7464306354522705\n",
      "Iteration: 14656, Loss: 0.7464306354522705\n",
      "Iteration: 14657, Loss: 0.7464304566383362\n",
      "Iteration: 14658, Loss: 0.7464302778244019\n",
      "Iteration: 14659, Loss: 0.7464301586151123\n",
      "Iteration: 14660, Loss: 0.746429979801178\n",
      "Iteration: 14661, Loss: 0.7464298605918884\n",
      "Iteration: 14662, Loss: 0.7464297413825989\n",
      "Iteration: 14663, Loss: 0.7464295625686646\n",
      "Iteration: 14664, Loss: 0.7464293241500854\n",
      "Iteration: 14665, Loss: 0.7464292049407959\n",
      "Iteration: 14666, Loss: 0.7464290857315063\n",
      "Iteration: 14667, Loss: 0.746428906917572\n",
      "Iteration: 14668, Loss: 0.7464288473129272\n",
      "Iteration: 14669, Loss: 0.7464286684989929\n",
      "Iteration: 14670, Loss: 0.7464284896850586\n",
      "Iteration: 14671, Loss: 0.746428370475769\n",
      "Iteration: 14672, Loss: 0.7464281916618347\n",
      "Iteration: 14673, Loss: 0.7464280724525452\n",
      "Iteration: 14674, Loss: 0.7464279532432556\n",
      "Iteration: 14675, Loss: 0.7464278340339661\n",
      "Iteration: 14676, Loss: 0.7464276552200317\n",
      "Iteration: 14677, Loss: 0.7464275360107422\n",
      "Iteration: 14678, Loss: 0.7464273571968079\n",
      "Iteration: 14679, Loss: 0.7464271783828735\n",
      "Iteration: 14680, Loss: 0.746427059173584\n",
      "Iteration: 14681, Loss: 0.7464268803596497\n",
      "Iteration: 14682, Loss: 0.7464267611503601\n",
      "Iteration: 14683, Loss: 0.7464266419410706\n",
      "Iteration: 14684, Loss: 0.746426522731781\n",
      "Iteration: 14685, Loss: 0.7464264035224915\n",
      "Iteration: 14686, Loss: 0.7464262247085571\n",
      "Iteration: 14687, Loss: 0.7464260458946228\n",
      "Iteration: 14688, Loss: 0.7464259266853333\n",
      "Iteration: 14689, Loss: 0.7464257478713989\n",
      "Iteration: 14690, Loss: 0.7464256286621094\n",
      "Iteration: 14691, Loss: 0.7464255690574646\n",
      "Iteration: 14692, Loss: 0.7464253902435303\n",
      "Iteration: 14693, Loss: 0.7464252710342407\n",
      "Iteration: 14694, Loss: 0.7464250922203064\n",
      "Iteration: 14695, Loss: 0.7464249730110168\n",
      "Iteration: 14696, Loss: 0.7464247941970825\n",
      "Iteration: 14697, Loss: 0.746424674987793\n",
      "Iteration: 14698, Loss: 0.7464244961738586\n",
      "Iteration: 14699, Loss: 0.7464243769645691\n",
      "Iteration: 14700, Loss: 0.7464241981506348\n",
      "Iteration: 14701, Loss: 0.7464240789413452\n",
      "Iteration: 14702, Loss: 0.7464239001274109\n",
      "Iteration: 14703, Loss: 0.7464237809181213\n",
      "Iteration: 14704, Loss: 0.746423602104187\n",
      "Iteration: 14705, Loss: 0.7464235424995422\n",
      "Iteration: 14706, Loss: 0.7464233636856079\n",
      "Iteration: 14707, Loss: 0.7464232444763184\n",
      "Iteration: 14708, Loss: 0.7464231252670288\n",
      "Iteration: 14709, Loss: 0.7464230060577393\n",
      "Iteration: 14710, Loss: 0.7464230060577393\n",
      "Iteration: 14711, Loss: 0.7464228272438049\n",
      "Iteration: 14712, Loss: 0.7464227080345154\n",
      "Iteration: 14713, Loss: 0.746422529220581\n",
      "Iteration: 14714, Loss: 0.7464224100112915\n",
      "Iteration: 14715, Loss: 0.7464222311973572\n",
      "Iteration: 14716, Loss: 0.7464221119880676\n",
      "Iteration: 14717, Loss: 0.7464219331741333\n",
      "Iteration: 14718, Loss: 0.7464218139648438\n",
      "Iteration: 14719, Loss: 0.7464216351509094\n",
      "Iteration: 14720, Loss: 0.7464215159416199\n",
      "Iteration: 14721, Loss: 0.7464212775230408\n",
      "Iteration: 14722, Loss: 0.7464210987091064\n",
      "Iteration: 14723, Loss: 0.7464209794998169\n",
      "Iteration: 14724, Loss: 0.7464208602905273\n",
      "Iteration: 14725, Loss: 0.7464207410812378\n",
      "Iteration: 14726, Loss: 0.7464205622673035\n",
      "Iteration: 14727, Loss: 0.7464204430580139\n",
      "Iteration: 14728, Loss: 0.7464202642440796\n",
      "Iteration: 14729, Loss: 0.7464202642440796\n",
      "Iteration: 14730, Loss: 0.7464200854301453\n",
      "Iteration: 14731, Loss: 0.7464199662208557\n",
      "Iteration: 14732, Loss: 0.7464198470115662\n",
      "Iteration: 14733, Loss: 0.7464197874069214\n",
      "Iteration: 14734, Loss: 0.7464199066162109\n",
      "Iteration: 14735, Loss: 0.7464196681976318\n",
      "Iteration: 14736, Loss: 0.7464195489883423\n",
      "Iteration: 14737, Loss: 0.7464194297790527\n",
      "Iteration: 14738, Loss: 0.7464193105697632\n",
      "Iteration: 14739, Loss: 0.7464191913604736\n",
      "Iteration: 14740, Loss: 0.7464190721511841\n",
      "Iteration: 14741, Loss: 0.7464188933372498\n",
      "Iteration: 14742, Loss: 0.7464187741279602\n",
      "Iteration: 14743, Loss: 0.7464185953140259\n",
      "Iteration: 14744, Loss: 0.7464184761047363\n",
      "Iteration: 14745, Loss: 0.7464183568954468\n",
      "Iteration: 14746, Loss: 0.7464182376861572\n",
      "Iteration: 14747, Loss: 0.7464180588722229\n",
      "Iteration: 14748, Loss: 0.7464179396629333\n",
      "Iteration: 14749, Loss: 0.7464178800582886\n",
      "Iteration: 14750, Loss: 0.7464178204536438\n",
      "Iteration: 14751, Loss: 0.7464177012443542\n",
      "Iteration: 14752, Loss: 0.7464177012443542\n",
      "Iteration: 14753, Loss: 0.7464175820350647\n",
      "Iteration: 14754, Loss: 0.7464174032211304\n",
      "Iteration: 14755, Loss: 0.7464171648025513\n",
      "Iteration: 14756, Loss: 0.7464170455932617\n",
      "Iteration: 14757, Loss: 0.7464169263839722\n",
      "Iteration: 14758, Loss: 0.7464168071746826\n",
      "Iteration: 14759, Loss: 0.7464166283607483\n",
      "Iteration: 14760, Loss: 0.7464165091514587\n",
      "Iteration: 14761, Loss: 0.7464163899421692\n",
      "Iteration: 14762, Loss: 0.7464162707328796\n",
      "Iteration: 14763, Loss: 0.7464161515235901\n",
      "Iteration: 14764, Loss: 0.7464160323143005\n",
      "Iteration: 14765, Loss: 0.746415913105011\n",
      "Iteration: 14766, Loss: 0.7464157938957214\n",
      "Iteration: 14767, Loss: 0.7464156746864319\n",
      "Iteration: 14768, Loss: 0.7464155554771423\n",
      "Iteration: 14769, Loss: 0.7464151382446289\n",
      "Iteration: 14770, Loss: 0.7464150190353394\n",
      "Iteration: 14771, Loss: 0.7464148998260498\n",
      "Iteration: 14772, Loss: 0.7464147806167603\n",
      "Iteration: 14773, Loss: 0.7464146614074707\n",
      "Iteration: 14774, Loss: 0.7464145421981812\n",
      "Iteration: 14775, Loss: 0.7464144229888916\n",
      "Iteration: 14776, Loss: 0.746414303779602\n",
      "Iteration: 14777, Loss: 0.7464141249656677\n",
      "Iteration: 14778, Loss: 0.7464140057563782\n",
      "Iteration: 14779, Loss: 0.7464138865470886\n",
      "Iteration: 14780, Loss: 0.7464137673377991\n",
      "Iteration: 14781, Loss: 0.7464136481285095\n",
      "Iteration: 14782, Loss: 0.74641352891922\n",
      "Iteration: 14783, Loss: 0.74641352891922\n",
      "Iteration: 14784, Loss: 0.7464132905006409\n",
      "Iteration: 14785, Loss: 0.7464132308959961\n",
      "Iteration: 14786, Loss: 0.7464131116867065\n",
      "Iteration: 14787, Loss: 0.7464130520820618\n",
      "Iteration: 14788, Loss: 0.7464129328727722\n",
      "Iteration: 14789, Loss: 0.7464127540588379\n",
      "Iteration: 14790, Loss: 0.7464126348495483\n",
      "Iteration: 14791, Loss: 0.746412456035614\n",
      "Iteration: 14792, Loss: 0.7464123368263245\n",
      "Iteration: 14793, Loss: 0.7464121580123901\n",
      "Iteration: 14794, Loss: 0.7464120388031006\n",
      "Iteration: 14795, Loss: 0.7464117407798767\n",
      "Iteration: 14796, Loss: 0.7464116215705872\n",
      "Iteration: 14797, Loss: 0.7464114427566528\n",
      "Iteration: 14798, Loss: 0.7464113831520081\n",
      "Iteration: 14799, Loss: 0.7464112639427185\n",
      "Iteration: 14800, Loss: 0.746411144733429\n",
      "Iteration: 14801, Loss: 0.7464110851287842\n",
      "Iteration: 14802, Loss: 0.7464110851287842\n",
      "Iteration: 14803, Loss: 0.7464109659194946\n",
      "Iteration: 14804, Loss: 0.7464108467102051\n",
      "Iteration: 14805, Loss: 0.7464107871055603\n",
      "Iteration: 14806, Loss: 0.746410608291626\n",
      "Iteration: 14807, Loss: 0.7464104890823364\n",
      "Iteration: 14808, Loss: 0.7464103698730469\n",
      "Iteration: 14809, Loss: 0.7464102506637573\n",
      "Iteration: 14810, Loss: 0.7464101314544678\n",
      "Iteration: 14811, Loss: 0.7464099526405334\n",
      "Iteration: 14812, Loss: 0.7464098334312439\n",
      "Iteration: 14813, Loss: 0.7464097142219543\n",
      "Iteration: 14814, Loss: 0.7464095950126648\n",
      "Iteration: 14815, Loss: 0.7464094758033752\n",
      "Iteration: 14816, Loss: 0.7464093565940857\n",
      "Iteration: 14817, Loss: 0.7464091777801514\n",
      "Iteration: 14818, Loss: 0.7464091777801514\n",
      "Iteration: 14819, Loss: 0.7464090585708618\n",
      "Iteration: 14820, Loss: 0.7464089393615723\n",
      "Iteration: 14821, Loss: 0.7464088201522827\n",
      "Iteration: 14822, Loss: 0.7464087009429932\n",
      "Iteration: 14823, Loss: 0.7464085817337036\n",
      "Iteration: 14824, Loss: 0.7464084625244141\n",
      "Iteration: 14825, Loss: 0.7464083433151245\n",
      "Iteration: 14826, Loss: 0.7464082837104797\n",
      "Iteration: 14827, Loss: 0.7464081645011902\n",
      "Iteration: 14828, Loss: 0.7464080452919006\n",
      "Iteration: 14829, Loss: 0.7464079260826111\n",
      "Iteration: 14830, Loss: 0.7464078068733215\n",
      "Iteration: 14831, Loss: 0.746407687664032\n",
      "Iteration: 14832, Loss: 0.7464075684547424\n",
      "Iteration: 14833, Loss: 0.7464074492454529\n",
      "Iteration: 14834, Loss: 0.7464073300361633\n",
      "Iteration: 14835, Loss: 0.7464072108268738\n",
      "Iteration: 14836, Loss: 0.7464070916175842\n",
      "Iteration: 14837, Loss: 0.7464069724082947\n",
      "Iteration: 14838, Loss: 0.7464069724082947\n",
      "Iteration: 14839, Loss: 0.7464068531990051\n",
      "Iteration: 14840, Loss: 0.7464066743850708\n",
      "Iteration: 14841, Loss: 0.7464065551757812\n",
      "Iteration: 14842, Loss: 0.7464064359664917\n",
      "Iteration: 14843, Loss: 0.7464061379432678\n",
      "Iteration: 14844, Loss: 0.7464060187339783\n",
      "Iteration: 14845, Loss: 0.7464060187339783\n",
      "Iteration: 14846, Loss: 0.7464058995246887\n",
      "Iteration: 14847, Loss: 0.7464057803153992\n",
      "Iteration: 14848, Loss: 0.7464056015014648\n",
      "Iteration: 14849, Loss: 0.7464054226875305\n",
      "Iteration: 14850, Loss: 0.746405303478241\n",
      "Iteration: 14851, Loss: 0.7464051842689514\n",
      "Iteration: 14852, Loss: 0.7464050054550171\n",
      "Iteration: 14853, Loss: 0.7464048862457275\n",
      "Iteration: 14854, Loss: 0.746404767036438\n",
      "Iteration: 14855, Loss: 0.7464046478271484\n",
      "Iteration: 14856, Loss: 0.7464045286178589\n",
      "Iteration: 14857, Loss: 0.7464044094085693\n",
      "Iteration: 14858, Loss: 0.7464042901992798\n",
      "Iteration: 14859, Loss: 0.7464041709899902\n",
      "Iteration: 14860, Loss: 0.746404230594635\n",
      "Iteration: 14861, Loss: 0.746404230594635\n",
      "Iteration: 14862, Loss: 0.7464041113853455\n",
      "Iteration: 14863, Loss: 0.7464039921760559\n",
      "Iteration: 14864, Loss: 0.7464038729667664\n",
      "Iteration: 14865, Loss: 0.7464037537574768\n",
      "Iteration: 14866, Loss: 0.746403694152832\n",
      "Iteration: 14867, Loss: 0.7464035153388977\n",
      "Iteration: 14868, Loss: 0.7464033961296082\n",
      "Iteration: 14869, Loss: 0.7464032769203186\n",
      "Iteration: 14870, Loss: 0.746403157711029\n",
      "Iteration: 14871, Loss: 0.7464030385017395\n",
      "Iteration: 14872, Loss: 0.7464029788970947\n",
      "Iteration: 14873, Loss: 0.7464028596878052\n",
      "Iteration: 14874, Loss: 0.7464027404785156\n",
      "Iteration: 14875, Loss: 0.7464026212692261\n",
      "Iteration: 14876, Loss: 0.7464025020599365\n",
      "Iteration: 14877, Loss: 0.746402382850647\n",
      "Iteration: 14878, Loss: 0.7464023232460022\n",
      "Iteration: 14879, Loss: 0.7464022040367126\n",
      "Iteration: 14880, Loss: 0.7464020848274231\n",
      "Iteration: 14881, Loss: 0.7464019656181335\n",
      "Iteration: 14882, Loss: 0.746401846408844\n",
      "Iteration: 14883, Loss: 0.7464017271995544\n",
      "Iteration: 14884, Loss: 0.7464016079902649\n",
      "Iteration: 14885, Loss: 0.7464014887809753\n",
      "Iteration: 14886, Loss: 0.7464013695716858\n",
      "Iteration: 14887, Loss: 0.746401309967041\n",
      "Iteration: 14888, Loss: 0.7464011907577515\n",
      "Iteration: 14889, Loss: 0.7464010715484619\n",
      "Iteration: 14890, Loss: 0.7464009523391724\n",
      "Iteration: 14891, Loss: 0.7464009523391724\n",
      "Iteration: 14892, Loss: 0.7464008331298828\n",
      "Iteration: 14893, Loss: 0.7464007139205933\n",
      "Iteration: 14894, Loss: 0.7464005947113037\n",
      "Iteration: 14895, Loss: 0.7464004755020142\n",
      "Iteration: 14896, Loss: 0.7464003562927246\n",
      "Iteration: 14897, Loss: 0.7464002966880798\n",
      "Iteration: 14898, Loss: 0.7464001774787903\n",
      "Iteration: 14899, Loss: 0.7464001774787903\n",
      "Iteration: 14900, Loss: 0.7464000582695007\n",
      "Iteration: 14901, Loss: 0.7463999390602112\n",
      "Iteration: 14902, Loss: 0.7463998198509216\n",
      "Iteration: 14903, Loss: 0.7463997006416321\n",
      "Iteration: 14904, Loss: 0.7463996410369873\n",
      "Iteration: 14905, Loss: 0.7463995218276978\n",
      "Iteration: 14906, Loss: 0.7463994026184082\n",
      "Iteration: 14907, Loss: 0.7463992834091187\n",
      "Iteration: 14908, Loss: 0.7463991641998291\n",
      "Iteration: 14909, Loss: 0.7463990449905396\n",
      "Iteration: 14910, Loss: 0.74639892578125\n",
      "Iteration: 14911, Loss: 0.7463988661766052\n",
      "Iteration: 14912, Loss: 0.7463987469673157\n",
      "Iteration: 14913, Loss: 0.7463986277580261\n",
      "Iteration: 14914, Loss: 0.7463985085487366\n",
      "Iteration: 14915, Loss: 0.746398389339447\n",
      "Iteration: 14916, Loss: 0.7463982701301575\n",
      "Iteration: 14917, Loss: 0.7463982105255127\n",
      "Iteration: 14918, Loss: 0.7463980913162231\n",
      "Iteration: 14919, Loss: 0.7463979721069336\n",
      "Iteration: 14920, Loss: 0.746397852897644\n",
      "Iteration: 14921, Loss: 0.7463977336883545\n",
      "Iteration: 14922, Loss: 0.7463976144790649\n",
      "Iteration: 14923, Loss: 0.7463975548744202\n",
      "Iteration: 14924, Loss: 0.7463974356651306\n",
      "Iteration: 14925, Loss: 0.7463974356651306\n",
      "Iteration: 14926, Loss: 0.7463973164558411\n",
      "Iteration: 14927, Loss: 0.7463971972465515\n",
      "Iteration: 14928, Loss: 0.746397078037262\n",
      "Iteration: 14929, Loss: 0.7463970184326172\n",
      "Iteration: 14930, Loss: 0.7463968992233276\n",
      "Iteration: 14931, Loss: 0.7463968992233276\n",
      "Iteration: 14932, Loss: 0.7463967800140381\n",
      "Iteration: 14933, Loss: 0.7463967204093933\n",
      "Iteration: 14934, Loss: 0.7463966012001038\n",
      "Iteration: 14935, Loss: 0.7463964819908142\n",
      "Iteration: 14936, Loss: 0.7463963627815247\n",
      "Iteration: 14937, Loss: 0.7463963031768799\n",
      "Iteration: 14938, Loss: 0.7463961839675903\n",
      "Iteration: 14939, Loss: 0.746396005153656\n",
      "Iteration: 14940, Loss: 0.7463958859443665\n",
      "Iteration: 14941, Loss: 0.7463956475257874\n",
      "Iteration: 14942, Loss: 0.7463955283164978\n",
      "Iteration: 14943, Loss: 0.7463954091072083\n",
      "Iteration: 14944, Loss: 0.7463955283164978\n",
      "Iteration: 14945, Loss: 0.746395468711853\n",
      "Iteration: 14946, Loss: 0.7463952302932739\n",
      "Iteration: 14947, Loss: 0.7463951110839844\n",
      "Iteration: 14948, Loss: 0.7463951706886292\n",
      "Iteration: 14949, Loss: 0.7463950514793396\n",
      "Iteration: 14950, Loss: 0.74639493227005\n",
      "Iteration: 14951, Loss: 0.7463948130607605\n",
      "Iteration: 14952, Loss: 0.7463947534561157\n",
      "Iteration: 14953, Loss: 0.7463946342468262\n",
      "Iteration: 14954, Loss: 0.7463943958282471\n",
      "Iteration: 14955, Loss: 0.7463943362236023\n",
      "Iteration: 14956, Loss: 0.7463942170143127\n",
      "Iteration: 14957, Loss: 0.7463940978050232\n",
      "Iteration: 14958, Loss: 0.7463939785957336\n",
      "Iteration: 14959, Loss: 0.7463939189910889\n",
      "Iteration: 14960, Loss: 0.7463937997817993\n",
      "Iteration: 14961, Loss: 0.7463936805725098\n",
      "Iteration: 14962, Loss: 0.7463935613632202\n",
      "Iteration: 14963, Loss: 0.746393620967865\n",
      "Iteration: 14964, Loss: 0.7463935017585754\n",
      "Iteration: 14965, Loss: 0.7463933825492859\n",
      "Iteration: 14966, Loss: 0.7463933229446411\n",
      "Iteration: 14967, Loss: 0.7463932633399963\n",
      "Iteration: 14968, Loss: 0.7463932037353516\n",
      "Iteration: 14969, Loss: 0.746393084526062\n",
      "Iteration: 14970, Loss: 0.7463930249214172\n",
      "Iteration: 14971, Loss: 0.7463929057121277\n",
      "Iteration: 14972, Loss: 0.7463927865028381\n",
      "Iteration: 14973, Loss: 0.7463926672935486\n",
      "Iteration: 14974, Loss: 0.7463926076889038\n",
      "Iteration: 14975, Loss: 0.7463924884796143\n",
      "Iteration: 14976, Loss: 0.7463923692703247\n",
      "Iteration: 14977, Loss: 0.7463922500610352\n",
      "Iteration: 14978, Loss: 0.7463921904563904\n",
      "Iteration: 14979, Loss: 0.7463919520378113\n",
      "Iteration: 14980, Loss: 0.7463918328285217\n",
      "Iteration: 14981, Loss: 0.746391773223877\n",
      "Iteration: 14982, Loss: 0.7463916540145874\n",
      "Iteration: 14983, Loss: 0.7463915348052979\n",
      "Iteration: 14984, Loss: 0.7463914155960083\n",
      "Iteration: 14985, Loss: 0.7463913559913635\n",
      "Iteration: 14986, Loss: 0.746391236782074\n",
      "Iteration: 14987, Loss: 0.7463911175727844\n",
      "Iteration: 14988, Loss: 0.7463909983634949\n",
      "Iteration: 14989, Loss: 0.7463909387588501\n",
      "Iteration: 14990, Loss: 0.7463908195495605\n",
      "Iteration: 14991, Loss: 0.746390700340271\n",
      "Iteration: 14992, Loss: 0.746390700340271\n",
      "Iteration: 14993, Loss: 0.7463906407356262\n",
      "Iteration: 14994, Loss: 0.7463906407356262\n",
      "Iteration: 14995, Loss: 0.7463905215263367\n",
      "Iteration: 14996, Loss: 0.7463904619216919\n",
      "Iteration: 14997, Loss: 0.7463903427124023\n",
      "Iteration: 14998, Loss: 0.7463902235031128\n",
      "Iteration: 14999, Loss: 0.7463901042938232\n",
      "Iteration: 15000, Loss: 0.7463900446891785\n",
      "Iteration: 15001, Loss: 0.7463899254798889\n",
      "Iteration: 15002, Loss: 0.7463898062705994\n",
      "Iteration: 15003, Loss: 0.7463897466659546\n",
      "Iteration: 15004, Loss: 0.746389627456665\n",
      "Iteration: 15005, Loss: 0.7463895082473755\n",
      "Iteration: 15006, Loss: 0.7463894486427307\n",
      "Iteration: 15007, Loss: 0.7463895678520203\n",
      "Iteration: 15008, Loss: 0.7463895082473755\n",
      "Iteration: 15009, Loss: 0.7463893890380859\n",
      "Iteration: 15010, Loss: 0.7463893294334412\n",
      "Iteration: 15011, Loss: 0.7463891506195068\n",
      "Iteration: 15012, Loss: 0.7463889718055725\n",
      "Iteration: 15013, Loss: 0.7463887929916382\n",
      "Iteration: 15014, Loss: 0.7463887333869934\n",
      "Iteration: 15015, Loss: 0.7463887929916382\n",
      "Iteration: 15016, Loss: 0.7463886737823486\n",
      "Iteration: 15017, Loss: 0.7463884949684143\n",
      "Iteration: 15018, Loss: 0.7463884353637695\n",
      "Iteration: 15019, Loss: 0.74638831615448\n",
      "Iteration: 15020, Loss: 0.7463882565498352\n",
      "Iteration: 15021, Loss: 0.7463880777359009\n",
      "Iteration: 15022, Loss: 0.7463878989219666\n",
      "Iteration: 15023, Loss: 0.7463878393173218\n",
      "Iteration: 15024, Loss: 0.7463877201080322\n",
      "Iteration: 15025, Loss: 0.7463876605033875\n",
      "Iteration: 15026, Loss: 0.7463875412940979\n",
      "Iteration: 15027, Loss: 0.7463875412940979\n",
      "Iteration: 15028, Loss: 0.7463874220848083\n",
      "Iteration: 15029, Loss: 0.746387243270874\n",
      "Iteration: 15030, Loss: 0.746387243270874\n",
      "Iteration: 15031, Loss: 0.7463871836662292\n",
      "Iteration: 15032, Loss: 0.7463870644569397\n",
      "Iteration: 15033, Loss: 0.7463870048522949\n",
      "Iteration: 15034, Loss: 0.7463868856430054\n",
      "Iteration: 15035, Loss: 0.7463867664337158\n",
      "Iteration: 15036, Loss: 0.746386706829071\n",
      "Iteration: 15037, Loss: 0.7463866472244263\n",
      "Iteration: 15038, Loss: 0.7463862299919128\n",
      "Iteration: 15039, Loss: 0.7463862299919128\n",
      "Iteration: 15040, Loss: 0.7463862299919128\n",
      "Iteration: 15041, Loss: 0.7463861107826233\n",
      "Iteration: 15042, Loss: 0.7463859915733337\n",
      "Iteration: 15043, Loss: 0.746385931968689\n",
      "Iteration: 15044, Loss: 0.7463858127593994\n",
      "Iteration: 15045, Loss: 0.7463856935501099\n",
      "Iteration: 15046, Loss: 0.7463856339454651\n",
      "Iteration: 15047, Loss: 0.7463855743408203\n",
      "Iteration: 15048, Loss: 0.7463854551315308\n",
      "Iteration: 15049, Loss: 0.7463853359222412\n",
      "Iteration: 15050, Loss: 0.7463851571083069\n",
      "Iteration: 15051, Loss: 0.7463850975036621\n",
      "Iteration: 15052, Loss: 0.7463850378990173\n",
      "Iteration: 15053, Loss: 0.7463849186897278\n",
      "Iteration: 15054, Loss: 0.7463847994804382\n",
      "Iteration: 15055, Loss: 0.7463847398757935\n",
      "Iteration: 15056, Loss: 0.7463845610618591\n",
      "Iteration: 15057, Loss: 0.7463845014572144\n",
      "Iteration: 15058, Loss: 0.7463843822479248\n",
      "Iteration: 15059, Loss: 0.7463842630386353\n",
      "Iteration: 15060, Loss: 0.7463842034339905\n",
      "Iteration: 15061, Loss: 0.7463840842247009\n",
      "Iteration: 15062, Loss: 0.7463839650154114\n",
      "Iteration: 15063, Loss: 0.7463840246200562\n",
      "Iteration: 15064, Loss: 0.7463839650154114\n",
      "Iteration: 15065, Loss: 0.7463838458061218\n",
      "Iteration: 15066, Loss: 0.7463837265968323\n",
      "Iteration: 15067, Loss: 0.7463836073875427\n",
      "Iteration: 15068, Loss: 0.7463836073875427\n",
      "Iteration: 15069, Loss: 0.7463834881782532\n",
      "Iteration: 15070, Loss: 0.7463833689689636\n",
      "Iteration: 15071, Loss: 0.7463833689689636\n",
      "Iteration: 15072, Loss: 0.7463831901550293\n",
      "Iteration: 15073, Loss: 0.7463830709457397\n",
      "Iteration: 15074, Loss: 0.746383011341095\n",
      "Iteration: 15075, Loss: 0.746383011341095\n",
      "Convergence reached\n",
      "Accuracy test: 1.000, Accuracy train: 1.000, Accuracy neuron and time shuffled: 0.500, Accuracy trial shuffled: 0.300\n"
     ]
    }
   ],
   "source": [
    "X = np.array(np.nan_to_num(tensor).transpose((2,0,1)), dtype=np.float32)\n",
    "#X = (X - np.mean(X, axis=1)[:,None,:])\n",
    "y = np.array(labels, dtype=np.int64)\n",
    "print(f'X: {X.shape}, y: {y.shape}')\n",
    "\n",
    "# y[y==1] = 0\n",
    "# y[y>=2] = 1\n",
    "# y[y==2] = 0\n",
    "# y[y==3] = 1\n",
    "\n",
    "\n",
    "DEVICE = tensor_regression.util.set_device(use_GPU=True)\n",
    "\n",
    "u, c = np.unique(y, return_counts=True)\n",
    "wei = c.sum() / c\n",
    "wei = wei*4\n",
    "print(wei)\n",
    "\n",
    "hyperparameters = {}\n",
    "hyperparameters['L2'] = [0.001] #[0.0125] #[0.0005] #[0.0, 0.001, 0.002, 0.004, 0.01, 0.02, 0.04]\n",
    "hyperparameters['lr'] = [0.007] #[0.005] #[0.05] #[0.0003, 0.001, 0.003, 0.01, 0.03]\n",
    "hyperparameters['rank'] = [4] #[4] #[1,2,3,4]\n",
    "hyperparameters['Bcp_init_scale'] = [0.625] #[0.625] #[0.5, 1.0, 2.0]\n",
    "hyperparameters['non_negative'] = [[True, False, True]] #[[True, True, True]] #[[False, False, False], [True, False, False], [False, True, False], [True, True, False]]\n",
    "hyperparameters['iteration'] = list(np.arange(1)) #list(np.arange(5))\n",
    "\n",
    "results = {}\n",
    "for ii, h_vals in enumerate(itertools.product(*hyperparameters.values())):\n",
    "    h_dict = {key: hv for key, hv in zip(hyperparameters.keys(), h_vals)}\n",
    "    print(f'hyperparameters: {h_dict}')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.5, shuffle=True, stratify=y)\n",
    "\n",
    "    cpmlr = mtr.CP_logistic_regression(\n",
    "        X_train,\n",
    "        y_train, \n",
    "        rank=h_dict['rank'],\n",
    "        non_negative=h_dict['non_negative'],\n",
    "        weights=None,\n",
    "        Bcp_init=None,\n",
    "        Bcp_init_scale=h_dict['Bcp_init_scale'],\n",
    "        device=DEVICE,\n",
    "        softplus_kwargs={\n",
    "            'beta': 50,\n",
    "            'threshold':1\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # cpmlr.fit(\n",
    "    #     lambda_L2=0.000, \n",
    "    #     max_iter=200, \n",
    "    #     tol=1e-50, \n",
    "    #     patience=10,\n",
    "    #     weights=weights,\n",
    "    #     verbose=2,\n",
    "    #     running_loss_logging_interval=1,\n",
    "    #     LBFGS_kwargs={\n",
    "    #         'lr' : 10000, \n",
    "    #         'max_iter' : 20, \n",
    "    #         'max_eval' : None, \n",
    "    #         'tolerance_grad' : 1e-07, \n",
    "    #         'tolerance_change' : 1e-09, \n",
    "    #         'history_size' : 100, \n",
    "    #         'line_search_fn' : \"strong_wolfe\"\n",
    "    #     }\n",
    "    #  )\n",
    "\n",
    "    cpmlr.fit_Adam(\n",
    "        lambda_L2=h_dict['L2'],\n",
    "        max_iter=40000, \n",
    "        tol=1e-5, \n",
    "        patience=100,\n",
    "        weights=wei,\n",
    "        verbose=2,\n",
    "        Adam_kwargs={\n",
    "                'lr' : h_dict['lr'], \n",
    "    #                             'betas' : (0.9, 0.999), \n",
    "    #                             'eps' : 1e-08, \n",
    "    #                             'weight_decay' : 0, \n",
    "                'amsgrad' : True\n",
    "            }\n",
    "         )\n",
    "\n",
    "    # # print(time.time() - tic)\n",
    "    # print(f'loss: {cpmlr.loss_running[-1]}')\n",
    "\n",
    "    # loss_all.append(cpmlr.loss_running[-1])\n",
    "\n",
    "    logit, pred = cpmlr.predict(X=X_test, y_true=y_test)\n",
    "    cm = mtr.confusion_matrix(pred, y_test)\n",
    "    acc_val = np.sum(np.diag(cm))/np.sum(cm)\n",
    "\n",
    "    logit, pred = cpmlr.predict(X=X_train, y_true=y_train)\n",
    "    cm = mtr.confusion_matrix(pred, y_train)\n",
    "    acc_train = np.sum(np.diag(cm))/np.sum(cm)\n",
    "\n",
    "    # Make neuron- and time-shuffled data\n",
    "    X_train_shuffled = X_train.copy()\n",
    "    X_train_shuffled = X_train_shuffled[:, np.random.permutation(X_train_shuffled.shape[1]), :]\n",
    "    X_train_shuffled = X_train_shuffled[:, :, np.random.permutation(X_train_shuffled.shape[2])]\n",
    "    logit, pred = cpmlr.predict(X=X_train_shuffled, y_true=y_train)\n",
    "    cm = mtr.confusion_matrix(pred, y_train)\n",
    "    acc_train_shuffled = np.sum(np.diag(cm))/np.sum(cm)\n",
    "\n",
    "    # Make trial-shuffled cm\n",
    "    logit, pred = cpmlr.predict(X=X_train[np.random.permutation(X_train.shape[0])], y_true=y_train)\n",
    "    cm = mtr.confusion_matrix(pred, y_train)\n",
    "    acc_train_shuffled_trial = np.sum(np.diag(cm))/np.sum(cm)\n",
    "    \n",
    "    print(f'Accuracy test: {acc_val:.3f}, Accuracy train: {acc_train:.3f}, Accuracy neuron and time shuffled: {acc_train_shuffled:.3f}, Accuracy trial shuffled: {acc_train_shuffled_trial:.3f}')\n",
    "\n",
    "    \n",
    "    results[ii] = {}\n",
    "    results[ii]['acc_val'] = acc_val\n",
    "    results[ii]['acc_train'] = acc_train\n",
    "    results[ii]['acc_train_shuffleNeurXTime'] = acc_train_shuffled\n",
    "    results[ii]['acc_train_shuffleTrial'] = acc_train_shuffled_trial\n",
    "    results[ii]['loss_running'] = cpmlr.loss_running\n",
    "    results[ii]['loss_train_final'] = cpmlr.loss_running[-1]\n",
    "    results[ii]['hyperparameters'] = h_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "78de3efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy TRAIN: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15245ee5c60>"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logit, pred = cpmlr.predict(X=X_train, y_true=y_train)\n",
    "cm = mtr.confusion_matrix(pred, y_train)\n",
    "acc = np.sum(np.diag(cm))/np.sum(cm)\n",
    "\n",
    "print(f'Accuracy TRAIN: {acc}')\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "dcb4a811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy TEST: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x152428a0f40>"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logit, pred = cpmlr.predict(X=X_test, y_true=y_test)\n",
    "cm = mtr.confusion_matrix(pred, y_test)\n",
    "acc = np.sum(np.diag(cm))/np.sum(cm)\n",
    "\n",
    "print(f'Accuracy TEST: {acc}')\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "c3724611",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cpmlr.plot_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "088a2aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.]]),\n",
       " 0.5)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpmlr.make_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a2f851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a937e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596731f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f3752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35c43ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67233ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b161d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d25b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35538f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13082be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6fc43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4000c690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6fc7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a8ab80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00ab04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef065b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae03a80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227b5204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d886215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca13d0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726b78fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deda5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
